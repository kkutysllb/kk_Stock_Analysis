#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å› å­é€‰æ‹©å™¨ - åŸºäºæœºå™¨å­¦ä¹ çš„æ™ºèƒ½å› å­é€‰æ‹©
ç»“åˆç»Ÿè®¡æ–¹æ³•å’Œæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œä»261ä¸ªæŠ€æœ¯å› å­ä¸­ç­›é€‰å‡ºæœ€æœ‰æ•ˆçš„é¢„æµ‹å› å­
"""

import os
import sys
import pandas as pd
import numpy as np
import warnings
from typing import Dict, List, Tuple, Optional, Any, Union
from datetime import datetime
import yaml
import logging
from dataclasses import dataclass
import json

# æœºå™¨å­¦ä¹ åº“
from sklearn.feature_selection import (
    SelectKBest, SelectPercentile, SelectFpr, SelectFdr, SelectFwe,
    RFE, RFECV, SelectFromModel, VarianceThreshold
)
from sklearn.feature_selection import (
    f_regression, mutual_info_regression, chi2, f_classif
)
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn.linear_model import LassoCV, RidgeCV, ElasticNetCV
from sklearn.svm import SVR
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb
import lightgbm as lgb

# ç»Ÿè®¡åº“
from scipy import stats
from scipy.stats import normaltest, jarque_bera
import seaborn as sns
import matplotlib.pyplot as plt

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))))
from qlib_quantitative.factor_mining.core.factor_analyzer import FactorAnalyzer, FactorAnalysisResult

warnings.filterwarnings('ignore')

@dataclass
class FactorSelectionResult:
    """å› å­é€‰æ‹©ç»“æœæ•°æ®ç±»"""
    method: str
    selected_factors: List[str]
    factor_scores: Dict[str, float]
    selection_metrics: Dict[str, float]
    cross_val_score: float
    feature_importance: Optional[Dict[str, float]]
    selection_date: datetime
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'method': self.method,
            'selected_factors': self.selected_factors,
            'factor_scores': self.factor_scores,
            'selection_metrics': self.selection_metrics,
            'cross_val_score': self.cross_val_score,
            'feature_importance': self.feature_importance,
            'selection_date': self.selection_date.isoformat()
        }


class FactorSelector:
    """
    å› å­é€‰æ‹©å™¨
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. åŸºäºç»Ÿè®¡æŒ‡æ ‡çš„å› å­ç­›é€‰
    2. åŸºäºæœºå™¨å­¦ä¹ çš„å› å­é€‰æ‹©
    3. å¤šç§é€‰æ‹©ç®—æ³•çš„é›†æˆ
    4. å› å­é‡è¦æ€§è¯„ä¼°
    5. å› å­ç¨³å®šæ€§éªŒè¯
    """
    
    def __init__(self, config_path: str = None):
        """
        åˆå§‹åŒ–å› å­é€‰æ‹©å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.logger = self._setup_logger()
        
        # åŠ è½½é…ç½®
        if config_path is None:
            config_path = os.path.join(os.path.dirname(__file__), "../config/factor_mining_config.yaml")
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
            self.logger.info(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            raise
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.factor_analyzer = FactorAnalyzer(config_path)
        self.selection_results = {}
        self.selected_factors = []
        
        self.logger.info("ğŸš€ å› å­é€‰æ‹©å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger(self.__class__.__name__)
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def statistical_factor_selection(self, 
                                   factor_analysis_results: Dict[str, FactorAnalysisResult],
                                   selection_criteria: Dict[str, Any] = None) -> FactorSelectionResult:
        """
        åŸºäºç»Ÿè®¡æŒ‡æ ‡çš„å› å­é€‰æ‹©
        
        Args:
            factor_analysis_results: å› å­åˆ†æç»“æœ
            selection_criteria: é€‰æ‹©æ ‡å‡†
            
        Returns:
            å› å­é€‰æ‹©ç»“æœ
        """
        try:
            self.logger.info("ğŸ“Š å¼€å§‹ç»Ÿè®¡å› å­é€‰æ‹©")
            
            # é»˜è®¤é€‰æ‹©æ ‡å‡†
            if selection_criteria is None:
                selection_criteria = {
                    'min_ic_ir': 0.5,      # æœ€å°IC_IR
                    'min_ic_mean': 0.02,   # æœ€å°ICå‡å€¼
                    'max_p_value': 0.05,   # æœ€å¤§på€¼
                    'min_significance': True, # å¿…é¡»æ˜¾è‘—
                    'top_k': 50            # é€‰æ‹©å‰Kä¸ª
                }
            
            selected_factors = []
            factor_scores = {}
            
            # ç­›é€‰æ¡ä»¶
            for factor_name, result in factor_analysis_results.items():
                # IC_IRç­›é€‰
                if abs(result.ic_ir) < selection_criteria.get('min_ic_ir', 0):
                    continue
                
                # ICå‡å€¼ç­›é€‰  
                if abs(result.ic_mean) < selection_criteria.get('min_ic_mean', 0):
                    continue
                
                # æ˜¾è‘—æ€§ç­›é€‰
                if selection_criteria.get('min_significance', False) and not result.significance:
                    continue
                
                # på€¼ç­›é€‰
                if result.p_value > selection_criteria.get('max_p_value', 1.0):
                    continue
                
                # è®¡ç®—ç»¼åˆå¾—åˆ†
                score = abs(result.ic_ir) * 0.4 + abs(result.ic_mean) * 0.3 + \
                       result.sharpe_ratio * 0.2 + (1 - result.p_value) * 0.1
                
                selected_factors.append(factor_name)
                factor_scores[factor_name] = score
            
            # æŒ‰å¾—åˆ†æ’åºï¼Œé€‰æ‹©Top K
            top_k = selection_criteria.get('top_k', len(selected_factors))
            sorted_factors = sorted(factor_scores.items(), key=lambda x: x[1], reverse=True)
            
            final_factors = [name for name, score in sorted_factors[:top_k]]
            final_scores = {name: score for name, score in sorted_factors[:top_k]}
            
            # è®¡ç®—é€‰æ‹©æŒ‡æ ‡
            selection_metrics = {
                'total_factors': len(factor_analysis_results),
                'passed_criteria': len(selected_factors),
                'final_selected': len(final_factors),
                'selection_ratio': len(final_factors) / len(factor_analysis_results),
                'avg_ic_ir': np.mean([factor_analysis_results[f].ic_ir for f in final_factors]),
                'avg_ic_mean': np.mean([factor_analysis_results[f].ic_mean for f in final_factors]),
                'significant_ratio': sum([factor_analysis_results[f].significance for f in final_factors]) / len(final_factors)
            }
            
            result = FactorSelectionResult(
                method='statistical',
                selected_factors=final_factors,
                factor_scores=final_scores,
                selection_metrics=selection_metrics,
                cross_val_score=0.0,  # ç»Ÿè®¡æ–¹æ³•ä¸éœ€è¦äº¤å‰éªŒè¯
                feature_importance=None,
                selection_date=datetime.now()
            )
            
            self.logger.info(f"âœ… ç»Ÿè®¡å› å­é€‰æ‹©å®Œæˆ: {len(final_factors)}ä¸ªå› å­")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ç»Ÿè®¡å› å­é€‰æ‹©å¤±è´¥: {e}")
            return FactorSelectionResult(
                method='statistical',
                selected_factors=[],
                factor_scores={},
                selection_metrics={},
                cross_val_score=0.0,
                feature_importance=None,
                selection_date=datetime.now()
            )
    
    def ml_factor_selection(self, 
                          X: pd.DataFrame, 
                          y: pd.Series,
                          method: str = 'rfecv',
                          n_features: int = 50) -> FactorSelectionResult:
        """
        åŸºäºæœºå™¨å­¦ä¹ çš„å› å­é€‰æ‹©
        
        Args:
            X: å› å­æ•°æ®
            y: ç›®æ ‡å˜é‡
            method: é€‰æ‹©æ–¹æ³• (rfecv, selectkbest, lasso, xgboost, etc.)
            n_features: é€‰æ‹©çš„å› å­æ•°é‡
            
        Returns:
            å› å­é€‰æ‹©ç»“æœ
        """
        try:
            self.logger.info(f"ğŸ¤– å¼€å§‹æœºå™¨å­¦ä¹ å› å­é€‰æ‹©: {method}")
            
            if X.empty or y.empty:
                raise ValueError("è¾“å…¥æ•°æ®ä¸ºç©º")
            
            # æ•°æ®é¢„å¤„ç†
            X_clean, y_clean = self._clean_data_for_ml(X, y)
            
            if len(X_clean) < 100:
                self.logger.warning("âš ï¸ æ ·æœ¬æ•°é‡è¿‡å°‘ï¼Œå¯èƒ½å½±å“é€‰æ‹©æ•ˆæœ")
            
            # æ ¹æ®æ–¹æ³•é€‰æ‹©å› å­
            if method == 'rfecv':
                result = self._rfecv_selection(X_clean, y_clean, n_features)
            elif method == 'selectkbest':
                result = self._selectkbest_selection(X_clean, y_clean, n_features)
            elif method == 'lasso':
                result = self._lasso_selection(X_clean, y_clean, n_features)
            elif method == 'xgboost':
                result = self._xgboost_selection(X_clean, y_clean, n_features)
            elif method == 'lightgbm':
                result = self._lightgbm_selection(X_clean, y_clean, n_features)
            elif method == 'random_forest':
                result = self._random_forest_selection(X_clean, y_clean, n_features)
            elif method == 'mutual_info':
                result = self._mutual_info_selection(X_clean, y_clean, n_features)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„é€‰æ‹©æ–¹æ³•: {method}")
            
            self.logger.info(f"âœ… {method} å› å­é€‰æ‹©å®Œæˆ: {len(result.selected_factors)}ä¸ªå› å­")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ æœºå™¨å­¦ä¹ å› å­é€‰æ‹©å¤±è´¥: {e}")
            return FactorSelectionResult(
                method=method,
                selected_factors=[],
                factor_scores={},
                selection_metrics={},
                cross_val_score=0.0,
                feature_importance=None,
                selection_date=datetime.now()
            )
    
    def _clean_data_for_ml(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:
        """æ¸…ç†æœºå™¨å­¦ä¹ æ•°æ®"""
        # ç§»é™¤ç¼ºå¤±å€¼
        valid_idx = y.notna() & X.notna().all(axis=1)
        X_clean = X[valid_idx].copy()
        y_clean = y[valid_idx].copy()
        
        # ç§»é™¤æ— å˜åŒ–çš„ç‰¹å¾
        var_threshold = VarianceThreshold(threshold=0.01)
        X_clean = pd.DataFrame(
            var_threshold.fit_transform(X_clean),
            columns=X_clean.columns[var_threshold.get_support()],
            index=X_clean.index
        )
        
        return X_clean, y_clean
    
    def _rfecv_selection(self, X: pd.DataFrame, y: pd.Series, n_features: int) -> FactorSelectionResult:
        """é€’å½’ç‰¹å¾æ¶ˆé™¤äº¤å‰éªŒè¯"""
        try:
            # ä½¿ç”¨éšæœºæ£®æ—ä½œä¸ºåŸºç¡€ä¼°è®¡å™¨
            estimator = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)
            
            # RFECV
            selector = RFECV(
                estimator=estimator,
                min_features_to_select=min(n_features, len(X.columns)),
                cv=5,
                scoring='neg_mean_squared_error',
                n_jobs=-1
            )
            
            X_selected = selector.fit_transform(X, y)
            selected_features = X.columns[selector.support_].tolist()
            
            # è®¡ç®—ç‰¹å¾é‡è¦æ€§
            feature_importance = {}
            if hasattr(selector.estimator_, 'feature_importances_'):
                importances = selector.estimator_.feature_importances_
                for i, feature in enumerate(selected_features):
                    feature_importance[feature] = float(importances[i])
            
            # äº¤å‰éªŒè¯å¾—åˆ†
            cv_scores = cross_val_score(estimator, X_selected, y, cv=5, scoring='neg_mean_squared_error')
            cv_score = float(-np.mean(cv_scores))
            
            # å› å­å¾—åˆ†ï¼ˆä½¿ç”¨é‡è¦æ€§ï¼‰
            factor_scores = feature_importance.copy()
            
            # é€‰æ‹©æŒ‡æ ‡
            selection_metrics = {
                'optimal_features': int(selector.n_features_),
                'cv_score': cv_score,
                'cv_std': float(np.std(cv_scores))
            }
            
            return FactorSelectionResult(
                method='rfecv',
                selected_factors=selected_features,
                factor_scores=factor_scores,
                selection_metrics=selection_metrics,
                cross_val_score=cv_score,
                feature_importance=feature_importance,
                selection_date=datetime.now()
            )
            
        except Exception as e:
            self.logger.error(f"âŒ RFECVé€‰æ‹©å¤±è´¥: {e}")
            return FactorSelectionResult(
                method='rfecv',
                selected_factors=[],
                factor_scores={},
                selection_metrics={},
                cross_val_score=0.0,
                feature_importance=None,
                selection_date=datetime.now()
            )
    
    def _selectkbest_selection(self, X: pd.DataFrame, y: pd.Series, n_features: int) -> FactorSelectionResult:
        """SelectKBestç‰¹å¾é€‰æ‹©"""
        try:
            # ä½¿ç”¨f_regressionè¯„åˆ†å‡½æ•°
            selector = SelectKBest(score_func=f_regression, k=min(n_features, len(X.columns)))
            X_selected = selector.fit_transform(X, y)
            
            selected_features = X.columns[selector.get_support()].tolist()
            scores = selector.scores_
            
            # å› å­å¾—åˆ†
            factor_scores = {}
            for i, feature in enumerate(selected_features):
                idx = X.columns.get_loc(feature)
                factor_scores[feature] = float(scores[idx])
            
            # äº¤å‰éªŒè¯
            estimator = RandomForestRegressor(n_estimators=50, random_state=42)
            cv_scores = cross_val_score(estimator, X_selected, y, cv=5, scoring='neg_mean_squared_error')
            cv_score = float(-np.mean(cv_scores))
            
            selection_metrics = {
                'avg_score': float(np.mean(scores)),
                'cv_score': cv_score
            }
            
            return FactorSelectionResult(
                method='selectkbest',
                selected_factors=selected_features,
                factor_scores=factor_scores,
                selection_metrics=selection_metrics,
                cross_val_score=cv_score,
                feature_importance=None,
                selection_date=datetime.now()
            )
            
        except Exception as e:
            self.logger.error(f"âŒ SelectKBesté€‰æ‹©å¤±è´¥: {e}")
            return FactorSelectionResult(
                method='selectkbest',
                selected_factors=[],
                factor_scores={},
                selection_metrics={},
                cross_val_score=0.0,
                feature_importance=None,
                selection_date=datetime.now()
            )
    
    def _lasso_selection(self, X: pd.DataFrame, y: pd.Series, n_features: int) -> FactorSelectionResult:
        """Lassoå›å½’ç‰¹å¾é€‰æ‹©"""
        try:
            # LassoCV
            lasso = LassoCV(cv=5, random_state=42, n_jobs=-1, max_iter=1000)
            lasso.fit(X, y)
            
            # è·å–éé›¶ç³»æ•°çš„ç‰¹å¾
            non_zero_coef = np.abs(lasso.coef_) > 1e-5
            selected_features = X.columns[non_zero_coef].tolist()
            
            # å¦‚æœé€‰æ‹©çš„ç‰¹å¾å¤ªå¤šï¼ŒæŒ‰ç³»æ•°ç»å¯¹å€¼æ’åºé€‰æ‹©top n
            if len(selected_features) > n_features:
                coef_abs = np.abs(lasso.coef_[non_zero_coef])
                sorted_idx = np.argsort(coef_abs)[::-1][:n_features]
                selected_features = [selected_features[i] for i in sorted_idx]
            
            # å› å­å¾—åˆ†ï¼ˆç³»æ•°ç»å¯¹å€¼ï¼‰
            factor_scores = {}
            for feature in selected_features:
                idx = X.columns.get_loc(feature)
                factor_scores[feature] = float(np.abs(lasso.coef_[idx]))
            
            # äº¤å‰éªŒè¯å¾—åˆ†
            cv_score = float(-lasso.score(X[selected_features], y))
            
            selection_metrics = {
                'alpha': float(lasso.alpha_),
                'n_iter': int(lasso.n_iter_),
                'cv_score': cv_score
            }
            
            return FactorSelectionResult(
                method='lasso',
                selected_factors=selected_features,
                factor_scores=factor_scores,
                selection_metrics=selection_metrics,
                cross_val_score=cv_score,
                feature_importance=None,
                selection_date=datetime.now()
            )
            
        except Exception as e:
            self.logger.error(f"âŒ Lassoé€‰æ‹©å¤±è´¥: {e}")
            return FactorSelectionResult(
                method='lasso',
                selected_factors=[],
                factor_scores={},
                selection_metrics={},
                cross_val_score=0.0,
                feature_importance=None,
                selection_date=datetime.now()
            )
    
    def _xgboost_selection(self, X: pd.DataFrame, y: pd.Series, n_features: int) -> FactorSelectionResult:
        """XGBoostç‰¹å¾é€‰æ‹©"""
        try:
            # XGBoostæ¨¡å‹
            xgb_model = xgb.XGBRegressor(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42,
                n_jobs=-1
            )
            
            xgb_model.fit(X, y)
            
            # è·å–ç‰¹å¾é‡è¦æ€§
            importance = xgb_model.feature_importances_
            feature_importance = dict(zip(X.columns, importance))
            
            # é€‰æ‹©top nç‰¹å¾
            sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
            selected_features = [name for name, score in sorted_features[:n_features]]
            factor_scores = {name: float(score) for name, score in sorted_features[:n_features]}
            
            # äº¤å‰éªŒè¯
            cv_scores = cross_val_score(xgb_model, X[selected_features], y, cv=5, scoring='neg_mean_squared_error')
            cv_score = float(-np.mean(cv_scores))
            
            selection_metrics = {
                'avg_importance': float(np.mean(importance)),
                'cv_score': cv_score
            }
            
            return FactorSelectionResult(
                method='xgboost',
                selected_factors=selected_features,
                factor_scores=factor_scores,
                selection_metrics=selection_metrics,
                cross_val_score=cv_score,
                feature_importance=feature_importance,
                selection_date=datetime.now()
            )
            
        except Exception as e:
            self.logger.error(f"âŒ XGBoosté€‰æ‹©å¤±è´¥: {e}")
            return FactorSelectionResult(
                method='xgboost',
                selected_factors=[],
                factor_scores={},
                selection_metrics={},
                cross_val_score=0.0,
                feature_importance=None,
                selection_date=datetime.now()
            )
    
    def _lightgbm_selection(self, X: pd.DataFrame, y: pd.Series, n_features: int) -> FactorSelectionResult:
        """LightGBMç‰¹å¾é€‰æ‹©"""
        try:
            # LightGBMæ¨¡å‹
            lgb_model = lgb.LGBMRegressor(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42,
                n_jobs=-1,
                verbose=-1
            )
            
            lgb_model.fit(X, y)
            
            # è·å–ç‰¹å¾é‡è¦æ€§
            importance = lgb_model.feature_importances_
            feature_importance = dict(zip(X.columns, importance))
            
            # é€‰æ‹©top nç‰¹å¾
            sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
            selected_features = [name for name, score in sorted_features[:n_features]]
            factor_scores = {name: float(score) for name, score in sorted_features[:n_features]}
            
            # äº¤å‰éªŒè¯
            cv_scores = cross_val_score(lgb_model, X[selected_features], y, cv=5, scoring='neg_mean_squared_error')
            cv_score = float(-np.mean(cv_scores))
            
            selection_metrics = {
                'avg_importance': float(np.mean(importance)),
                'cv_score': cv_score
            }
            
            return FactorSelectionResult(
                method='lightgbm',
                selected_factors=selected_features,
                factor_scores=factor_scores,
                selection_metrics=selection_metrics,
                cross_val_score=cv_score,
                feature_importance=feature_importance,
                selection_date=datetime.now()
            )
            
        except Exception as e:
            self.logger.error(f"âŒ LightGBMé€‰æ‹©å¤±è´¥: {e}")
            return FactorSelectionResult(
                method='lightgbm',
                selected_factors=[],
                factor_scores={},
                selection_metrics={},
                cross_val_score=0.0,
                feature_importance=None,
                selection_date=datetime.now()
            )
    
    def _random_forest_selection(self, X: pd.DataFrame, y: pd.Series, n_features: int) -> FactorSelectionResult:
        """éšæœºæ£®æ—ç‰¹å¾é€‰æ‹©"""
        try:
            # éšæœºæ£®æ—æ¨¡å‹
            rf_model = RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                random_state=42,
                n_jobs=-1
            )
            
            rf_model.fit(X, y)
            
            # è·å–ç‰¹å¾é‡è¦æ€§
            importance = rf_model.feature_importances_
            feature_importance = dict(zip(X.columns, importance))
            
            # é€‰æ‹©top nç‰¹å¾
            sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
            selected_features = [name for name, score in sorted_features[:n_features]]
            factor_scores = {name: float(score) for name, score in sorted_features[:n_features]}
            
            # äº¤å‰éªŒè¯
            cv_scores = cross_val_score(rf_model, X[selected_features], y, cv=5, scoring='neg_mean_squared_error')
            cv_score = float(-np.mean(cv_scores))
            
            selection_metrics = {
                'avg_importance': float(np.mean(importance)),
                'cv_score': cv_score
            }
            
            return FactorSelectionResult(
                method='random_forest',
                selected_factors=selected_features,
                factor_scores=factor_scores,
                selection_metrics=selection_metrics,
                cross_val_score=cv_score,
                feature_importance=feature_importance,
                selection_date=datetime.now()
            )
            
        except Exception as e:
            self.logger.error(f"âŒ éšæœºæ£®æ—é€‰æ‹©å¤±è´¥: {e}")
            return FactorSelectionResult(
                method='random_forest',
                selected_factors=[],
                factor_scores={},
                selection_metrics={},
                cross_val_score=0.0,
                feature_importance=None,
                selection_date=datetime.now()
            )
    
    def _mutual_info_selection(self, X: pd.DataFrame, y: pd.Series, n_features: int) -> FactorSelectionResult:
        """äº’ä¿¡æ¯ç‰¹å¾é€‰æ‹©"""
        try:
            # è®¡ç®—äº’ä¿¡æ¯
            mutual_info_scores = mutual_info_regression(X, y, random_state=42)
            
            # é€‰æ‹©top nç‰¹å¾
            feature_scores = dict(zip(X.columns, mutual_info_scores))
            sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)
            selected_features = [name for name, score in sorted_features[:n_features]]
            factor_scores = {name: float(score) for name, score in sorted_features[:n_features]}
            
            # äº¤å‰éªŒè¯
            estimator = RandomForestRegressor(n_estimators=50, random_state=42)
            cv_scores = cross_val_score(estimator, X[selected_features], y, cv=5, scoring='neg_mean_squared_error')
            cv_score = float(-np.mean(cv_scores))
            
            selection_metrics = {
                'avg_mutual_info': float(np.mean(mutual_info_scores)),
                'cv_score': cv_score
            }
            
            return FactorSelectionResult(
                method='mutual_info',
                selected_factors=selected_features,
                factor_scores=factor_scores,
                selection_metrics=selection_metrics,
                cross_val_score=cv_score,
                feature_importance=None,
                selection_date=datetime.now()
            )
            
        except Exception as e:
            self.logger.error(f"âŒ äº’ä¿¡æ¯é€‰æ‹©å¤±è´¥: {e}")
            return FactorSelectionResult(
                method='mutual_info',
                selected_factors=[],
                factor_scores={},
                selection_metrics={},
                cross_val_score=0.0,
                feature_importance=None,
                selection_date=datetime.now()
            )
    
    def ensemble_factor_selection(self, 
                                 X: pd.DataFrame, 
                                 y: pd.Series,
                                 methods: List[str] = None,
                                 n_features: int = 50,
                                 vote_threshold: float = 0.3) -> FactorSelectionResult:
        """
        é›†æˆå› å­é€‰æ‹©
        
        Args:
            X: å› å­æ•°æ®
            y: ç›®æ ‡å˜é‡
            methods: é€‰æ‹©æ–¹æ³•åˆ—è¡¨
            n_features: é€‰æ‹©çš„å› å­æ•°é‡
            vote_threshold: æŠ•ç¥¨é˜ˆå€¼
            
        Returns:
            é›†æˆé€‰æ‹©ç»“æœ
        """
        try:
            self.logger.info("ğŸ¯ å¼€å§‹é›†æˆå› å­é€‰æ‹©")
            
            if methods is None:
                methods = ['xgboost', 'lightgbm', 'random_forest', 'lasso', 'selectkbest']
            
            # è¿è¡Œå„ç§æ–¹æ³•
            method_results = {}
            for method in methods:
                try:
                    result = self.ml_factor_selection(X, y, method, n_features * 2)  # é€‰æ‹©æ›´å¤šå€™é€‰
                    method_results[method] = result
                    self.logger.info(f"  âœ… {method}: {len(result.selected_factors)}ä¸ªå› å­")
                except Exception as e:
                    self.logger.warning(f"  âš ï¸ {method} å¤±è´¥: {e}")
                    continue
            
            if not method_results:
                raise ValueError("æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†")
            
            # ç»Ÿè®¡æŠ•ç¥¨ç»“æœ
            factor_votes = {}
            factor_scores_sum = {}
            
            for method, result in method_results.items():
                for factor in result.selected_factors:
                    factor_votes[factor] = factor_votes.get(factor, 0) + 1
                    factor_scores_sum[factor] = factor_scores_sum.get(factor, 0) + \
                                              result.factor_scores.get(factor, 0)
            
            # è®¡ç®—æŠ•ç¥¨æ¯”ä¾‹å’Œå¹³å‡å¾—åˆ†
            n_methods = len(method_results)
            factor_vote_ratios = {f: votes/n_methods for f, votes in factor_votes.items()}
            factor_avg_scores = {f: score/factor_votes[f] for f, score in factor_scores_sum.items()}
            
            # æŒ‰æŠ•ç¥¨æ¯”ä¾‹å’Œå¹³å‡å¾—åˆ†ç­›é€‰
            qualified_factors = []
            for factor, vote_ratio in factor_vote_ratios.items():
                if vote_ratio >= vote_threshold:
                    qualified_factors.append((factor, vote_ratio, factor_avg_scores[factor]))
            
            # æ’åºé€‰æ‹©
            qualified_factors.sort(key=lambda x: (x[1], x[2]), reverse=True)
            selected_factors = [f[0] for f in qualified_factors[:n_features]]
            
            # æœ€ç»ˆå¾—åˆ†
            final_scores = {}
            for factor, vote_ratio, avg_score in qualified_factors[:n_features]:
                final_scores[factor] = vote_ratio * 0.7 + (avg_score / max(factor_avg_scores.values())) * 0.3
            
            # äº¤å‰éªŒè¯æœ€ç»ˆç»“æœ
            estimator = RandomForestRegressor(n_estimators=100, random_state=42)
            cv_scores = cross_val_score(estimator, X[selected_factors], y, cv=5, scoring='neg_mean_squared_error')
            cv_score = float(-np.mean(cv_scores))
            
            # é€‰æ‹©æŒ‡æ ‡
            selection_metrics = {
                'n_methods': n_methods,
                'vote_threshold': vote_threshold,
                'qualified_factors': len(qualified_factors),
                'final_selected': len(selected_factors),
                'avg_vote_ratio': np.mean([f[1] for f in qualified_factors[:n_features]]),
                'cv_score': cv_score
            }
            
            result = FactorSelectionResult(
                method='ensemble',
                selected_factors=selected_factors,
                factor_scores=final_scores,
                selection_metrics=selection_metrics,
                cross_val_score=cv_score,
                feature_importance=None,
                selection_date=datetime.now()
            )
            
            self.logger.info(f"âœ… é›†æˆå› å­é€‰æ‹©å®Œæˆ: {len(selected_factors)}ä¸ªå› å­")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ é›†æˆå› å­é€‰æ‹©å¤±è´¥: {e}")
            return FactorSelectionResult(
                method='ensemble',
                selected_factors=[],
                factor_scores={},
                selection_metrics={},
                cross_val_score=0.0,
                feature_importance=None,
                selection_date=datetime.now()
            )
    
    def run_comprehensive_selection(self, 
                                  start_date: str = None,
                                  end_date: str = None,
                                  return_period: int = 20) -> Dict[str, FactorSelectionResult]:
        """
        è¿è¡Œå…¨é¢çš„å› å­é€‰æ‹©æµç¨‹
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            return_period: æ”¶ç›Šç‡å‘¨æœŸ
            
        Returns:
            é€‰æ‹©ç»“æœå­—å…¸
        """
        try:
            self.logger.info("ğŸš€ å¼€å§‹å…¨é¢å› å­é€‰æ‹©æµç¨‹")
            
            # 1. è¿è¡Œå› å­åˆ†æ
            analysis_results = self.factor_analyzer.run_factor_analysis(start_date, end_date)
            if not analysis_results:
                raise ValueError("å› å­åˆ†æå¤±è´¥")
            
            # 2. è·å–å› å­åˆ†æç»“æœ
            period_key = f'{return_period}d'
            factor_analysis = analysis_results['factor_analysis'].get(period_key, {})
            
            if not factor_analysis:
                # å¦‚æœå½“å‰å‘¨æœŸæ²¡æœ‰ç»“æœï¼Œå°è¯•ä½¿ç”¨å…¶ä»–å‘¨æœŸçš„ç»“æœ
                self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°{return_period}å¤©å‘¨æœŸçš„å› å­åˆ†æç»“æœï¼Œå°è¯•ä½¿ç”¨å…¶ä»–å‘¨æœŸ")
                
                # æŸ¥æ‰¾å¯ç”¨çš„å‘¨æœŸ
                available_periods = [k for k, v in analysis_results['factor_analysis'].items() if v]
                if available_periods:
                    period_key = available_periods[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨å‘¨æœŸ
                    factor_analysis = analysis_results['factor_analysis'][period_key]
                    self.logger.info(f"âœ… ä½¿ç”¨{period_key}å‘¨æœŸçš„å› å­åˆ†æç»“æœ")
                else:
                    raise ValueError(f"æ‰€æœ‰å‘¨æœŸéƒ½æ²¡æœ‰æœ‰æ•ˆçš„å› å­åˆ†æç»“æœ")
            
            # 3. ç»Ÿè®¡å› å­é€‰æ‹©
            statistical_result = self.statistical_factor_selection(factor_analysis)
            
            # 4. å‡†å¤‡æœºå™¨å­¦ä¹ æ•°æ®
            # åŠ è½½A500æˆåˆ†è‚¡
            stock_codes = self.factor_analyzer.load_a500_universe()
            
            # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤æ—¥æœŸ
            if start_date is None:
                start_date = self.config['data_config']['train_period']['start_date']
            if end_date is None:
                end_date = self.config['data_config']['train_period']['end_date']
                
            # åŠ è½½å› å­æ•°æ®
            factor_data = self.factor_analyzer.load_factor_data(stock_codes, start_date, end_date)
            return_data = self.factor_analyzer.calculate_forward_returns(stock_codes, start_date, end_date)
            
            if factor_data.empty or return_data.empty:
                raise ValueError("æ— æ³•åŠ è½½æœºå™¨å­¦ä¹ æ•°æ®")
            
            # åˆå¹¶æ•°æ®
            return_col = f'return_{return_period}d'
            merged_data = factor_data.join(return_data[return_col], how='inner')
            
            X = merged_data[self.factor_analyzer.factor_fields].dropna()
            y = merged_data[return_col].loc[X.index]
            
            # 5. é›†æˆæœºå™¨å­¦ä¹ é€‰æ‹©
            ensemble_result = self.ensemble_factor_selection(X, y, n_features=50)
            
            # 6. ä¿å­˜ç»“æœ
            results = {
                'statistical': statistical_result,
                'ensemble': ensemble_result
            }
            
            self.selection_results = results
            
            # 7. é€‰æ‹©æœ€ä½³ç»“æœä½œä¸ºæœ€ç»ˆç»“æœ
            if ensemble_result.cross_val_score > 0:
                self.selected_factors = ensemble_result.selected_factors
            else:
                self.selected_factors = statistical_result.selected_factors
            
            self.logger.info(f"âœ… å…¨é¢å› å­é€‰æ‹©å®Œæˆï¼Œæœ€ç»ˆé€‰æ‹©{len(self.selected_factors)}ä¸ªå› å­")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ å…¨é¢å› å­é€‰æ‹©å¤±è´¥: {e}")
            return {}
    
    def get_selected_factors(self, method: str = 'best') -> List[str]:
        """
        è·å–é€‰æ‹©çš„å› å­
        
        Args:
            method: é€‰æ‹©æ–¹æ³• ('best', 'statistical', 'ensemble')
            
        Returns:
            é€‰æ‹©çš„å› å­åˆ—è¡¨
        """
        try:
            if method == 'best':
                return self.selected_factors
            elif method in self.selection_results:
                return self.selection_results[method].selected_factors
            else:
                self.logger.warning(f"âš ï¸ æ–¹æ³• {method} ä¸å­˜åœ¨ï¼Œè¿”å›é»˜è®¤ç»“æœ")
                return self.selected_factors
                
        except Exception as e:
            self.logger.error(f"âŒ è·å–é€‰æ‹©å› å­å¤±è´¥: {e}")
            return []


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸš€ æµ‹è¯•å› å­é€‰æ‹©å™¨...")
    
    try:
        selector = FactorSelector()
        
        # è¿è¡Œå…¨é¢é€‰æ‹©
        results = selector.run_comprehensive_selection(
            start_date="2023-01-01",
            end_date="2023-12-31"
        )
        
        if results:
            print("âœ… å› å­é€‰æ‹©æµ‹è¯•æˆåŠŸ")
            
            # è·å–é€‰æ‹©çš„å› å­
            selected_factors = selector.get_selected_factors()
            print(f"ğŸ“Š æœ€ç»ˆé€‰æ‹©å› å­: {len(selected_factors)}ä¸ª")
            print(f"å‰10ä¸ªå› å­: {selected_factors[:10]}")
        else:
            print("âŒ å› å­é€‰æ‹©æµ‹è¯•å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()