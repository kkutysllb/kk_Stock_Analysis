#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å› å­åˆ†æå™¨ - æ™ºèƒ½å› å­æŒ–æ˜ç³»ç»Ÿæ ¸å¿ƒæ¨¡å—
åŸºäº261ä¸ªæŠ€æœ¯å› å­åˆ†æä¸­è¯A500æŒ‡æ•°æˆåˆ†è‚¡çš„æ”¶ç›Šé¢„æµ‹èƒ½åŠ›
"""

import os
import sys
import pandas as pd
import numpy as np
import warnings
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime, timedelta
import yaml
import logging
from dataclasses import dataclass
from concurrent.futures import ProcessPoolExecutor, as_completed

# ç§‘å­¦è®¡ç®—åº“
from scipy import stats
from scipy.stats import pearsonr, spearmanr
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))))
from api.global_db import db_handler

# å¯¼å…¥åŠ é€Ÿæ¨¡å—
try:
    from ..acceleration import create_device_manager, GPUAccelerator
    ACCELERATION_AVAILABLE = True
except ImportError:
    ACCELERATION_AVAILABLE = False

# å¯¼å…¥è‡ªå®šä¹‰å› å­è®¡ç®—å™¨
try:
    from .custom_factor_calculator import CustomFactorCalculator
    CUSTOM_FACTOR_AVAILABLE = True
except ImportError:
    CUSTOM_FACTOR_AVAILABLE = False

warnings.filterwarnings('ignore')

@dataclass
class FactorAnalysisResult:
    """å› å­åˆ†æç»“æœæ•°æ®ç±»"""
    factor_name: str
    ic_mean: float
    ic_std: float
    ic_ir: float
    rank_ic: float
    t_stat: float
    p_value: float
    significance: bool
    turnover: float
    sharpe_ratio: float
    max_drawdown: float
    analysis_date: datetime
    sample_size: int
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'factor_name': self.factor_name,
            'ic_mean': self.ic_mean,
            'ic_std': self.ic_std,
            'ic_ir': self.ic_ir,
            'rank_ic': self.rank_ic,
            't_stat': self.t_stat,
            'p_value': self.p_value,
            'significance': self.significance,
            'turnover': self.turnover,
            'sharpe_ratio': self.sharpe_ratio,
            'max_drawdown': self.max_drawdown,
            'analysis_date': self.analysis_date,
            'sample_size': self.sample_size
        }


class FactorAnalyzer:
    """
    å› å­åˆ†æå™¨
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. åŠ è½½A500æˆåˆ†è‚¡çš„261ä¸ªæŠ€æœ¯å› å­æ•°æ®
    2. è®¡ç®—å› å­ä¸æ”¶ç›Šç‡çš„ç›¸å…³æ€§æŒ‡æ ‡
    3. å› å­æœ‰æ•ˆæ€§è¯„ä¼°å’Œæ’åº
    4. å› å­ç¨³å®šæ€§åˆ†æ
    5. å› å­æ­£äº¤åŒ–å¤„ç†
    """
    
    def __init__(self, config_path: str = None):
        """
        åˆå§‹åŒ–å› å­åˆ†æå™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.logger = self._setup_logger()
        
        # åŠ è½½é…ç½®
        if config_path is None:
            config_path = os.path.join(os.path.dirname(__file__), "../config/factor_mining_config.yaml")
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
            self.logger.info(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
        except Exception as e:
            self.logger.error(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            raise
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.db_handler = db_handler
        self.scaler = self._get_scaler()
        self.factor_data = {}
        self.return_data = {}
        self.analysis_results = {}
        
        # åˆå§‹åŒ–ç¡¬ä»¶åŠ é€Ÿ
        self.device_manager = None
        self.gpu_accelerator = None
        if ACCELERATION_AVAILABLE:
            try:
                self.device_manager = create_device_manager(self.config)
                if self.device_manager and self.device_manager.device_type:
                    # åªæœ‰è®¾å¤‡ç®¡ç†å™¨æˆåŠŸåˆå§‹åŒ–æ‰åˆ›å»ºGPUåŠ é€Ÿå™¨
                    self.gpu_accelerator = GPUAccelerator(self.device_manager, self.config.get('acceleration_config', {}))
                    self.logger.info(f"âš¡ ç¡¬ä»¶åŠ é€Ÿå·²å¯ç”¨: {self.device_manager.device_type.upper()}")
                else:
                    self.logger.info("ğŸ’» ç¡¬ä»¶åŠ é€Ÿæœªæ£€æµ‹åˆ°å¯ç”¨è®¾å¤‡ï¼Œä½¿ç”¨CPUæ¨¡å¼")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ç¡¬ä»¶åŠ é€Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨CPUæ¨¡å¼: {e}")
                self.device_manager = None
                self.gpu_accelerator = None
        else:
            self.logger.info("ğŸ’» åŠ é€Ÿæ¨¡å—æœªå®‰è£…ï¼Œä½¿ç”¨CPUè®¡ç®—æ¨¡å¼")
        
        # åˆå§‹åŒ–è‡ªå®šä¹‰å› å­è®¡ç®—å™¨
        self.custom_factor_calculator = None
        if CUSTOM_FACTOR_AVAILABLE:
            try:
                self.custom_factor_calculator = CustomFactorCalculator()
                self.logger.info("ğŸ”§ è‡ªå®šä¹‰å› å­è®¡ç®—å™¨å·²åˆå§‹åŒ–")
            except Exception as e:
                self.logger.warning(f"âš ï¸ è‡ªå®šä¹‰å› å­è®¡ç®—å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            self.logger.info("âš ï¸ è‡ªå®šä¹‰å› å­è®¡ç®—å™¨æ¨¡å—æœªæ‰¾åˆ°")
        
        # å› å­åˆ—è¡¨ - ä»é…ç½®æ–‡ä»¶ä¸­è·å–
        self.factor_fields = self._load_factor_fields()
        
        self.logger.info(f"ğŸš€ å› å­åˆ†æå™¨åˆå§‹åŒ–å®Œæˆï¼Œå…±{len(self.factor_fields)}ä¸ªå› å­")
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger(self.__class__.__name__)
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _get_scaler(self):
        """è·å–æ ‡å‡†åŒ–å™¨"""
        scaler_type = self.config['factor_config']['preprocessing']['standardization']
        
        if scaler_type == 'zscore':
            return StandardScaler()
        elif scaler_type == 'minmax':
            return MinMaxScaler()
        elif scaler_type == 'robust':
            return RobustScaler()
        else:
            return StandardScaler()
    
    def _load_factor_fields(self) -> List[str]:
        """
        åŠ è½½æ‰€æœ‰å› å­å­—æ®µåˆ—è¡¨ï¼ˆ326ä¸ªï¼‰
        ä¼˜å…ˆä»factor_mining_config.yamlä¸­è·å–å®Œæ•´å› å­åˆ—è¡¨
        """
        try:
            # ä¼˜å…ˆä»é…ç½®æ–‡ä»¶ä¸­åŠ è½½æ‰€æœ‰å› å­
            factor_fields = []
            factor_config = self.config['factor_config']['factor_categories']
            
            # åŠ è½½æ‰€æœ‰ç±»åˆ«çš„å› å­
            for category, factors in factor_config.items():
                if category == 'custom_derived_factors':
                    # å¤„ç†è‡ªå®šä¹‰è¡ç”Ÿå› å­
                    for subcategory, subfactors in factors.items():
                        if isinstance(subfactors, dict):
                            factor_fields.extend(list(subfactors.keys()))
                elif isinstance(factors, list):
                    factor_fields.extend(factors)
            
            # å»é‡å¹¶æ’åºï¼Œæ’é™¤ç´¢å¼•å­—æ®µ
            factor_fields = sorted(list(set(factor_fields)))
            
            # æ’é™¤éå› å­å­—æ®µ
            exclude_fields = {'trade_date', 'ts_code', '_id'}
            factor_fields = [f for f in factor_fields if f not in exclude_fields]
            
            self.logger.info(f"ğŸ“Š åŠ è½½å› å­å­—æ®µ: {len(factor_fields)}ä¸ª (å·²æ’é™¤ç´¢å¼•å­—æ®µ)")
            basic_count = len([f for f in factor_config.get('basic_factors', []) if f in factor_fields])
            valuation_count = len([f for f in factor_config.get('valuation_factors', []) if f in factor_fields])
            liquidity_count = len([f for f in factor_config.get('liquidity_factors', []) if f in factor_fields])
            technical_count = len([f for f in factor_config.get('technical_factors', []) if f in factor_fields])
            self.logger.info(f"   â”œâ”€ åŸºç¡€å› å­: {basic_count}ä¸ª")
            self.logger.info(f"   â”œâ”€ ä¼°å€¼å› å­: {valuation_count}ä¸ª") 
            self.logger.info(f"   â”œâ”€ æµåŠ¨æ€§å› å­: {liquidity_count}ä¸ª")
            self.logger.info(f"   â”œâ”€ æŠ€æœ¯æŒ‡æ ‡å› å­: {technical_count}ä¸ª")
            
            # ç»Ÿè®¡è‡ªå®šä¹‰å› å­
            custom_count = 0
            if 'custom_derived_factors' in factor_config:
                for subcategory, subfactors in factor_config['custom_derived_factors'].items():
                    if isinstance(subfactors, dict):
                        custom_count += len(subfactors)
            self.logger.info(f"   â””â”€ è‡ªå®šä¹‰è¡ç”Ÿå› å­: {custom_count}ä¸ª")
            
            # éªŒè¯æ•°æ®åº“ä¸­å®é™…å­˜åœ¨çš„å› å­
            validated_factors = self._validate_database_factors(factor_fields)
            
            return validated_factors
            
        except Exception as e:
            self.logger.error(f"âŒ é…ç½®æ–‡ä»¶å› å­åŠ è½½å¤±è´¥: {e}")
            # å›é€€åˆ°JSONæ–‡ä»¶åŠ è½½
            return self._load_from_json_backup()
    
    def _validate_database_factors(self, factor_fields: List[str]) -> List[str]:
        """éªŒè¯æ•°æ®åº“ä¸­å®é™…å­˜åœ¨çš„å› å­å­—æ®µï¼Œåªä½¿ç”¨stock_factor_proé›†åˆ"""
        try:
            # åªä»stock_factor_proé›†åˆè·å–å­—æ®µ
            collection = self.db_handler.get_collection('stock_factor_pro')
            sample_doc = collection.find_one({}, {'_id': 0})
            if not sample_doc:
                self.logger.warning("âš ï¸ æ•°æ®åº“ä¸­æ²¡æœ‰å› å­æ•°æ®")
                return []
            
            # è·å–æ•°æ®åº“ä¸­å®é™…å­˜åœ¨çš„å­—æ®µ
            db_fields = set(sample_doc.keys()) - {'trade_date', 'ts_code', '_id'}
            self.logger.info(f"ğŸ“Š stock_factor_pro: {len(db_fields)}ä¸ªå› å­")
            
            # åˆ†ç¦»æ•°æ®åº“å› å­å’Œè‡ªå®šä¹‰è¡ç”Ÿå› å­
            db_factors = []
            custom_factors = []
            missing_factors = []
            
            for factor in factor_fields:
                if factor in db_fields:
                    db_factors.append(factor)
                elif self._is_custom_derived_factor(factor):
                    custom_factors.append(factor)
                else:
                    missing_factors.append(factor)
            
            self.logger.info(f"ğŸ“Š å› å­éªŒè¯ç»“æœ:")
            self.logger.info(f"   âœ… æ•°æ®åº“å› å­: {len(db_factors)}ä¸ª")
            self.logger.info(f"   ğŸ”§ è‡ªå®šä¹‰å› å­: {len(custom_factors)}ä¸ª")
            self.logger.info(f"   âŒ æœªçŸ¥å› å­: {len(missing_factors)}ä¸ª")
            
            if missing_factors:
                self.logger.warning(f"   æœªçŸ¥å› å­å‰10ä¸ª: {missing_factors[:10]}")
            
            # è¿”å›æ•°æ®åº“å› å­ + è‡ªå®šä¹‰å› å­
            return db_factors + custom_factors
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®åº“å› å­éªŒè¯å¤±è´¥: {e}")
            return factor_fields  # å¤±è´¥æ—¶è¿”å›åŸåˆ—è¡¨
    
    def _is_custom_derived_factor(self, factor_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºè‡ªå®šä¹‰è¡ç”Ÿå› å­"""
        try:
            custom_factors_config = self.config.get('factor_config', {}).get('factor_categories', {}).get('custom_derived_factors', {})
            
            for subcategory, subfactors in custom_factors_config.items():
                if isinstance(subfactors, dict) and factor_name in subfactors:
                    return True
            return False
        except:
            return False
    
    def _load_from_json_backup(self) -> List[str]:
        """ä»JSONæ–‡ä»¶åŠ è½½æŠ€æœ¯å› å­ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ"""
        try:
            factor_file = os.path.join(
                os.path.dirname(__file__), 
                "../../../backtrader_strategies/docs/stock_factor_pro_fields_analysis.json"
            )
            
            import json
            with open(factor_file, 'r', encoding='utf-8') as f:
                factor_analysis = json.load(f)
            
            # æå–æ‰€æœ‰å› å­å­—æ®µå
            factor_fields = []
            # æ ¹æ®å®é™…JSONæ–‡ä»¶ç»“æ„æå–å› å­å­—æ®µ
            factor_categories = ['emotion_fields', 'trend_fields', 'other_fields']
            for category in factor_categories:
                fields = factor_analysis.get(category, [])
                if isinstance(fields, list):
                    factor_fields.extend(fields)
            
            # å»é‡å¹¶æ’åº
            factor_fields = sorted(list(set(factor_fields)))
            
            self.logger.warning(f"âš ï¸ ä½¿ç”¨JSONå¤‡ç”¨æ–¹æ¡ˆï¼Œä»…åŠ è½½æŠ€æœ¯å› å­: {len(factor_fields)}ä¸ª")
            return factor_fields
            
        except Exception as e:
            self.logger.error(f"âŒ JSONå¤‡ç”¨æ–¹æ¡ˆä¹Ÿå¤±è´¥: {e}")
            # æœ€åå¤‡ç”¨æ–¹æ¡ˆ
            return self._get_backup_factors()
    
    def _get_backup_factors(self) -> List[str]:
        """è·å–å¤‡ç”¨å› å­åˆ—è¡¨"""
        backup_factors = []
        factor_config = self.config['factor_config']['factor_categories']
        
        for category, factors in factor_config.items():
            if isinstance(factors, list):
                backup_factors.extend(factors)
        
        return backup_factors
    
    def load_a500_universe(self, date: str = None) -> List[str]:
        """
        åŠ è½½A500æŒ‡æ•°æˆåˆ†è‚¡åˆ—è¡¨
        
        Args:
            date: æŒ‡å®šæ—¥æœŸï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°æ—¥æœŸ
            
        Returns:
            A500æˆåˆ†è‚¡ä»£ç åˆ—è¡¨
        """
        try:
            collection = self.db_handler.get_collection('index_weight')
            index_code = self.config['data_config']['index_code']
            
            # æŸ¥è¯¢æ¡ä»¶
            query = {'index_code': index_code}
            if date:
                query['trade_date'] = date
            
            # è·å–æœ€æ–°æ—¥æœŸçš„æˆåˆ†è‚¡
            cursor = collection.find(query).sort('trade_date', -1).limit(500)
            
            stock_codes = []
            latest_date = None
            
            for doc in cursor:
                current_date = doc.get('trade_date')
                if latest_date is None:
                    latest_date = current_date
                elif current_date != latest_date:
                    break
                
                if 'con_code' in doc and doc['con_code']:
                    stock_codes.append(doc['con_code'])
            
            stock_codes = sorted(list(set(stock_codes)))
            self.logger.info(f"ğŸ“Š åŠ è½½A500æˆåˆ†è‚¡: {len(stock_codes)}åªï¼Œæ—¥æœŸ: {latest_date}")
            
            return stock_codes
            
        except Exception as e:
            self.logger.error(f"âŒ A500æˆåˆ†è‚¡åŠ è½½å¤±è´¥: {e}")
            return []
    
    def load_factor_data(self, 
                        stock_codes: List[str], 
                        start_date: str, 
                        end_date: str) -> pd.DataFrame:
        """
        åŠ è½½å› å­æ•°æ®
        
        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            å› å­æ•°æ®DataFrame
        """
        try:
            self.logger.info(f"ğŸ“Š å¼€å§‹åŠ è½½å› å­æ•°æ®: {len(stock_codes)}åªè‚¡ç¥¨ï¼Œ{start_date} to {end_date}")
            
            # 1. åŠ è½½åŸå§‹å› å­æ•°æ®ï¼ˆæœªæ ‡å‡†åŒ–ï¼‰
            raw_data = self._load_primary_factors_raw(stock_codes, start_date, end_date)
            
            if raw_data.empty:
                self.logger.error("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•å› å­æ•°æ®")
                return pd.DataFrame()
            
            # 2. è®¡ç®—è‡ªå®šä¹‰è¡ç”Ÿå› å­ï¼ˆä½¿ç”¨åŸå§‹æ•°æ®ï¼‰
            custom_data = self._calculate_custom_factors(raw_data, stock_codes, start_date, end_date)
            
            # 3. åˆå¹¶è‡ªå®šä¹‰å› å­
            if not custom_data.empty:
                combined_data = raw_data.join(custom_data, how='outer')
                self.logger.info(f"ğŸ“Š åˆå¹¶æ•°æ®: æ•°æ®åº“{raw_data.shape[1]}ä¸ª + è‡ªå®šä¹‰{custom_data.shape[1]}ä¸ª = {combined_data.shape[1]}ä¸ª")
            else:
                combined_data = raw_data
                self.logger.info(f"ğŸ“Š æœ€ç»ˆæ•°æ®: {raw_data.shape[1]}ä¸ª (æ— è‡ªå®šä¹‰å› å­)")
            
            # 4. å¯¹åˆå¹¶åçš„æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼ˆåŒ…æ‹¬æ ‡å‡†åŒ–ï¼‰
            final_data = self._preprocess_factor_data(combined_data)
            self.logger.info(f"ğŸ“Š æ•°æ®åº“å› å­æ•°æ®: {final_data.shape[1]}ä¸ªå› å­")
            
            return final_data
            
        except Exception as e:
            self.logger.error(f"âŒ å› å­æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _load_primary_factors_raw(self, stock_codes: List[str], start_date: str, end_date: str) -> pd.DataFrame:
        """åŠ è½½åŸå§‹å› å­æ•°æ® (stock_factor_pro) - ä¸è¿›è¡Œæ ‡å‡†åŒ–"""
        try:
            collection = self.db_handler.get_collection('stock_factor_pro')
            
            # æ‰¹é‡æŸ¥è¯¢ - ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹å¤„ç†å¤§å°
            all_data = []
            base_batch_size = self.config.get('factor_config', {}).get('factor_analysis', {}).get('batch_size', 50)
            
            # æ ¹æ®ç¡¬ä»¶æ€§èƒ½ä¼˜åŒ–æ‰¹å¤§å°
            if self.device_manager and hasattr(self.device_manager, 'get_optimal_batch_size'):
                try:
                    batch_size = self.device_manager.get_optimal_batch_size(
                        base_batch_size, 
                        len(self.factor_fields), 
                        len(stock_codes)
                    )
                except Exception as e:
                    self.logger.warning(f"âš ï¸ æ‰¹å¤§å°ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
                    batch_size = base_batch_size
            else:
                batch_size = base_batch_size
            
            for i in range(0, len(stock_codes), batch_size):
                batch_stocks = stock_codes[i:i+batch_size]
                
                # è½¬æ¢æ—¥æœŸæ ¼å¼ (YYYY-MM-DD -> YYYYMMDD)
                start_date_str = start_date.replace('-', '')
                end_date_str = end_date.replace('-', '')
                
                query = {
                    'ts_code': {'$in': batch_stocks},
                    'trade_date': {
                        '$gte': start_date_str,
                        '$lte': end_date_str
                    }
                }
                
                # æŠ•å½± - åªè·å–éœ€è¦çš„å­—æ®µ
                projection = {'_id': 0, 'ts_code': 1, 'trade_date': 1}
                for factor in self.factor_fields:
                    projection[factor] = 1
                
                cursor = collection.find(query, projection)
                batch_data = list(cursor)
                all_data.extend(batch_data)
                
                self.logger.info(f"  æ‰¹æ¬¡ {i//batch_size + 1}: åŠ è½½äº† {len(batch_data)} æ¡è®°å½•")
            
            if not all_data:
                self.logger.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•å› å­æ•°æ®")
                return pd.DataFrame()
            
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(all_data)
            
            # é‡å‘½åå­—æ®µä»¥ä¿æŒä¸€è‡´æ€§
            if 'ts_code' in df.columns:
                df = df.rename(columns={'ts_code': 'stock_code'})
            
            # åªåšåŸºæœ¬çš„æ•°æ®æ¸…ç†ï¼Œä¸è¿›è¡Œæ ‡å‡†åŒ–
            df = self._basic_data_cleanup(df)
            
            self.logger.info(f"âœ… åŸå§‹å› å­æ•°æ®åŠ è½½å®Œæˆ: {df.shape}")
            return df
            
        except Exception as e:
            self.logger.error(f"âŒ åŸå§‹å› å­æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _load_primary_factors(self, stock_codes: List[str], start_date: str, end_date: str) -> pd.DataFrame:
        """åŠ è½½ä¸»è¦å› å­æ•°æ® (stock_factor_pro)"""
        try:
            collection = self.db_handler.get_collection('stock_factor_pro')
            
            # æ‰¹é‡æŸ¥è¯¢ - ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹å¤„ç†å¤§å°
            all_data = []
            base_batch_size = self.config.get('factor_config', {}).get('factor_analysis', {}).get('batch_size', 50)
            
            # æ ¹æ®ç¡¬ä»¶æ€§èƒ½ä¼˜åŒ–æ‰¹å¤§å°
            if self.device_manager and hasattr(self.device_manager, 'get_optimal_batch_size'):
                try:
                    batch_size = self.device_manager.get_optimal_batch_size(
                        base_batch_size, 
                        len(self.factor_fields), 
                        len(stock_codes)
                    )
                except Exception as e:
                    self.logger.warning(f"âš ï¸ æ‰¹å¤§å°ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
                    batch_size = base_batch_size
            else:
                batch_size = base_batch_size
            
            for i in range(0, len(stock_codes), batch_size):
                batch_stocks = stock_codes[i:i+batch_size]
                
                # è½¬æ¢æ—¥æœŸæ ¼å¼ (YYYY-MM-DD -> YYYYMMDD)
                start_date_str = start_date.replace('-', '')
                end_date_str = end_date.replace('-', '')
                
                query = {
                    'ts_code': {'$in': batch_stocks},
                    'trade_date': {
                        '$gte': start_date_str,
                        '$lte': end_date_str
                    }
                }
                
                # æŠ•å½± - åªè·å–éœ€è¦çš„å­—æ®µ
                projection = {'_id': 0, 'ts_code': 1, 'trade_date': 1}
                for factor in self.factor_fields:
                    projection[factor] = 1
                
                cursor = collection.find(query, projection)
                batch_data = list(cursor)
                all_data.extend(batch_data)
                
                self.logger.info(f"  æ‰¹æ¬¡ {i//batch_size + 1}: åŠ è½½äº† {len(batch_data)} æ¡è®°å½•")
            
            if not all_data:
                self.logger.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•å› å­æ•°æ®")
                return pd.DataFrame()
            
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(all_data)
            
            # é‡å‘½åå­—æ®µä»¥ä¿æŒä¸€è‡´æ€§
            if 'ts_code' in df.columns:
                df = df.rename(columns={'ts_code': 'stock_code'})
            
            # æ•°æ®é¢„å¤„ç†
            df = self._preprocess_factor_data(df)
            
            self.logger.info(f"âœ… ä¸»è¦å› å­æ•°æ®åŠ è½½å®Œæˆ: {df.shape}")
            return df
            
        except Exception as e:
            self.logger.error(f"âŒ ä¸»è¦å› å­æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _load_mario_factors(self, stock_codes: List[str], start_date: str, end_date: str) -> pd.DataFrame:
        """åŠ è½½Marioå› å­æ•°æ®"""
        try:
            mario_data_list = []
            
            # åŠ è½½é«˜ä¼˜å…ˆçº§Marioå› å­
            high_priority_data = self._load_mario_collection('mario_factors_high_priority', stock_codes, start_date, end_date)
            if not high_priority_data.empty:
                mario_data_list.append(high_priority_data)
            
            # åŠ è½½ä¸­ä¼˜å…ˆçº§Marioå› å­
            medium_priority_data = self._load_mario_collection('mario_factors_medium_priority', stock_codes, start_date, end_date)
            if not medium_priority_data.empty:
                mario_data_list.append(medium_priority_data)
            
            if not mario_data_list:
                self.logger.warning("âš ï¸ æœªåŠ è½½åˆ°ä»»ä½•Marioå› å­æ•°æ®")
                return pd.DataFrame()
            
            # åˆå¹¶Marioå› å­æ•°æ®
            mario_df = mario_data_list[0]
            for i in range(1, len(mario_data_list)):
                mario_df = mario_df.join(mario_data_list[i], how='outer')
            
            self.logger.info(f"âœ… Marioå› å­æ•°æ®åŠ è½½å®Œæˆ: {mario_df.shape}")
            return mario_df
            
        except Exception as e:
            self.logger.error(f"âŒ Marioå› å­æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _load_mario_collection(self, collection_name: str, stock_codes: List[str], start_date: str, end_date: str) -> pd.DataFrame:
        """åŠ è½½å•ä¸ªMarioå› å­é›†åˆ"""
        try:
            collection = self.db_handler.get_collection(collection_name)
            
            # è½¬æ¢æ—¥æœŸæ ¼å¼
            start_date_str = start_date.replace('-', '')
            end_date_str = end_date.replace('-', '')
            
            query = {
                'ts_code': {'$in': stock_codes},
                'trade_date': {
                    '$gte': start_date_str,
                    '$lte': end_date_str
                }
            }
            
            # è·å–æ‰€æœ‰æ•°æ®
            cursor = collection.find(query, {'_id': 0})
            data_list = list(cursor)
            
            if not data_list:
                self.logger.warning(f"âš ï¸ {collection_name} é›†åˆä¸­æ²¡æœ‰æ•°æ®")
                return pd.DataFrame()
            
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(data_list)
            
            # é‡å‘½åå­—æ®µä»¥ä¿æŒä¸€è‡´æ€§
            if 'ts_code' in df.columns:
                df = df.rename(columns={'ts_code': 'stock_code'})
            
            # è®¾ç½®ç´¢å¼•
            if 'trade_date' in df.columns and 'stock_code' in df.columns:
                df['trade_date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d')
                df = df.set_index(['trade_date', 'stock_code'])
            
            self.logger.info(f"ğŸ“Š {collection_name}: åŠ è½½äº† {df.shape} æ•°æ®")
            return df
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ {collection_name} åŠ è½½å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _preprocess_factor_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        å› å­æ•°æ®é¢„å¤„ç†
        
        Args:
            df: åŸå§‹å› å­æ•°æ®
            
        Returns:
            é¢„å¤„ç†åçš„å› å­æ•°æ®
        """
        try:
            if df.empty:
                return df
            
            # è®¾ç½®ç´¢å¼•ï¼ˆå¦‚æœè¿˜æ²¡æœ‰è®¾ç½®ï¼‰
            if not isinstance(df.index, pd.MultiIndex):
                # å¤„ç†æ—¥æœŸæ ¼å¼ï¼šYYYYMMDD -> YYYY-MM-DD
                if 'trade_date' in df.columns:
                    df['trade_date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d')
                if 'trade_date' in df.columns and 'stock_code' in df.columns:
                    df = df.set_index(['trade_date', 'stock_code'])
            
            # æ•°æ®æ¸…æ´—
            preprocessing_config = self.config['factor_config']['preprocessing']
            
            # å¤„ç†å¼‚å¸¸å€¼
            df = self._handle_outliers(df, preprocessing_config)
            
            # æ•°æ®å¡«å……
            fill_method = preprocessing_config.get('fill_method', 'forward')
            if fill_method == 'forward':
                df = df.fillna(method='ffill')
            elif fill_method == 'backward':
                df = df.fillna(method='bfill')
            elif fill_method == 'interpolate':
                df = df.interpolate()
            
            # æ ‡å‡†åŒ– - å¯¹æ‰€æœ‰æ•°å€¼åˆ—è¿›è¡Œæ ‡å‡†åŒ–ï¼Œæ’é™¤ç´¢å¼•åˆ—
            factor_columns = [col for col in df.columns if col not in ['trade_date', 'stock_code', '_id']]
            if factor_columns:
                # åœ¨æ ‡å‡†åŒ–å‰å¤„ç†æ— ç©·å¤§å€¼å’Œæå€¼
                df = self._clean_numeric_data(df, factor_columns)
                df[factor_columns] = self.scaler.fit_transform(df[factor_columns])
            
            self.logger.info(f"ğŸ“Š å› å­æ•°æ®é¢„å¤„ç†å®Œæˆ: {df.shape}")
            return df
            
        except Exception as e:
            self.logger.error(f"âŒ å› å­æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            return df
    
    def _handle_outliers(self, df: pd.DataFrame, config: Dict) -> pd.DataFrame:
        """å¤„ç†å¼‚å¸¸å€¼"""
        method = config.get('outlier_method', 'mad')
        threshold = config.get('outlier_threshold', 3)
        
        factor_columns = [col for col in df.columns if col in self.factor_fields]
        
        for col in factor_columns:
            if col not in df.columns:
                continue
                
            if method == 'mad':
                # MADæ–¹æ³•
                median = df[col].median()
                mad = np.median(np.abs(df[col] - median))
                lower = median - threshold * mad
                upper = median + threshold * mad
            elif method == 'iqr':
                # IQRæ–¹æ³•
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower = Q1 - threshold * IQR
                upper = Q3 + threshold * IQR
            else:  # zscore
                # Z-scoreæ–¹æ³•
                mean = df[col].mean()
                std = df[col].std()
                lower = mean - threshold * std
                upper = mean + threshold * std
            
            # æˆªæ–­å¤„ç†
            df[col] = df[col].clip(lower=lower, upper=upper)
        
        return df
    
    def _clean_numeric_data(self, df: pd.DataFrame, factor_columns: List[str]) -> pd.DataFrame:
        """
        æ¸…ç†æ•°å€¼æ•°æ®ï¼Œå¤„ç†æ— ç©·å¤§å€¼å’Œæå€¼
        
        Args:
            df: æ•°æ®DataFrame
            factor_columns: éœ€è¦æ¸…ç†çš„å› å­åˆ—
            
        Returns:
            æ¸…ç†åçš„DataFrame
        """
        try:
            for col in factor_columns:
                if col not in df.columns:
                    continue
                
                # 1. æ›¿æ¢æ— ç©·å¤§å€¼ä¸ºNaN
                df[col] = df[col].replace([np.inf, -np.inf], np.nan)
                
                # 2. å¤„ç†æå€¼ï¼ˆè¶…å‡ºfloat64å®‰å…¨èŒƒå›´çš„å€¼ï¼‰
                # float64çš„å®‰å…¨èŒƒå›´å¤§çº¦æ˜¯ -1.7e308 åˆ° 1.7e308
                safe_max = 1e100  # ä½¿ç”¨æ›´ä¿å®ˆçš„é˜ˆå€¼
                safe_min = -1e100
                
                # å°†è¶…å‡ºå®‰å…¨èŒƒå›´çš„å€¼è®¾ä¸ºNaN
                df.loc[df[col] > safe_max, col] = np.nan
                df.loc[df[col] < safe_min, col] = np.nan
                
                # 3. æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ— æ•ˆå€¼
                if pd.isna(df[col]).all():
                    # å¦‚æœæ•´åˆ—éƒ½æ˜¯æ— æ•ˆå€¼ï¼Œç”¨0å¡«å……
                    df[col] = 0.0
                    self.logger.warning(f"âš ï¸ å› å­ {col} å…¨éƒ¨ä¸ºæ— æ•ˆå€¼ï¼Œå·²ç”¨0å¡«å……")
                elif pd.isna(df[col]).any():
                    # æœ‰éƒ¨åˆ†æ— æ•ˆå€¼ï¼Œç”¨ä¸­ä½æ•°å¡«å……
                    median_value = df[col].median()
                    if pd.isna(median_value):
                        median_value = 0.0
                    df[col] = df[col].fillna(median_value)
                    invalid_ratio = pd.isna(df[col]).sum() / len(df[col])
                    if invalid_ratio > 0.1:  # å¦‚æœè¶…è¿‡10%çš„å€¼æ— æ•ˆï¼Œè®°å½•è­¦å‘Š
                        self.logger.warning(f"âš ï¸ å› å­ {col} æœ‰ {invalid_ratio:.1%} çš„æ— æ•ˆå€¼å·²è¢«å¡«å……")
                
                # 4. æœ€ç»ˆæ£€æŸ¥æ•°æ®ç±»å‹
                if not pd.api.types.is_numeric_dtype(df[col]):
                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0)
            
            return df
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°å€¼æ•°æ®æ¸…ç†å¤±è´¥: {e}")
            # å¦‚æœæ¸…ç†å¤±è´¥ï¼Œè‡³å°‘ç¡®ä¿æ²¡æœ‰æ— ç©·å¤§å€¼
            for col in factor_columns:
                if col in df.columns:
                    df[col] = df[col].replace([np.inf, -np.inf], 0.0)
            return df
    
    def calculate_forward_returns(self, 
                                stock_codes: List[str],
                                start_date: str, 
                                end_date: str) -> pd.DataFrame:
        """
        è®¡ç®—å‰ç»æ”¶ç›Šç‡
        
        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            å‰ç»æ”¶ç›Šç‡DataFrame
        """
        try:
            self.logger.info(f"ğŸ“Š å¼€å§‹è®¡ç®—å‰ç»æ”¶ç›Šç‡")
            
            collection = self.db_handler.get_collection('stock_factor_pro')
            
            # è½¬æ¢æ—¥æœŸæ ¼å¼ (YYYY-MM-DD -> YYYYMMDD)
            start_date_str = start_date.replace('-', '')
            end_date_str = end_date.replace('-', '')
            
            # æŸ¥è¯¢ä»·æ ¼æ•°æ®
            query = {
                'ts_code': {'$in': stock_codes},
                'trade_date': {
                    '$gte': start_date_str,
                    '$lte': end_date_str
                }
            }
            
            projection = {
                '_id': 0,
                'ts_code': 1,
                'trade_date': 1,
                'close': 1,
                'close_qfq': 1  # ä½¿ç”¨å‰å¤æƒä»·æ ¼
            }
            
            cursor = collection.find(query, projection).sort([('ts_code', 1), ('trade_date', 1)])
            price_data = list(cursor)
            
            if not price_data:
                return pd.DataFrame()
            
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(price_data)
            # å¤„ç†æ—¥æœŸæ ¼å¼ï¼šYYYYMMDD -> YYYY-MM-DD
            df['trade_date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d')
            # é‡å‘½åå­—æ®µä»¥ä¿æŒä¸€è‡´æ€§
            if 'ts_code' in df.columns:
                df = df.rename(columns={'ts_code': 'stock_code'})
            
            # ä½¿ç”¨å¤æƒä»·æ ¼ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æ”¶ç›˜ä»·
            if 'close_qfq' in df.columns and df['close_qfq'].notna().any():
                price_col = 'close_qfq'
            else:
                price_col = 'close'
            
            # è®¡ç®—å„å‘¨æœŸå‰ç»æ”¶ç›Šç‡
            forward_periods = self.config['data_config']['forward_returns']
            return_data = []
            
            for stock in stock_codes:
                stock_df = df[df['stock_code'] == stock].copy()
                if stock_df.empty:
                    continue
                
                stock_df = stock_df.sort_values('trade_date')
                stock_df = stock_df.set_index('trade_date')
                
                for period in forward_periods:
                    # è®¡ç®—å‰ç»æ”¶ç›Šç‡
                    return_col = f'return_{period}d'
                    stock_df[return_col] = stock_df[price_col].pct_change(periods=period).shift(-period)
                
                # é‡ç½®ç´¢å¼•å¹¶æ·»åŠ è‚¡ç¥¨ä»£ç 
                stock_df = stock_df.reset_index()
                stock_df['stock_code'] = stock
                return_data.append(stock_df)
            
            if not return_data:
                return pd.DataFrame()
            
            result_df = pd.concat(return_data, ignore_index=True)
            
            # è®¾ç½®å¤šé‡ç´¢å¼•
            result_df = result_df.set_index(['trade_date', 'stock_code'])
            
            self.logger.info(f"âœ… å‰ç»æ”¶ç›Šç‡è®¡ç®—å®Œæˆ: {result_df.shape}")
            return result_df
            
        except Exception as e:
            self.logger.error(f"âŒ å‰ç»æ”¶ç›Šç‡è®¡ç®—å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def calculate_factor_ic(self, 
                          factor_data: pd.DataFrame, 
                          return_data: pd.DataFrame,
                          return_period: int = 20) -> Dict[str, FactorAnalysisResult]:
        """
        è®¡ç®—å› å­ICæŒ‡æ ‡
        
        Args:
            factor_data: å› å­æ•°æ®
            return_data: æ”¶ç›Šç‡æ•°æ®
            return_period: æ”¶ç›Šç‡å‘¨æœŸ
            
        Returns:
            å› å­åˆ†æç»“æœå­—å…¸
        """
        try:
            self.logger.info(f"ğŸ“Š å¼€å§‹è®¡ç®—å› å­ICæŒ‡æ ‡ï¼Œå‘¨æœŸ: {return_period}å¤©")
            
            return_col = f'return_{return_period}d'
            if return_col not in return_data.columns:
                self.logger.error(f"âŒ æ”¶ç›Šç‡åˆ— {return_col} ä¸å­˜åœ¨")
                return {}
            
            results = {}
            # åªä½¿ç”¨å®é™…å­˜åœ¨äºæ•°æ®ä¸­çš„å› å­å­—æ®µï¼Œæ’é™¤éå› å­åˆ—
            exclude_cols = {'trade_date', 'ts_code', 'stock_code', '_id'}
            factor_columns = [col for col in factor_data.columns 
                            if col not in exclude_cols and 
                            col in self.factor_fields and
                            col != return_col]
            
            self.logger.info(f"ğŸ“Š å¯ç”¨å› å­æ•°é‡: {len(factor_columns)}/{len(self.factor_fields)}")
            
            if not factor_columns:
                self.logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„å› å­æ•°æ®è¿›è¡ŒICè®¡ç®—")
                return {}
            
            # åˆå¹¶æ•°æ®
            merged_data = factor_data.join(return_data[return_col], how='inner')
            
            # ä½¿ç”¨GPUåŠ é€ŸICè®¡ç®—
            if self.gpu_accelerator and len(factor_columns) > 10:
                try:
                    # æ‰¹é‡GPUè®¡ç®—
                    factor_df = merged_data[factor_columns]
                    return_df = merged_data[[return_col]]
                    
                    ic_results = self.gpu_accelerator.accelerated_ic_calculation(
                        factor_df, return_df
                    )
                    
                    # è½¬æ¢ä¸ºFactorAnalysisResultæ ¼å¼
                    for factor in factor_columns:
                        if factor in ic_results.index:
                            ic_value = ic_results.loc[factor, return_col]
                            # åˆ›å»ºç®€åŒ–çš„ç»“æœå¯¹è±¡
                            result = FactorAnalysisResult(
                                factor_name=factor,
                                ic_mean=ic_value,
                                ic_std=0.0,  # GPUæ‰¹é‡è®¡ç®—æš‚ä¸è®¡ç®—è¿™äº›ç»Ÿè®¡é‡
                                ic_ir=0.0,
                                rank_ic=0.0,
                                t_stat=0.0,
                                p_value=0.0,
                                significance=abs(ic_value) > 0.02,
                                turnover=0.0,
                                sharpe_ratio=0.0,
                                max_drawdown=0.0,
                                analysis_date=datetime.now(),
                                sample_size=len(merged_data)
                            )
                            results[factor] = result
                    
                    self.logger.info(f"âš¡ GPUåŠ é€ŸICè®¡ç®—å®Œæˆ: {len(results)}ä¸ªå› å­")
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ GPUåŠ é€Ÿå¤±è´¥ï¼Œå›é€€åˆ°CPUè®¡ç®—: {e}")
                    # å›é€€åˆ°åŸå§‹CPUè®¡ç®—
                    for factor in factor_columns:
                        try:
                            result = self._analyze_single_factor(
                                merged_data, factor, return_col
                            )
                            if result:
                                results[factor] = result
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ å› å­ {factor} åˆ†æå¤±è´¥: {e}")
                            continue
            else:
                # CPUè®¡ç®— (å°æ‰¹é‡æˆ–æ— GPU)
                for factor in factor_columns:
                    try:
                        result = self._analyze_single_factor(
                            merged_data, factor, return_col
                        )
                        if result:
                            results[factor] = result
                            
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ å› å­ {factor} åˆ†æå¤±è´¥: {e}")
                        continue
            
            self.logger.info(f"âœ… å› å­ICè®¡ç®—å®Œæˆï¼Œæœ‰æ•ˆå› å­: {len(results)}")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ å› å­ICè®¡ç®—å¤±è´¥: {e}")
            return {}
    
    def _analyze_single_factor(self, 
                              data: pd.DataFrame, 
                              factor_name: str, 
                              return_col: str) -> Optional[FactorAnalysisResult]:
        """
        åˆ†æå•ä¸ªå› å­
        
        Args:
            data: åˆå¹¶çš„æ•°æ®
            factor_name: å› å­åç§°
            return_col: æ”¶ç›Šç‡åˆ—å
            
        Returns:
            å› å­åˆ†æç»“æœ
        """
        try:
            # è·å–æœ‰æ•ˆæ•°æ®
            valid_data = data[[factor_name, return_col]].dropna()
            
            if len(valid_data) < 30:  # æœ€å°‘éœ€è¦30ä¸ªè§‚æµ‹å€¼
                return None
            
            factor_values = valid_data[factor_name]
            return_values = valid_data[return_col]
            
            # è®¡ç®—ICæŒ‡æ ‡
            ic_corr, ic_pvalue = pearsonr(factor_values, return_values)
            rank_ic, _ = spearmanr(factor_values, return_values)
            
            # æ—¶é—´åºåˆ—ICåˆ†æ
            ic_series = self._calculate_rolling_ic(data, factor_name, return_col)
            
            if len(ic_series) < 10:
                return None
            
            ic_mean = np.mean(ic_series)
            ic_std = np.std(ic_series)
            ic_ir = ic_mean / ic_std if ic_std > 0 else 0
            
            # tæ£€éªŒ
            t_stat, p_value = stats.ttest_1samp(ic_series, 0)
            significance = p_value < 0.05
            
            # è®¡ç®—å…¶ä»–æŒ‡æ ‡
            turnover = self._calculate_factor_turnover(data, factor_name)
            sharpe_ratio = self._calculate_factor_sharpe(ic_series)
            max_drawdown = self._calculate_max_drawdown(ic_series)
            
            return FactorAnalysisResult(
                factor_name=factor_name,
                ic_mean=ic_mean,
                ic_std=ic_std,
                ic_ir=ic_ir,
                rank_ic=rank_ic,
                t_stat=t_stat,
                p_value=p_value,
                significance=significance,
                turnover=turnover,
                sharpe_ratio=sharpe_ratio,
                max_drawdown=max_drawdown,
                analysis_date=datetime.now(),
                sample_size=len(valid_data)
            )
            
        except Exception as e:
            self.logger.error(f"âŒ å› å­ {factor_name} åˆ†æå¤±è´¥: {e}")
            return None
    
    def _calculate_rolling_ic(self, 
                             data: pd.DataFrame, 
                             factor_name: str, 
                             return_col: str,
                             window: int = 60) -> np.ndarray:
        """è®¡ç®—æ»šåŠ¨IC"""
        try:
            # æŒ‰æ—¥æœŸåˆ†ç»„è®¡ç®—æ¯æ—¥IC
            daily_ic = []
            dates = data.index.get_level_values(0).unique().sort_values()
            
            for date in dates:
                try:
                    date_data = data.xs(date, level=0)[[factor_name, return_col]].dropna()
                    
                    if len(date_data) < 10:
                        continue
                    
                    ic, _ = pearsonr(date_data[factor_name], date_data[return_col])
                    if not np.isnan(ic):
                        daily_ic.append(ic)
                        
                except Exception:
                    continue
            
            return np.array(daily_ic)
            
        except Exception as e:
            self.logger.error(f"âŒ æ»šåŠ¨ICè®¡ç®—å¤±è´¥: {e}")
            return np.array([])
    
    def _calculate_factor_turnover(self, data: pd.DataFrame, factor_name: str) -> float:
        """è®¡ç®—å› å­æ¢æ‰‹ç‡"""
        try:
            # ç®€åŒ–çš„æ¢æ‰‹ç‡è®¡ç®—
            return 0.3  # æš‚æ—¶è¿”å›å›ºå®šå€¼
        except:
            return 0.0
    
    def _calculate_factor_sharpe(self, ic_series: np.ndarray) -> float:
        """è®¡ç®—å› å­å¤æ™®æ¯”ç‡"""
        try:
            if len(ic_series) == 0:
                return 0.0
            return np.mean(ic_series) / np.std(ic_series) if np.std(ic_series) > 0 else 0.0
        except:
            return 0.0
    
    def _calculate_max_drawdown(self, ic_series: np.ndarray) -> float:
        """è®¡ç®—æœ€å¤§å›æ’¤"""
        try:
            if len(ic_series) == 0:
                return 0.0
            
            cumulative = np.cumsum(ic_series)
            running_max = np.maximum.accumulate(cumulative)
            drawdown = (cumulative - running_max)
            return float(np.min(drawdown))
        except:
            return 0.0
    
    def rank_factors(self, 
                    analysis_results: Dict[str, FactorAnalysisResult],
                    ranking_method: str = 'ic_ir') -> List[Tuple[str, FactorAnalysisResult]]:
        """
        å› å­æ’åº
        
        Args:
            analysis_results: å› å­åˆ†æç»“æœ
            ranking_method: æ’åºæ–¹æ³• (ic_ir, ic_mean, significance)
            
        Returns:
            æ’åºåçš„å› å­åˆ—è¡¨
        """
        try:
            if ranking_method == 'ic_ir':
                sorted_factors = sorted(
                    analysis_results.items(),
                    key=lambda x: abs(x[1].ic_ir),
                    reverse=True
                )
            elif ranking_method == 'ic_mean':
                sorted_factors = sorted(
                    analysis_results.items(),
                    key=lambda x: abs(x[1].ic_mean),
                    reverse=True
                )
            elif ranking_method == 'significance':
                # å…ˆæŒ‰æ˜¾è‘—æ€§æ’åºï¼Œå†æŒ‰IC_IRæ’åº
                sorted_factors = sorted(
                    analysis_results.items(),
                    key=lambda x: (x[1].significance, abs(x[1].ic_ir)),
                    reverse=True
                )
            else:
                sorted_factors = list(analysis_results.items())
            
            self.logger.info(f"ğŸ“Š å› å­æ’åºå®Œæˆï¼Œæ–¹æ³•: {ranking_method}, å› å­æ•°: {len(sorted_factors)}")
            return sorted_factors
            
        except Exception as e:
            self.logger.error(f"âŒ å› å­æ’åºå¤±è´¥: {e}")
            return list(analysis_results.items())
    
    def run_factor_analysis(self, 
                           start_date: str = None, 
                           end_date: str = None) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„å› å­åˆ†ææµç¨‹
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        try:
            self.logger.info("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´å› å­åˆ†ææµç¨‹")
            
            # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤æ—¥æœŸ
            if start_date is None:
                start_date = self.config['data_config']['train_period']['start_date']
            if end_date is None:
                end_date = self.config['data_config']['train_period']['end_date']
            
            # 1. åŠ è½½A500æˆåˆ†è‚¡
            stock_codes = self.load_a500_universe()
            if not stock_codes:
                raise ValueError("æœªèƒ½åŠ è½½A500æˆåˆ†è‚¡")
            
            # 2. åŠ è½½å› å­æ•°æ®
            factor_data = self.load_factor_data(stock_codes, start_date, end_date)
            if factor_data.empty:
                raise ValueError("æœªèƒ½åŠ è½½å› å­æ•°æ®")
            
            # 3. è®¡ç®—å‰ç»æ”¶ç›Šç‡
            return_data = self.calculate_forward_returns(stock_codes, start_date, end_date)
            if return_data.empty:
                raise ValueError("æœªèƒ½è®¡ç®—å‰ç»æ”¶ç›Šç‡")
            
            # 4. è®¡ç®—å› å­ICæŒ‡æ ‡
            forward_periods = self.config['data_config']['forward_returns']
            all_results = {}
            
            for period in forward_periods:
                self.logger.info(f"ğŸ“Š åˆ†æ {period} å¤©å‰ç»æ”¶ç›Šç‡")
                period_results = self.calculate_factor_ic(factor_data, return_data, period)
                all_results[f'{period}d'] = period_results
            
            # 5. å› å­æ’åº
            ranking_results = {}
            for period, results in all_results.items():
                ranking_results[period] = self.rank_factors(results)
            
            # 6. ä¿å­˜ç»“æœ
            self.analysis_results = {
                'factor_analysis': all_results,
                'factor_ranking': ranking_results,
                'metadata': {
                    'start_date': start_date,
                    'end_date': end_date,
                    'stock_count': len(stock_codes),
                    'factor_count': len(self.factor_fields),
                    'analysis_date': datetime.now().isoformat()
                }
            }
            
            self.logger.info("âœ… å› å­åˆ†ææµç¨‹å®Œæˆ")
            return self.analysis_results
            
        except Exception as e:
            self.logger.error(f"âŒ å› å­åˆ†ææµç¨‹å¤±è´¥: {e}")
            return {}
    
    def get_top_factors(self, 
                       period: str = '20d', 
                       top_k: int = 50) -> List[str]:
        """
        è·å–Topå› å­åˆ—è¡¨
        
        Args:
            period: æ”¶ç›Šç‡å‘¨æœŸ
            top_k: å–å‰Kä¸ªå› å­
            
        Returns:
            Topå› å­åç§°åˆ—è¡¨
        """
        try:
            if not self.analysis_results:
                self.logger.warning("âš ï¸ è¯·å…ˆè¿è¡Œå› å­åˆ†æ")
                return []
            
            ranking = self.analysis_results.get('factor_ranking', {}).get(period, [])
            top_factors = [name for name, _ in ranking[:top_k]]
            
            self.logger.info(f"ğŸ“Š è·å–Top{top_k}å› å­: {period}å‘¨æœŸ")
            return top_factors
            
        except Exception as e:
            self.logger.error(f"âŒ è·å–Topå› å­å¤±è´¥: {e}")
            return []


    def _calculate_custom_factors(self, base_data: pd.DataFrame, stock_codes: List[str], start_date: str, end_date: str) -> pd.DataFrame:
        """
        è®¡ç®—è‡ªå®šä¹‰è¡ç”Ÿå› å­
        
        Args:
            base_data: åŸºç¡€å› å­æ•°æ®ï¼ˆåŒ…å«OHLCVå’ŒæŠ€æœ¯æŒ‡æ ‡ï¼‰
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            è‡ªå®šä¹‰å› å­æ•°æ®DataFrame
        """
        if not self.custom_factor_calculator:
            self.logger.warning("âš ï¸ è‡ªå®šä¹‰å› å­è®¡ç®—å™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡è‡ªå®šä¹‰å› å­è®¡ç®—")
            return pd.DataFrame()
        
        try:
            custom_data_list = []
            
            # ä¸ºæ¯åªè‚¡ç¥¨è®¡ç®—è‡ªå®šä¹‰å› å­
            for stock_code in stock_codes:
                # è·å–è¯¥è‚¡ç¥¨çš„æ•°æ® (ç´¢å¼•æ˜¯ [trade_date, stock_code])
                try:
                    stock_data = base_data.xs(stock_code, level='stock_code')
                except KeyError:
                    self.logger.debug(f"ğŸ“Š è‚¡ç¥¨{stock_code}åœ¨å½“å‰æ—¶é—´æ®µæ•°æ®ä¸å­˜åœ¨ï¼Œè·³è¿‡è‡ªå®šä¹‰å› å­è®¡ç®—")
                    continue
                
                if stock_data.empty:
                    continue
                
                # ç¡®ä¿æœ‰å¿…è¦çš„OHLCVå­—æ®µ
                required_fields = ['open', 'high', 'low', 'close', 'vol']
                if not all(field in stock_data.columns for field in required_fields):
                    self.logger.warning(f"âš ï¸ è‚¡ç¥¨{stock_code}ç¼ºå°‘å¿…è¦çš„OHLCVå­—æ®µï¼Œè·³è¿‡è‡ªå®šä¹‰å› å­è®¡ç®—")
                    continue
                
                # è®¡ç®—è‡ªå®šä¹‰å› å­
                custom_factors = self.custom_factor_calculator.calculate_all_custom_factors(stock_data)
                
                if custom_factors.empty:
                    continue
                
                # é‡å»ºMultiIndex: [trade_date, stock_code]
                custom_factors['stock_code'] = stock_code
                custom_factors = custom_factors.reset_index()
                custom_factors = custom_factors.set_index(['trade_date', 'stock_code'])
                
                custom_data_list.append(custom_factors)
            
            if custom_data_list:
                # åˆå¹¶æ‰€æœ‰è‚¡ç¥¨çš„è‡ªå®šä¹‰å› å­æ•°æ®
                custom_data = pd.concat(custom_data_list, axis=0)
                successful_stocks = len(custom_data_list)
                missing_stocks = len(stock_codes) - successful_stocks
                self.logger.info(f"ğŸ”§ è‡ªå®šä¹‰å› å­è®¡ç®—å®Œæˆ: {custom_data.shape[1]}ä¸ªå› å­ï¼Œ{successful_stocks}åªè‚¡ç¥¨æˆåŠŸ")
                if missing_stocks > 0:
                    self.logger.debug(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {missing_stocks}åªè‚¡ç¥¨åœ¨å½“å‰æ—¶é—´æ®µæ— æ•°æ® (æ€»å…±{len(stock_codes)}åª)")
                return custom_data
            else:
                self.logger.warning("âš ï¸ æ²¡æœ‰è®¡ç®—å‡ºä»»ä½•è‡ªå®šä¹‰å› å­")
                return pd.DataFrame()
                
        except Exception as e:
            self.logger.error(f"âŒ è‡ªå®šä¹‰å› å­è®¡ç®—å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _basic_data_cleanup(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åŸºæœ¬æ•°æ®æ¸…ç†ï¼Œä¸åŒ…å«æ ‡å‡†åŒ–
        
        Args:
            df: åŸå§‹æ•°æ®DataFrame
            
        Returns:
            æ¸…ç†åçš„DataFrame
        """
        try:
            # è½¬æ¢æ—¥æœŸæ ¼å¼
            if 'trade_date' in df.columns:
                df['trade_date'] = pd.to_datetime(df['trade_date'].astype(str), format='%Y%m%d')
            
            # è®¾ç½®MultiIndex
            if 'trade_date' in df.columns and 'stock_code' in df.columns:
                df = df.set_index(['trade_date', 'stock_code'])
            
            # è½¬æ¢æ•°å€¼ç±»å‹
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            for col in numeric_columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # åˆ é™¤å®Œå…¨ä¸ºç©ºçš„åˆ—
            df = df.dropna(how='all', axis=1)
            
            return df
            
        except Exception as e:
            self.logger.error(f"âŒ åŸºæœ¬æ•°æ®æ¸…ç†å¤±è´¥: {e}")
            return df

