#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çœŸå®è‚¡ç¥¨æ•°æ®æµ‹è¯•
ä½¿ç”¨é¡¹ç›®ä¸­çœŸå®çš„MongoDBè‚¡ç¥¨æ•°æ®è¿›è¡Œç¼ è®ºæ¨¡å—æµ‹è¯•

æ•°æ®æ¥æºï¼š
- stock_kline_daily: 720ä¸‡+æ¡æ—¥çº¿æ•°æ® (2019-2025)
- stock_kline_5min: 5åˆ†é’ŸKçº¿æ•°æ®
- stock_kline_30min: 30åˆ†é’ŸKçº¿æ•°æ®
"""

import unittest
import sys
import os
from datetime import datetime, timedelta
from typing import List, Dict, Any
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(project_root)

# æ·»åŠ ç¼ è®ºæ¨¡å—è·¯å¾„
chan_theory_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(chan_theory_root)

try:
    # å¯¼å…¥æ•°æ®åº“å¤„ç†å™¨
    from database.db_handler import get_db_handler
    
    # å¯¼å…¥ç¼ è®ºæ¨¡å—
    from models.kline import KLine, KLineList
    from models.fenxing import FenXing, FenXingList
    from models.bi import Bi, BiList, BiBuilder, BiConfig
    from models.enums import TimeLevel, BiDirection
    from core.kline_processor import KlineProcessor
    from config.chan_config import ChanConfig
    
    MODULES_AVAILABLE = True
except Exception as e:
    print(f"å¯¼å…¥å¤±è´¥: {e}")
    MODULES_AVAILABLE = False


class TestRealStockDataIntegration(unittest.TestCase):
    """æµ‹è¯•çœŸå®è‚¡ç¥¨æ•°æ®é›†æˆ"""
    
    @classmethod
    def setUpClass(cls):
        """ç±»çº§åˆ«çš„è®¾ç½®ï¼Œåªæ‰§è¡Œä¸€æ¬¡"""
        if not MODULES_AVAILABLE:
            raise unittest.SkipTest("æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œè·³è¿‡æ‰€æœ‰æµ‹è¯•")
        
        try:
            cls.db_handler = get_db_handler()
            cls.db = cls.db_handler.db
            cls.config = ChanConfig()
            cls.processor = KlineProcessor(cls.config)
            
            # æµ‹è¯•æ•°æ®åº“è¿æ¥
            cls.db.command('ping')
            print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
            
        except Exception as e:
            raise unittest.SkipTest(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
    
    
    def test_database_connection_and_collections(self):
        """æµ‹è¯•æ•°æ®åº“è¿æ¥å’Œé›†åˆå­˜åœ¨æ€§"""
        # æ£€æŸ¥å…³é”®é›†åˆæ˜¯å¦å­˜åœ¨
        collections = self.db.list_collection_names()
        
        key_collections = ['stock_kline_daily', 'stock_kline_5min', 'stock_kline_30min']
        existing_collections = [col for col in key_collections if col in collections]
        
        print(f"ğŸ“Š å‘ç°è‚¡ç¥¨Kçº¿é›†åˆ: {existing_collections}")
        
        # è‡³å°‘è¦æœ‰æ—¥çº¿æ•°æ®
        self.assertIn('stock_kline_daily', collections, "ç¼ºå°‘stock_kline_dailyé›†åˆ")
        
        # æ£€æŸ¥æ•°æ®é‡
        daily_count = self.db['stock_kline_daily'].count_documents({})
        self.assertGreater(daily_count, 1000000, f"æ—¥çº¿æ•°æ®é‡ä¸è¶³: {daily_count}")
        print(f"ğŸ“ˆ æ—¥çº¿æ•°æ®é‡: {daily_count:,} æ¡")
    
    def test_load_real_daily_kline_data(self):
        """æµ‹è¯•åŠ è½½çœŸå®æ—¥çº¿Kçº¿æ•°æ®"""
        collection = self.db['stock_kline_daily']
        
        # è·å–ä¸€ä¸ªæœ‰è¶³å¤Ÿæ•°æ®çš„è‚¡ç¥¨ä»£ç 
        stock_code = "000001.SZ"  # å¹³å®‰é“¶è¡Œï¼Œåº”è¯¥æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®
        
        # æŸ¥è¯¢æœ€è¿‘200æ ¹Kçº¿ï¼ˆå…ˆé™åºè·å–æœ€æ–°æ•°æ®ï¼Œç„¶ååè½¬ä¸ºå‡åºï¼‰
        query = {'ts_code': stock_code}
        cursor = collection.find(query).sort('trade_date', -1).limit(200)
        raw_data = list(cursor)
        # åè½¬ä¸ºå‡åºï¼ˆä»æ—§åˆ°æ–°ï¼‰
        raw_data.reverse()
        
        self.assertGreater(len(raw_data), 100, f"è‚¡ç¥¨ {stock_code} æ•°æ®é‡ä¸è¶³: {len(raw_data)}")
        print(f"ğŸ“Š è·å– {stock_code} æ•°æ®: {len(raw_data)} æ¡")
        
        # æ£€æŸ¥æ•°æ®ç»“æ„
        sample = raw_data[0]
        required_fields = ['trade_date', 'ts_code', 'open', 'high', 'low', 'close', 'vol']
        for field in required_fields:
            self.assertIn(field, sample, f"ç¼ºå°‘å­—æ®µ: {field}")
        
        print(f"âœ… æ•°æ®å­—æ®µå®Œæ•´: {list(sample.keys())}")
        
        # è½¬æ¢ä¸ºç¼ è®ºæ ¼å¼
        converted_data = self._convert_daily_data_to_chan_format(raw_data)
        self.assertGreater(len(converted_data), 50, "è½¬æ¢åæ•°æ®ä¸è¶³")
        
        # åˆ›å»ºKLineList
        kline_list = KLineList.from_mongo_data(converted_data, TimeLevel.DAILY)
        self.assertEqual(len(kline_list), len(converted_data))
        self.assertEqual(kline_list.level, TimeLevel.DAILY)
        print(f"âœ… åˆ›å»ºKLineListæˆåŠŸ: {len(kline_list)} æ ¹Kçº¿")
        
        return kline_list, stock_code
    
    def test_process_real_kline_data(self):
        """æµ‹è¯•å¤„ç†çœŸå®Kçº¿æ•°æ®ï¼ˆåŒ…å«åˆ†å‹è¯†åˆ«ï¼‰"""
        # åŠ è½½æ•°æ®
        kline_list, stock_code = self.test_load_real_daily_kline_data()
        
        print(f"\nâš™ï¸  å¼€å§‹å¤„ç† {stock_code} çš„Kçº¿æ•°æ®...")
        
        # è®°å½•å¤„ç†æ—¶é—´
        start_time = time.time()
        processed_klines, fenxings = self.processor.process_klines(kline_list)
        processing_time = time.time() - start_time
        
        # éªŒè¯å¤„ç†ç»“æœ
        self.assertIsInstance(processed_klines, KLineList)
        self.assertIsInstance(fenxings, FenXingList)
        self.assertGreater(len(processed_klines), 0, "å¤„ç†åKçº¿æ•°æ®ä¸ºç©º")
        self.assertLessEqual(len(processed_klines), len(kline_list), "å¤„ç†åKçº¿æ•°æ®ä¸åº”å¢åŠ ")
        
        # è·å–å¤„ç†ç»Ÿè®¡
        stats = self.processor.get_processing_statistics(kline_list, processed_klines, fenxings)
        print(f"ğŸ“ˆ å¤„ç†ç»Ÿè®¡:")
        print(f"  - åŸå§‹Kçº¿: {stats['original_count']} æ ¹")
        print(f"  - å¤„ç†åKçº¿: {stats['processed_count']} æ ¹")
        print(f"  - åˆå¹¶æ•°é‡: {stats['reduction_count']} æ ¹")
        print(f"  - åˆå¹¶æ¯”ä¾‹: {stats['reduction_ratio']:.2%}")
        print(f"  - åˆ†å‹æ•°é‡: {stats['fenxing_count']} ä¸ª")
        print(f"  - åˆ†å‹æ¯”ä¾‹: {stats['fenxing_ratio']:.2%}")
        print(f"  - å¤„ç†è€—æ—¶: {processing_time:.3f} ç§’")
        print(f"  - å¤„ç†é€Ÿåº¦: {len(kline_list)/processing_time:.0f} æ ¹/ç§’")
        
        # æ•°æ®è´¨é‡éªŒè¯
        errors = self.processor.validate_processed_klines(processed_klines)
        if errors:
            print(f"âš ï¸  æ•°æ®è´¨é‡é—®é¢˜ ({len(errors)} ä¸ª):")
            for i, error in enumerate(errors[:3]):
                print(f"    {i+1}. {error}")
        else:
            print("âœ… æ•°æ®è´¨é‡éªŒè¯é€šè¿‡")
        
        # åˆ†å‹éªŒè¯
        fenxing_errors = self._validate_fenxings(fenxings)
        if fenxing_errors:
            print(f"âš ï¸  åˆ†å‹è´¨é‡é—®é¢˜ ({len(fenxing_errors)} ä¸ª):")
            for i, error in enumerate(fenxing_errors[:3]):
                print(f"    {i+1}. {error}")
        else:
            print("âœ… åˆ†å‹è´¨é‡éªŒè¯é€šè¿‡")
        
        # æ€§èƒ½æ–­è¨€
        self.assertLess(processing_time, 1.0, "å¤„ç†æ—¶é—´è¿‡é•¿")
        self.assertEqual(len(errors), 0, f"æ•°æ®è´¨é‡éªŒè¯å¤±è´¥: {errors}")
        self.assertEqual(len(fenxing_errors), 0, f"åˆ†å‹è´¨é‡éªŒè¯å¤±è´¥: {fenxing_errors}")
        
        # æ˜¾ç¤ºæ ·æœ¬æ•°æ®
        self._display_sample_klines(processed_klines, stock_code)
        self._display_sample_fenxings(fenxings, stock_code)
        
        return processed_klines, fenxings
    
    def test_fenxing_identification_detailed(self):
        """è¯¦ç»†æµ‹è¯•åˆ†å‹è¯†åˆ«åŠŸèƒ½"""
        # åŠ è½½æ•°æ®
        kline_list, stock_code = self.test_load_real_daily_kline_data()
        
        print(f"\nğŸ” è¯¦ç»†åˆ†å‹è¯†åˆ«æµ‹è¯• - {stock_code}")
        
        # å¤„ç†Kçº¿å¹¶è¯†åˆ«åˆ†å‹
        processed_klines, fenxings = self.processor.process_klines(kline_list)
        
        # è¯¦ç»†éªŒè¯åˆ†å‹
        if not fenxings.is_empty():
            print(f"ğŸ“Š åˆ†å‹ç»Ÿè®¡ä¿¡æ¯:")
            stats = fenxings.get_statistics()
            for key, value in stats.items():
                if isinstance(value, tuple):
                    if key == 'price_range':
                        print(f"  - {key}: {value[0]:.2f} - {value[1]:.2f}")
                    elif key == 'time_range' and value[0] and value[1]:
                        print(f"  - {key}: {value[0].strftime('%Y-%m-%d')} - {value[1].strftime('%Y-%m-%d')}")
                elif isinstance(value, float):
                    print(f"  - {key}: {value:.3f}")
                else:
                    print(f"  - {key}: {value}")
            
            # æ£€éªŒåˆ†å‹è´¨é‡
            tops = fenxings.get_tops()
            bottoms = fenxings.get_bottoms()
            confirmed = fenxings.get_confirmed()
            
            print(f"\nğŸ“ˆ åˆ†å‹è¯¦æƒ…:")
            print(f"  - é¡¶åˆ†å‹: {len(tops)} ä¸ª")
            print(f"  - åº•åˆ†å‹: {len(bottoms)} ä¸ª") 
            print(f"  - å·²ç¡®è®¤: {len(confirmed)} ä¸ª")
            print(f"  - ç¡®è®¤ç‡: {len(confirmed) / len(fenxings) * 100:.1f}%")
            
            # æ˜¾ç¤ºåˆ†å‹å¼ºåº¦åˆ†å¸ƒ
            strengths = [f.strength for f in fenxings]
            if strengths:
                print(f"  - å¼ºåº¦èŒƒå›´: {min(strengths):.4f} - {max(strengths):.4f}")
                print(f"  - å¹³å‡å¼ºåº¦: {sum(strengths) / len(strengths):.4f}")
        
        # éªŒè¯åˆ†å‹è´¨é‡
        self.assertGreaterEqual(len(fenxings), 0, "åº”è¯¥è‡³å°‘è¯†åˆ«å‡ºä¸€äº›åˆ†å‹")
        if len(fenxings) > 0:
            self.assertGreater(len(fenxings.get_tops()), 0, "åº”è¯¥æœ‰é¡¶åˆ†å‹")
            self.assertGreater(len(fenxings.get_bottoms()), 0, "åº”è¯¥æœ‰åº•åˆ†å‹")
        
        return fenxings
    
    def test_bi_construction_from_real_data(self):
        """æµ‹è¯•ä»çœŸå®æ•°æ®æ„å»ºç¬”"""
        # åŠ è½½æ•°æ®å¹¶è¯†åˆ«åˆ†å‹
        kline_list, stock_code = self.test_load_real_daily_kline_data()
        processed_klines, fenxings = self.processor.process_klines(kline_list)
        
        print(f"\nğŸ“ ç¬”æ„å»ºæµ‹è¯• - {stock_code}")
        
        if fenxings.is_empty():
            print("  âš ï¸  æ— åˆ†å‹æ•°æ®ï¼Œè·³è¿‡ç¬”æ„å»ºæµ‹è¯•")
            return
        
        # ä½¿ç”¨ä¸åŒçš„ç¬”é…ç½®è¿›è¡Œæµ‹è¯•
        test_configs = [
            ("ä¸¥æ ¼æ¨¡å¼", BiConfig(fx_check_mode="strict", require_confirmation=False, min_amplitude_ratio=0.001)),
            ("å®½æ¾æ¨¡å¼", BiConfig(fx_check_mode="loss", require_confirmation=False, min_amplitude_ratio=0.001)),
            ("åŠä¸¥æ ¼æ¨¡å¼", BiConfig(fx_check_mode="half", require_confirmation=False, min_amplitude_ratio=0.001)),
            ("å®Œå…¨æ¨¡å¼", BiConfig(fx_check_mode="totally", require_confirmation=False, min_amplitude_ratio=0.001))
        ]
        
        results = {}
        for config_name, bi_config in test_configs:
            print(f"\n  ğŸ”§ æµ‹è¯•{config_name}...")
            
            start_time = time.time()
            bi_list = BiList.from_fenxings(fenxings.fenxings, bi_config, TimeLevel.DAILY, processed_klines.klines)
            construction_time = time.time() - start_time
            
            # éªŒè¯ç¬”æ„å»ºç»“æœ
            errors = bi_list.validate_sequence()
            chan_violations = bi_list.validate_chan_theory_rules()
            
            results[config_name] = {
                'bi_count': len(bi_list),
                'construction_time': construction_time,
                'sequence_errors': len(errors),
                'chan_violations': len(chan_violations),
                'statistics': bi_list.get_statistics()
            }
            
            print(f"    ğŸ“Š æ„å»ºç¬”æ•°: {len(bi_list)} ä¸ª")
            print(f"    â±ï¸  æ„å»ºè€—æ—¶: {construction_time:.4f} ç§’")
            print(f"    âœ… åºåˆ—é”™è¯¯: {len(errors)} ä¸ª")
            print(f"    ğŸ“ ç¼ è®ºè¿è§„: {len(chan_violations)} ä¸ª")
            
            if errors:
                print(f"    âš ï¸  åºåˆ—é”™è¯¯è¯¦æƒ…: {errors[:3]}")
            if chan_violations:
                print(f"    âš ï¸  ç¼ è®ºè¿è§„è¯¦æƒ…: {chan_violations[:3]}")
            
            # æ˜¾ç¤ºç¬”ç»Ÿè®¡ä¿¡æ¯
            stats = bi_list.get_statistics()
            if stats['total_count'] > 0:
                print(f"    ğŸ“ˆ å‘ä¸Šç¬”: {stats['up_count']} ä¸ª")
                print(f"    ğŸ“‰ å‘ä¸‹ç¬”: {stats['down_count']} ä¸ª")
                print(f"    ğŸ“Š å¹³å‡å¹…åº¦: {stats['avg_amplitude']:.3%}")
                print(f"    ğŸ’ª å¹³å‡å¼ºåº¦: {stats['avg_strength']:.3f}")
                print(f"    ğŸ¯ å¹³å‡çº¯åº¦: {stats['avg_purity']:.3f}")
        
        # æ˜¾ç¤ºæœ€ä½³é…ç½®ç»“æœ
        best_config = max(results.keys(), key=lambda k: results[k]['bi_count'] - results[k]['sequence_errors'] - results[k]['chan_violations'])
        print(f"\n  ğŸ† æœ€ä½³é…ç½®: {best_config}")
        print(f"    ğŸ“Š ç¬”æ•°: {results[best_config]['bi_count']}")
        print(f"    âœ… è´¨é‡å¾—åˆ†: {results[best_config]['bi_count'] - results[best_config]['sequence_errors'] - results[best_config]['chan_violations']}")
        
        # è¯¦ç»†å±•ç¤ºæœ€ä½³é…ç½®çš„ç¬”
        best_bi_list = BiList.from_fenxings(fenxings.fenxings, test_configs[list(results.keys()).index(best_config)][1], TimeLevel.DAILY, processed_klines.klines)
        self._display_sample_bis(best_bi_list, stock_code)
        
        # æµ‹è¯•é«˜çº§åˆ†æåŠŸèƒ½
        self._test_bi_advanced_analysis(best_bi_list, stock_code)
        
        # åŸºæœ¬æ–­è¨€
        self.assertGreater(len(best_bi_list), 0, "åº”è¯¥æ„å»ºå‡ºè‡³å°‘ä¸€æ ¹ç¬”")
        self.assertEqual(results[best_config]['sequence_errors'], 0, f"æœ€ä½³é…ç½®ä¸åº”è¯¥æœ‰åºåˆ—é”™è¯¯: {results[best_config]}")
        
        return best_bi_list
    
    def test_bi_construction_algorithm_fix(self):
        """æµ‹è¯•ä¿®å¤åçš„ç¬”æ„å»ºç®—æ³•ï¼ˆé‡ç‚¹éªŒè¯è¿ç»­åˆ†å‹å¤„ç†ï¼‰"""
        print(f"\nğŸ”§ ç¬”æ„å»ºç®—æ³•ä¿®å¤éªŒè¯æµ‹è¯•")
        
        # åŠ è½½çœŸå®æ•°æ®
        kline_list, stock_code = self.test_load_real_daily_kline_data()
        processed_klines, fenxings = self.processor.process_klines(kline_list)
        
        if fenxings.is_empty():
            print("  âš ï¸  æ— åˆ†å‹æ•°æ®ï¼Œè·³è¿‡ç¬”æ„å»ºç®—æ³•éªŒè¯")
            return
        
        print(f"  ğŸ“Š æµ‹è¯•æ•°æ®: {stock_code}, {len(fenxings)}ä¸ªåˆ†å‹")
        
        # åˆ›å»ºæµ‹è¯•ç”¨çš„BiBuilderå®ä¾‹
        bi_config = BiConfig(fx_check_mode="loss", require_confirmation=False, min_amplitude_ratio=0.001)
        builder = BiBuilder(bi_config)
        
        # æµ‹è¯•è¿ç»­åˆ†å‹ä¼˜åŒ–ç®—æ³•ï¼ˆè¿™æ˜¯ä¿®å¤çš„é‡ç‚¹ï¼‰
        print(f"\n  ğŸ¯ æµ‹è¯•è¿ç»­åˆ†å‹å¤„ç†ç®—æ³•...")
        
        # åˆ†æåŸå§‹åˆ†å‹åºåˆ—ä¸­çš„è¿ç»­æƒ…å†µ
        consecutive_stats = self._analyze_consecutive_fenxings(fenxings.fenxings)
        print(f"    åŸå§‹åˆ†å‹åˆ†æ:")
        print(f"      - è¿ç»­é¡¶åˆ†å‹ç»„: {consecutive_stats['consecutive_top_groups']} ç»„")
        print(f"      - è¿ç»­åº•åˆ†å‹ç»„: {consecutive_stats['consecutive_bottom_groups']} ç»„")
        print(f"      - æœ€å¤§è¿ç»­é•¿åº¦: {consecutive_stats['max_consecutive_length']}")
        print(f"      - éœ€è¦ä¼˜åŒ–çš„åˆ†å‹: {consecutive_stats['redundant_fenxings']} ä¸ª")
        
        # æµ‹è¯•ä¿®å¤åçš„ä¼˜åŒ–ç®—æ³•
        start_time = time.time()
        optimized_fenxings = builder._optimize_consecutive_fenxings(fenxings.fenxings)
        optimization_time = time.time() - start_time
        
        # åˆ†æä¼˜åŒ–åçš„ç»“æœ
        optimized_stats = self._analyze_consecutive_fenxings(optimized_fenxings)
        reduction_count = len(fenxings.fenxings) - len(optimized_fenxings)
        reduction_ratio = reduction_count / len(fenxings.fenxings) if len(fenxings.fenxings) > 0 else 0
        
        print(f"    ä¼˜åŒ–ç»“æœ:")
        print(f"      - ä¼˜åŒ–è€—æ—¶: {optimization_time:.4f} ç§’")
        print(f"      - åˆ†å‹æ•°é‡: {len(fenxings.fenxings)} -> {len(optimized_fenxings)}")
        print(f"      - å‡å°‘æ•°é‡: {reduction_count} ä¸ª")
        print(f"      - å‡å°‘æ¯”ä¾‹: {reduction_ratio:.1%}")
        print(f"      - è¿ç»­é¡¶åˆ†å‹ç»„: {optimized_stats['consecutive_top_groups']} ç»„")
        print(f"      - è¿ç»­åº•åˆ†å‹ç»„: {optimized_stats['consecutive_bottom_groups']} ç»„")
        print(f"      - æœ€å¤§è¿ç»­é•¿åº¦: {optimized_stats['max_consecutive_length']}")
        
        # éªŒè¯ä¿®å¤çš„æ ¸å¿ƒé€»è¾‘ï¼šè¿ç»­åˆ†å‹åº”è¯¥è¢«æ¶ˆé™¤
        self.assertEqual(optimized_stats['consecutive_top_groups'], 0, 
                        "ä¼˜åŒ–åä¸åº”è¯¥å­˜åœ¨è¿ç»­é¡¶åˆ†å‹ç»„")
        self.assertEqual(optimized_stats['consecutive_bottom_groups'], 0,
                        "ä¼˜åŒ–åä¸åº”è¯¥å­˜åœ¨è¿ç»­åº•åˆ†å‹ç»„")
        self.assertEqual(optimized_stats['max_consecutive_length'], 1,
                        "ä¼˜åŒ–åæœ€å¤§è¿ç»­é•¿åº¦åº”è¯¥ä¸º1")
        
        # éªŒè¯ç¼ è®ºæ ‡å‡†ï¼šä¿ç•™ç¬¬ä¸€ä¸ªåŸåˆ™
        if consecutive_stats['consecutive_top_groups'] > 0 or consecutive_stats['consecutive_bottom_groups'] > 0:
            print(f"    âœ… ç¼ è®ºæ ‡å‡†éªŒè¯:")
            print(f"      - è¿ç»­åˆ†å‹å·²è¢«æ­£ç¡®å¤„ç†")
            print(f"      - éµå¾ª'ä¿ç•™ç¬¬ä¸€ä¸ªï¼Œèˆå¼ƒåç»­'åŸåˆ™")
            
        # æµ‹è¯•å®Œæ•´çš„ç¬”æ„å»ºæµç¨‹
        print(f"\n  ğŸ”¨ æµ‹è¯•å®Œæ•´ç¬”æ„å»ºæµç¨‹...")
        
        start_time = time.time()
        bis = builder.build_from_fenxings(fenxings.fenxings, processed_klines.klines)
        construction_time = time.time() - start_time
        
        print(f"    æ„å»ºç»“æœ:")
        print(f"      - æ„å»ºè€—æ—¶: {construction_time:.4f} ç§’")
        print(f"      - æ„å»ºç¬”æ•°: {len(bis)} ä¸ª")
        
        if len(bis) > 0:
            # éªŒè¯ç¬”çš„è´¨é‡
            sequence_errors = self._validate_bi_sequence(bis)
            direction_errors = self._validate_bi_directions(bis)
            connection_errors = self._validate_bi_connections(bis)
            
            print(f"    è´¨é‡éªŒè¯:")
            print(f"      - åºåˆ—é”™è¯¯: {len(sequence_errors)} ä¸ª")
            print(f"      - æ–¹å‘é”™è¯¯: {len(direction_errors)} ä¸ª")
            print(f"      - è¿æ¥é”™è¯¯: {len(connection_errors)} ä¸ª")
            
            if sequence_errors:
                print(f"      - åºåˆ—é”™è¯¯è¯¦æƒ…: {sequence_errors[:3]}")
            if direction_errors:
                print(f"      - æ–¹å‘é”™è¯¯è¯¦æƒ…: {direction_errors[:3]}")
            if connection_errors:
                print(f"      - è¿æ¥é”™è¯¯è¯¦æƒ…: {connection_errors[:3]}")
            
            # æ˜¾ç¤ºæ„å»ºçš„ç¬”æ ·æœ¬
            self._display_constructed_bis_sample(bis, stock_code)
            
            # æ ¸å¿ƒéªŒè¯ï¼šç¬”æ„å»ºè´¨é‡
            self.assertEqual(len(sequence_errors), 0, f"ç¬”åºåˆ—éªŒè¯å¤±è´¥: {sequence_errors}")
            self.assertEqual(len(direction_errors), 0, f"ç¬”æ–¹å‘éªŒè¯å¤±è´¥: {direction_errors}")
            self.assertEqual(len(connection_errors), 0, f"ç¬”è¿æ¥éªŒè¯å¤±è´¥: {connection_errors}")
            
            print(f"    âœ… ç¬”æ„å»ºè´¨é‡éªŒè¯é€šè¿‡")
        
        # æ€§èƒ½è¦æ±‚éªŒè¯
        self.assertLess(optimization_time, 0.01, "åˆ†å‹ä¼˜åŒ–æ—¶é—´è¿‡é•¿")
        self.assertLess(construction_time, 0.1, "ç¬”æ„å»ºæ—¶é—´è¿‡é•¿")
        
        print(f"  ğŸ‰ ç¬”æ„å»ºç®—æ³•ä¿®å¤éªŒè¯é€šè¿‡")
        
        return bis
    
    def _analyze_consecutive_fenxings(self, fenxings: List[FenXing]) -> Dict[str, Any]:
        """åˆ†æåˆ†å‹åºåˆ—ä¸­çš„è¿ç»­æƒ…å†µ"""
        if len(fenxings) <= 1:
            return {
                'consecutive_top_groups': 0,
                'consecutive_bottom_groups': 0,
                'max_consecutive_length': 1,
                'redundant_fenxings': 0
            }
        
        consecutive_top_groups = 0
        consecutive_bottom_groups = 0
        max_consecutive_length = 1
        redundant_fenxings = 0
        
        i = 0
        while i < len(fenxings):
            current_type = fenxings[i].fenxing_type
            consecutive_count = 1
            
            # ç»Ÿè®¡è¿ç»­åŒç±»å‹åˆ†å‹
            j = i + 1
            while j < len(fenxings) and fenxings[j].fenxing_type == current_type:
                consecutive_count += 1
                j += 1
            
            # è®°å½•è¿ç»­æƒ…å†µ
            if consecutive_count > 1:
                if current_type.name == 'TOP':
                    consecutive_top_groups += 1
                else:
                    consecutive_bottom_groups += 1
                
                max_consecutive_length = max(max_consecutive_length, consecutive_count)
                redundant_fenxings += (consecutive_count - 1)  # é™¤äº†ç¬¬ä¸€ä¸ªï¼Œå…¶ä»–éƒ½æ˜¯å†—ä½™çš„
            
            i = j
        
        return {
            'consecutive_top_groups': consecutive_top_groups,
            'consecutive_bottom_groups': consecutive_bottom_groups,
            'max_consecutive_length': max_consecutive_length,
            'redundant_fenxings': redundant_fenxings
        }
    
    def _validate_bi_sequence(self, bis: List[Bi]) -> List[str]:
        """éªŒè¯ç¬”åºåˆ—çš„æœ‰æ•ˆæ€§"""
        errors = []
        
        if len(bis) <= 1:
            return errors
        
        for i in range(len(bis) - 1):
            current_bi = bis[i]
            next_bi = bis[i + 1]
            
            # æ£€æŸ¥æ—¶é—´é¡ºåº
            if current_bi.end_time > next_bi.start_time:
                errors.append(f"ç¬”{i+1}å’Œç¬”{i+2}æ—¶é—´é¡ºåºé”™è¯¯")
        
        return errors
    
    def _validate_bi_directions(self, bis: List[Bi]) -> List[str]:
        """éªŒè¯ç¬”æ–¹å‘äº¤æ›¿"""
        errors = []
        
        if len(bis) <= 1:
            return errors
        
        for i in range(len(bis) - 1):
            if bis[i].direction == bis[i + 1].direction:
                errors.append(f"ç¬”{i+1}å’Œç¬”{i+2}æ–¹å‘ç›¸åŒ({bis[i].direction.value})")
        
        return errors
    
    def _validate_bi_connections(self, bis: List[Bi]) -> List[str]:
        """éªŒè¯ç¬”è¿æ¥çš„æ­£ç¡®æ€§"""
        errors = []
        
        if len(bis) <= 1:
            return errors
        
        for i in range(len(bis) - 1):
            current_bi = bis[i]
            next_bi = bis[i + 1]
            
            # æ£€æŸ¥åˆ†å‹è¿æ¥
            if current_bi.end_fenxing != next_bi.start_fenxing:
                errors.append(f"ç¬”{i+1}å’Œç¬”{i+2}åˆ†å‹è¿æ¥é”™è¯¯")
        
        return errors
    
    def _display_constructed_bis_sample(self, bis: List[Bi], stock_code: str):
        """æ˜¾ç¤ºæ„å»ºçš„ç¬”æ ·æœ¬"""
        if not bis:
            return
        
        print(f"    ğŸ“‹ æ„å»ºç¬”æ ·æœ¬ (å‰5ä¸ª):")
        
        sample_count = min(5, len(bis))
        for i in range(sample_count):
            bi = bis[i]
            direction_icon = "ğŸ“ˆ" if bi.is_up else "ğŸ“‰"
            
            print(f"      {i+1}. {bi.start_time.strftime('%Y-%m-%d')} -> {bi.end_time.strftime('%Y-%m-%d')} "
                  f"{direction_icon} {bi.direction.value} "
                  f"{bi.start_price:.2f}->{bi.end_price:.2f} "
                  f"({bi.amplitude_ratio:+.2%}) "
                  f"å¼ºåº¦:{bi.strength:.3f} {bi.duration}æ ¹Kçº¿")
    
    def _test_bi_advanced_analysis(self, bi_list, stock_code: str):
        """æµ‹è¯•ç¬”çš„é«˜çº§åˆ†æåŠŸèƒ½"""
        print(f"\n  ğŸ” ç¬”é«˜çº§åˆ†æ - {stock_code}")
        
        # è¶‹åŠ¿å»¶ç»­æ¨¡å¼è¯†åˆ«
        trend_patterns = bi_list.find_trend_continuation_patterns()
        print(f"    ğŸ“ˆ è¶‹åŠ¿å»¶ç»­æ¨¡å¼: {len(trend_patterns)} ä¸ª")
        for i, pattern in enumerate(trend_patterns[:3]):
            print(f"      {i+1}. ç´¢å¼•{pattern['start_index']}-{pattern['end_index']} "
                  f"{pattern['direction'].value} å¼ºåº¦:{pattern['strength']:.3f}")
        
        # æ½œåœ¨åè½¬ç‚¹æ£€æµ‹
        reversal_points = bi_list.detect_potential_reversal_points()
        print(f"    ğŸ”„ æ½œåœ¨åè½¬ç‚¹: {len(reversal_points)} ä¸ª")
        for i, point in enumerate(reversal_points[:3]):
            print(f"      {i+1}. ç´¢å¼•{point['bi_index']} "
                  f"ä»·æ ¼:{point['price']:.2f} ç½®ä¿¡åº¦:{point['confidence']:.2f} "
                  f"ç±»å‹:{point['type']}")
        
        # é‡å ç¬”æ£€æµ‹
        overlapping_bis = bi_list.find_overlapping_bis(threshold=0.3)
        print(f"    ğŸ”— é‡å ç¬”å¯¹: {len(overlapping_bis)} å¯¹")
        for i, (bi1, bi2, overlap) in enumerate(overlapping_bis[:3]):
            print(f"      {i+1}. é‡å åº¦:{overlap:.2f} "
                  f"ç¬”1:{bi1.start_price:.2f}->{bi1.end_price:.2f} "
                  f"ç¬”2:{bi2.start_price:.2f}->{bi2.end_price:.2f}")
    
    def _display_sample_bis(self, bi_list, stock_code: str):
        """æ˜¾ç¤ºæ ·æœ¬ç¬”æ•°æ®"""
        if bi_list.is_empty():
            print(f"\nğŸ“‹ {stock_code} æ— ç¬”æ•°æ®")
            return
            
        print(f"\nğŸ“‹ {stock_code} æ ·æœ¬ç¬”æ•°æ® (æœ€è¿‘5æ ¹):")
        
        sample_count = min(5, len(bi_list))
        for i in range(sample_count):
            bi = bi_list[i]
            direction_icon = "ğŸ“ˆ" if bi.is_up else "ğŸ“‰"
            
            print(f"  {i+1}. {bi.start_time.strftime('%Y-%m-%d')} -> {bi.end_time.strftime('%Y-%m-%d')} "
                  f"{direction_icon} {bi.direction.value} "
                  f"{bi.start_price:.2f}->{bi.end_price:.2f} "
                  f"({bi.amplitude_ratio:+.2%}) "
                  f"å¼ºåº¦:{bi.strength:.3f} çº¯åº¦:{bi.purity:.3f} "
                  f"{bi.duration}æ ¹Kçº¿")
    
    def test_bi_performance_different_configs(self):
        """æµ‹è¯•ä¸åŒé…ç½®ä¸‹çš„ç¬”æ„å»ºæ€§èƒ½"""
        # åŠ è½½å¤§é‡æ•°æ®
        collection = self.db['stock_kline_daily']
        stock_code = "000001.SZ"
        
        # è·å–æ›´å¤šæ•°æ®æ¥æµ‹è¯•æ€§èƒ½
        query = {'ts_code': stock_code}
        cursor = collection.find(query).sort('trade_date', -1).limit(500)
        raw_data = list(cursor)
        # åè½¬ä¸ºå‡åºï¼ˆä»æ—§åˆ°æ–°ï¼‰
        raw_data.reverse()
        
        if len(raw_data) < 200:
            self.skipTest(f"æ•°æ®é‡ä¸è¶³è¿›è¡Œæ€§èƒ½æµ‹è¯•: {len(raw_data)}")
        
        # è½¬æ¢å¹¶å¤„ç†æ•°æ®
        converted_data = self._convert_daily_data_to_chan_format(raw_data)
        kline_list = KLineList.from_mongo_data(converted_data, TimeLevel.DAILY)
        processed_klines, fenxings = self.processor.process_klines(kline_list)
        
        print(f"\nâš¡ ç¬”æ„å»ºæ€§èƒ½æµ‹è¯• - {stock_code} ({len(fenxings)}ä¸ªåˆ†å‹)")
        
        # æµ‹è¯•ä¸åŒé…ç½®çš„æ€§èƒ½
        performance_configs = [
            ("ä¸¥æ ¼+ç¡®è®¤", BiConfig(fx_check_mode="strict", require_confirmation=True)),
            ("ä¸¥æ ¼+æ— ç¡®è®¤", BiConfig(fx_check_mode="strict", require_confirmation=False)),
            ("å®½æ¾+æ— ç¡®è®¤", BiConfig(fx_check_mode="loss", require_confirmation=False)),
            ("é»˜è®¤é…ç½®", BiConfig())
        ]
        
        performance_results = {}
        
        for config_name, bi_config in performance_configs:
            # å¤šæ¬¡è¿è¡Œå–å¹³å‡å€¼
            times = []
            bi_counts = []
            
            for run in range(3):
                start_time = time.time()
                bi_list = BiList.from_fenxings(fenxings.fenxings, bi_config, TimeLevel.DAILY)
                end_time = time.time()
                
                times.append(end_time - start_time)
                bi_counts.append(len(bi_list))
            
            avg_time = sum(times) / len(times)
            avg_count = sum(bi_counts) / len(bi_counts)
            
            performance_results[config_name] = {
                'avg_time': avg_time,
                'avg_count': avg_count,
                'throughput': len(fenxings) / avg_time if avg_time > 0 else 0
            }
            
            print(f"  ğŸ“Š {config_name}:")
            print(f"    â±ï¸  å¹³å‡è€—æ—¶: {avg_time:.4f} ç§’")
            print(f"    ğŸ“ å¹³å‡ç¬”æ•°: {avg_count:.1f} ä¸ª")
            print(f"    ğŸš€ å¤„ç†é€Ÿåº¦: {len(fenxings) / avg_time:.0f} åˆ†å‹/ç§’")
        
        # æ€§èƒ½æ–­è¨€
        for config_name, result in performance_results.items():
            self.assertLess(result['avg_time'], 0.1, f"{config_name} å¤„ç†æ—¶é—´è¿‡é•¿: {result['avg_time']:.4f}s")
            self.assertGreater(result['throughput'], 100, f"{config_name} å¤„ç†é€Ÿåº¦è¿‡æ…¢: {result['throughput']:.0f}")
        
        # æ‰¾å‡ºæœ€å¿«çš„é…ç½®
        fastest_config = min(performance_results.keys(), key=lambda k: performance_results[k]['avg_time'])
        print(f"\n  ğŸ† æœ€å¿«é…ç½®: {fastest_config} ({performance_results[fastest_config]['avg_time']:.4f}s)")
    
    def test_bi_data_integrity(self):
        """æµ‹è¯•ç¬”æ•°æ®å®Œæ•´æ€§"""
        # åŠ è½½æµ‹è¯•æ•°æ®
        kline_list, stock_code = self.test_load_real_daily_kline_data()
        processed_klines, fenxings = self.processor.process_klines(kline_list)
        
        if fenxings.is_empty():
            self.skipTest("æ— åˆ†å‹æ•°æ®ï¼Œè·³è¿‡ç¬”å®Œæ•´æ€§æµ‹è¯•")
        
        print(f"\nğŸ” ç¬”æ•°æ®å®Œæ•´æ€§æµ‹è¯• - {stock_code}")
        
        # æ„å»ºç¬”ï¼ˆä½¿ç”¨å®½æ¾é…ç½®ç¡®ä¿èƒ½æ„å»ºå‡ºç¬”ï¼‰
        bi_config = BiConfig(fx_check_mode="loss", require_confirmation=False, min_amplitude_ratio=0.001)
        bi_list = BiList.from_fenxings(fenxings.fenxings, bi_config, TimeLevel.DAILY, processed_klines.klines)
        
        # å®Œæ•´æ€§æ£€æŸ¥
        integrity_issues = []
        
        # 1. æ£€æŸ¥ç¬”åºåˆ—è¿æ¥æ€§
        for i in range(len(bi_list) - 1):
            current_bi = bi_list[i]
            next_bi = bi_list[i + 1]
            
            # æ£€æŸ¥åˆ†å‹è¿æ¥
            if current_bi.end_fenxing != next_bi.start_fenxing:
                integrity_issues.append(f"ç¬”{i+1}å’Œç¬”{i+2}åˆ†å‹æœªæ­£ç¡®è¿æ¥")
            
            # æ£€æŸ¥æ—¶é—´è¿ç»­æ€§ï¼ˆç¼ è®ºä¸­ç¬”ä¹‹é—´åœ¨åˆ†å‹å¤„è¿æ¥ï¼Œå…è®¸ä¸€ä¸ªKçº¿çš„é‡å ï¼‰
            # åªæœ‰å½“ç»“æŸæ—¶é—´ä¸¥æ ¼å¤§äºå¼€å§‹æ—¶é—´æ—¶æ‰ç®—æ—¶é—´é‡å é—®é¢˜
            if current_bi.end_time > next_bi.start_time:
                integrity_issues.append(f"ç¬”{i+1}å’Œç¬”{i+2}æ—¶é—´å¼‚å¸¸é‡å ")
        
        # 2. æ£€æŸ¥æ–¹å‘äº¤æ›¿
        for i in range(len(bi_list) - 1):
            if bi_list[i].direction == bi_list[i + 1].direction:
                integrity_issues.append(f"ç¬”{i+1}å’Œç¬”{i+2}æ–¹å‘ç›¸åŒ")
        
        # 3. æ£€æŸ¥ä»·æ ¼åˆç†æ€§
        for i, bi in enumerate(bi_list):
            if bi.start_price <= 0 or bi.end_price <= 0:
                integrity_issues.append(f"ç¬”{i+1}ä»·æ ¼å¼‚å¸¸: {bi.start_price} -> {bi.end_price}")
            
            if bi.amplitude_ratio < 0:
                integrity_issues.append(f"ç¬”{i+1}å¹…åº¦å¼‚å¸¸: {bi.amplitude_ratio}")
        
        # 4. æ£€æŸ¥ç»Ÿè®¡ä¸€è‡´æ€§
        stats = bi_list.get_statistics()
        expected_total = stats['up_count'] + stats['down_count']
        if expected_total != stats['total_count']:
            integrity_issues.append(f"ç»Ÿè®¡æ•°æ®ä¸ä¸€è‡´: {expected_total} != {stats['total_count']}")
        
        print(f"  ğŸ“Š å®Œæ•´æ€§æ£€æŸ¥ç»“æœ:")
        print(f"    æ€»ç¬”æ•°: {len(bi_list)}")
        print(f"    å‘ç°é—®é¢˜: {len(integrity_issues)} ä¸ª")
        
        if integrity_issues:
            print(f"  âš ï¸  å®Œæ•´æ€§é—®é¢˜:")
            for i, issue in enumerate(integrity_issues[:5]):
                print(f"    {i+1}. {issue}")
        else:
            print(f"  âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡")
        
        # æ–­è¨€ï¼šæ•°æ®å®Œæ•´æ€§
        self.assertEqual(len(integrity_issues), 0, f"æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥: {integrity_issues}")
        
        return bi_list
    
    def test_multiple_stocks_processing(self):
        """æµ‹è¯•å¤šåªè‚¡ç¥¨æ•°æ®å¤„ç†"""
        collection = self.db['stock_kline_daily']
        
        # é€‰æ‹©å‡ åªæœ‰ä»£è¡¨æ€§çš„è‚¡ç¥¨
        test_stocks = ["000001.SZ", "000002.SZ", "600000.SH", "600036.SH"]
        
        results = {}
        total_start_time = time.time()
        
        for stock_code in test_stocks:
            print(f"\nğŸ“Š å¤„ç†è‚¡ç¥¨: {stock_code}")
            
            try:
                # æŸ¥è¯¢æ•°æ®ï¼ˆç¡®ä¿æ—¶é—´é¡ºåºæ­£ç¡®ï¼‰
                query = {'ts_code': stock_code}
                cursor = collection.find(query).sort('trade_date', -1).limit(100)
                raw_data = list(cursor)
                # åè½¬ä¸ºå‡åºï¼ˆä»æ—§åˆ°æ–°ï¼‰  
                raw_data.reverse()
                
                if len(raw_data) < 50:
                    print(f"  âš ï¸  æ•°æ®ä¸è¶³ï¼Œè·³è¿‡")
                    continue
                
                # è½¬æ¢å¹¶å¤„ç†
                converted_data = self._convert_daily_data_to_chan_format(raw_data)
                kline_list = KLineList.from_mongo_data(converted_data, TimeLevel.DAILY)
                
                start_time = time.time()
                processed_klines, fenxings = self.processor.process_klines(kline_list)
                processing_time = time.time() - start_time
                
                # ç»Ÿè®¡
                stats = self.processor.get_processing_statistics(kline_list, processed_klines, fenxings)
                results[stock_code] = {
                    'original_count': stats['original_count'],
                    'processed_count': stats['processed_count'],
                    'reduction_ratio': stats['reduction_ratio'],
                    'fenxing_count': stats['fenxing_count'],
                    'processing_time': processing_time
                }
                
                print(f"  âœ… å®Œæˆ: {stats['original_count']}->{stats['processed_count']} "
                      f"({stats['reduction_ratio']:.1%} åˆå¹¶, {stats['fenxing_count']}ä¸ªåˆ†å‹, {processing_time:.3f}s)")
                
            except Exception as e:
                print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
                continue
        
        total_time = time.time() - total_start_time
        
        # æ±‡æ€»ç»“æœ
        print(f"\nğŸ“Š å¤šè‚¡ç¥¨å¤„ç†æ±‡æ€» (è€—æ—¶ {total_time:.2f}s):")
        for stock_code, result in results.items():
            print(f"  {stock_code}: {result['original_count']}->{result['processed_count']} "
                  f"({result['reduction_ratio']:.1%}, {result['fenxing_count']}ä¸ªåˆ†å‹, {result['processing_time']:.3f}s)")
        
        self.assertGreater(len(results), 0, "æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•è‚¡ç¥¨")
        
        # æ€§èƒ½è¦æ±‚
        avg_time = sum(r['processing_time'] for r in results.values()) / len(results)
        self.assertLess(avg_time, 0.5, f"å¹³å‡å¤„ç†æ—¶é—´è¿‡é•¿: {avg_time:.3f}s")
    
    def test_different_time_levels(self):
        """æµ‹è¯•ä¸åŒæ—¶é—´çº§åˆ«çš„æ•°æ®"""
        test_cases = [
            ('stock_kline_daily', TimeLevel.DAILY, 100),
            ('stock_kline_30min', TimeLevel.MIN_30, 200),
            ('stock_kline_5min', TimeLevel.MIN_5, 500)
        ]
        
        for collection_name, time_level, limit in test_cases:
            if collection_name not in self.db.list_collection_names():
                print(f"âš ï¸  è·³è¿‡ä¸å­˜åœ¨çš„é›†åˆ: {collection_name}")
                continue
                
            print(f"\nğŸ“Š æµ‹è¯• {time_level.value} çº§åˆ«æ•°æ®...")
            
            try:
                collection = self.db[collection_name]
                
                # è·å–æ ·æœ¬æ•°æ®
                cursor = collection.find().limit(limit)
                raw_data = list(cursor)
                
                if len(raw_data) < 50:
                    print(f"  âš ï¸  æ•°æ®é‡ä¸è¶³: {len(raw_data)}")
                    continue
                
                # è½¬æ¢æ•°æ®æ ¼å¼
                if time_level == TimeLevel.DAILY:
                    converted_data = self._convert_daily_data_to_chan_format(raw_data)
                else:
                    converted_data = self._convert_intraday_data_to_chan_format(raw_data)
                
                # åˆ›å»ºå’Œå¤„ç†Kçº¿
                kline_list = KLineList.from_mongo_data(converted_data, time_level)
                processed_klines, fenxings = self.processor.process_klines(kline_list)
                
                # éªŒè¯
                self.assertGreater(len(processed_klines), 0)
                self.assertEqual(processed_klines.level, time_level)
                
                print(f"  âœ… {time_level.value}: {len(kline_list)}->{len(processed_klines)} æ ¹Kçº¿, {len(fenxings)}ä¸ªåˆ†å‹")
                
            except Exception as e:
                print(f"  âŒ å¤„ç† {time_level.value} æ•°æ®å¤±è´¥: {e}")
                if "timed out" in str(e):
                    print(f"  â„¹ï¸  {collection_name} æ•°æ®é‡å¯èƒ½è¿‡å¤§ï¼ŒæŸ¥è¯¢è¶…æ—¶")
                continue
    
    def _convert_daily_data_to_chan_format(self, raw_data: List[Dict]) -> List[Dict]:
        """è½¬æ¢æ—¥çº¿æ•°æ®ä¸ºç¼ è®ºæ ¼å¼"""
        converted_data = []
        
        for item in raw_data:
            try:
                # è§£æäº¤æ˜“æ—¥æœŸ
                trade_date_str = str(item['trade_date'])
                timestamp = datetime.strptime(trade_date_str, '%Y%m%d')
                
                converted_item = {
                    'timestamp': timestamp,
                    'open': float(item['open']),
                    'high': float(item['high']),
                    'low': float(item['low']),
                    'close': float(item['close']),
                    'volume': int(float(item['vol'])),  # volå¯èƒ½æ˜¯æµ®ç‚¹æ•°
                    'amount': float(item.get('amount', 0)),
                    'symbol': item['ts_code']
                }
                
                # åŸºæœ¬æ•°æ®éªŒè¯
                if (converted_item['open'] > 0 and converted_item['high'] > 0 and 
                    converted_item['low'] > 0 and converted_item['close'] > 0):
                    converted_data.append(converted_item)
                    
            except (ValueError, KeyError) as e:
                continue  # è·³è¿‡æœ‰é—®é¢˜çš„æ•°æ®
        
        # æŒ‰æ—¶é—´æ’åº
        converted_data.sort(key=lambda x: x['timestamp'])
        return converted_data
    
    def _convert_intraday_data_to_chan_format(self, raw_data: List[Dict]) -> List[Dict]:
        """è½¬æ¢åˆ†é’Ÿçº§æ•°æ®ä¸ºç¼ è®ºæ ¼å¼"""
        converted_data = []
        
        for item in raw_data:
            try:
                # å¤„ç†åˆ†é’Ÿçº§æ•°æ®çš„æ—¶é—´æˆ³
                if 'timestamp' in item:
                    if isinstance(item['timestamp'], datetime):
                        timestamp = item['timestamp']
                    else:
                        timestamp = datetime.fromisoformat(str(item['timestamp']))
                elif 'trade_date' in item and 'trade_time' in item:
                    date_str = str(item['trade_date'])
                    time_str = str(item['trade_time'])
                    timestamp = datetime.strptime(f"{date_str} {time_str}", '%Y%m%d %H:%M:%S')
                else:
                    timestamp = datetime.now()
                
                converted_item = {
                    'timestamp': timestamp,
                    'open': float(item['open']),
                    'high': float(item['high']),
                    'low': float(item['low']),
                    'close': float(item['close']),
                    'volume': int(float(item.get('vol', item.get('volume', 1000)))),
                    'amount': float(item.get('amount', 0)),
                    'symbol': item.get('ts_code', item.get('symbol', 'UNKNOWN'))
                }
                
                if (converted_item['open'] > 0 and converted_item['high'] > 0 and 
                    converted_item['low'] > 0 and converted_item['close'] > 0):
                    converted_data.append(converted_item)
                    
            except Exception:
                continue
        
        converted_data.sort(key=lambda x: x['timestamp'])
        return converted_data
    
    def _display_sample_klines(self, klines, stock_code: str):
        """æ˜¾ç¤ºæ ·æœ¬Kçº¿æ•°æ®"""
        print(f"\nğŸ“‹ {stock_code} æ ·æœ¬Kçº¿æ•°æ® (æœ€è¿‘5æ ¹):")
        
        sample_count = min(5, len(klines))
        for i in range(sample_count):
            kline = klines[i]
            change = kline.close - kline.open
            change_pct = (change / kline.open) * 100 if kline.open > 0 else 0
            
            color = "ğŸŸ¢" if change >= 0 else "ğŸ”´"
            
            print(f"  {i+1}. {kline.timestamp.strftime('%Y-%m-%d')} "
                  f"{color} O:{kline.open:.2f} H:{kline.high:.2f} "
                  f"L:{kline.low:.2f} C:{kline.close:.2f} "
                  f"({change:+.2f}, {change_pct:+.2f}%) V:{kline.volume:,}")
    
    def _display_sample_fenxings(self, fenxings, stock_code: str):
        """æ˜¾ç¤ºæ ·æœ¬åˆ†å‹æ•°æ®"""
        if fenxings.is_empty():
            print(f"\nğŸ“‹ {stock_code} æœªå‘ç°åˆ†å‹")
            return
            
        print(f"\nğŸ“‹ {stock_code} æ ·æœ¬åˆ†å‹æ•°æ® (æœ€è¿‘5ä¸ª):")
        
        sample_count = min(5, len(fenxings))
        for i in range(sample_count):
            fenxing = fenxings[i]
            type_icon = "ğŸ”º" if fenxing.is_top else "ğŸ”»"
            
            print(f"  {i+1}. {fenxing.timestamp.strftime('%Y-%m-%d')} "
                  f"{type_icon} {fenxing.fenxing_type.value} "
                  f"ä»·æ ¼:{fenxing.price:.2f} å¼ºåº¦:{fenxing.strength:.3f} "
                  f"ç½®ä¿¡åº¦:{fenxing.confidence:.2f} "
                  f"ç¡®è®¤:{fenxing.is_confirmed} çª—å£:{fenxing.window_size}")
    
    def _validate_fenxings(self, fenxings) -> List[str]:
        """éªŒè¯åˆ†å‹æ•°æ®"""
        errors = []
        
        if fenxings.is_empty():
            return errors
        
        # æ£€æŸ¥åˆ†å‹æ—¶é—´é¡ºåº
        for i in range(1, len(fenxings)):
            if fenxings[i].timestamp <= fenxings[i-1].timestamp:
                errors.append(f"åˆ†å‹æ—¶é—´é¡ºåºé”™è¯¯: ç´¢å¼•{i}")
        
        # æ£€æŸ¥åˆ†å‹æœ‰æ•ˆæ€§ï¼ˆä½¿ç”¨å®½æ¾éªŒè¯ï¼Œå› ä¸ºåˆ†å‹æ˜¯é€šè¿‡ç®—æ³•è¯†åˆ«çš„ï¼‰
        for i, fenxing in enumerate(fenxings):
            # ä½¿ç”¨å®½æ¾æ¨¡å¼éªŒè¯ï¼Œé¿å…å› ä¸ºä¸¥æ ¼æ¨¡å¼å¯¼è‡´çš„å‡é˜³æ€§é”™è¯¯
            if not fenxing.is_valid_fenxing(min_strength=0.0, strict_mode=False):
                errors.append(f"æ— æ•ˆåˆ†å‹: ç´¢å¼•{i}, ç±»å‹{fenxing.fenxing_type}")
        
        # æ£€æŸ¥åˆ†å‹å¼ºåº¦
        for i, fenxing in enumerate(fenxings):
            if fenxing.strength < 0:
                errors.append(f"åˆ†å‹å¼ºåº¦å¼‚å¸¸: ç´¢å¼•{i}, å¼ºåº¦{fenxing.strength}")
        
        return errors

    def test_seg_construction_from_real_data(self):
        """ä½¿ç”¨çœŸå®æ•°æ®æµ‹è¯•çº¿æ®µæ„å»º"""
        print("\n=== æµ‹è¯•çº¿æ®µæ„å»ºï¼ˆçœŸå®æ•°æ®ï¼‰===")
        
        # è·å–æµ‹è¯•ç¬”æ•°æ®
        bis = self._get_test_bis()
        print(f"æµ‹è¯•ç¬”æ•°é‡: {len(bis)}")
        
        if len(bis) < 3:
            print("ç¬”æ•°é‡ä¸è¶³ï¼Œæ— æ³•æµ‹è¯•çº¿æ®µæ„å»º")
            return
        
        # æµ‹è¯•çº¿æ®µæ„å»º
        start_time = time.time()
        seg_list = SegList.from_bis(bis)
        end_time = time.time()
        
        processing_time = end_time - start_time
        print(f"çº¿æ®µæ„å»ºè€—æ—¶: {processing_time:.3f}s")
        print(f"æ„å»ºçº¿æ®µæ•°é‡: {len(seg_list)}")
        
        if len(seg_list) > 0:
            # ç»Ÿè®¡ä¿¡æ¯
            stats = seg_list.get_statistics()
            print(f"çº¿æ®µç»Ÿè®¡ä¿¡æ¯:")
            print(f"  - æ€»è®¡: {stats['total_count']} çº¿æ®µ")
            print(f"  - å‘ä¸Š: {stats['up_count']} çº¿æ®µ")
            print(f"  - å‘ä¸‹: {stats['down_count']} çº¿æ®µ")
            print(f"  - æœ‰æ•ˆ: {stats['valid_count']} çº¿æ®µ")
            print(f"  - å¹³å‡å¹…åº¦: {stats['avg_amplitude']:.3%}")
            print(f"  - å¹³å‡å¼ºåº¦: {stats['avg_strength']:.3f}")
            print(f"  - æ½œåœ¨ä¸­æ¢: {stats['potential_zhongshus']} ä¸ª")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªçº¿æ®µçš„è¯¦ç»†ä¿¡æ¯
            print(f"\nå‰5ä¸ªçº¿æ®µè¯¦æƒ…:")
            for i, seg in enumerate(seg_list[:5]):
                print(f"  çº¿æ®µ{i+1}: {seg}")
        
        self.assertGreaterEqual(len(seg_list), 0, "åº”è¯¥èƒ½æ„å»ºå‡ºçº¿æ®µ")
    
    def test_seg_data_integrity(self):
        """æµ‹è¯•çº¿æ®µæ•°æ®å®Œæ•´æ€§"""
        print("\n=== æµ‹è¯•çº¿æ®µæ•°æ®å®Œæ•´æ€§ ===")
        
        # è·å–æµ‹è¯•æ•°æ®
        bis = self._get_test_bis()[:100]  # ä½¿ç”¨è¾ƒå°‘æ•°æ®è¿›è¡Œè¯¦ç»†æ£€æŸ¥
        seg_list = SegList.from_bis(bis)
        
        if len(seg_list) == 0:
            print("æ²¡æœ‰æ„å»ºå‡ºçº¿æ®µï¼Œè·³è¿‡å®Œæ•´æ€§æµ‹è¯•")
            return
        
        # éªŒè¯çº¿æ®µåºåˆ—
        integrity_issues = seg_list.validate_seg_sequence()
        
        print(f"å‘ç°å®Œæ•´æ€§é—®é¢˜: {len(integrity_issues)} ä¸ª")
        for issue in integrity_issues[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªé—®é¢˜
            print(f"  - {issue}")
        
        # è¿™é‡Œæˆ‘ä»¬å®¹å¿ä¸€äº›å®Œæ•´æ€§é—®é¢˜ï¼Œå› ä¸ºçº¿æ®µç®—æ³•ç›¸å¯¹å¤æ‚
        # ä¸»è¦æ£€æŸ¥æ˜¯å¦æœ‰åŸºæœ¬çš„çº¿æ®µæ„å»ºå‡ºæ¥
        self.assertGreaterEqual(len(seg_list), 0, "åº”è¯¥èƒ½æ„å»ºå‡ºåŸºæœ¬çº¿æ®µ")
    
    def test_zhongshu_construction_from_real_data(self):
        """ä½¿ç”¨çœŸå®æ•°æ®æµ‹è¯•ä¸­æ¢æ„å»º"""
        print("\n=== æµ‹è¯•ä¸­æ¢æ„å»ºï¼ˆçœŸå®æ•°æ®ï¼‰===")
        
        # è·å–æµ‹è¯•çº¿æ®µæ•°æ®
        bis = self._get_test_bis()
        seg_list = SegList.from_bis(bis)
        print(f"è¾“å…¥çº¿æ®µæ•°é‡: {len(seg_list)}")
        
        if len(seg_list) < 3:
            print("çº¿æ®µæ•°é‡ä¸è¶³ï¼Œæ— æ³•æµ‹è¯•ä¸­æ¢æ„å»º")
            return
        
        # æµ‹è¯•ä¸­æ¢æ„å»º
        start_time = time.time()
        zhongshu_list = ZhongShuList.from_segs(seg_list.segs)
        end_time = time.time()
        
        processing_time = end_time - start_time
        print(f"ä¸­æ¢æ„å»ºè€—æ—¶: {processing_time:.3f}s")
        print(f"æ„å»ºä¸­æ¢æ•°é‡: {len(zhongshu_list)}")
        
        if len(zhongshu_list) > 0:
            # ç»Ÿè®¡ä¿¡æ¯
            stats = zhongshu_list.get_statistics()
            print(f"ä¸­æ¢ç»Ÿè®¡ä¿¡æ¯:")
            print(f"  - æ€»è®¡: {stats['total_count']} ä¸­æ¢")
            print(f"  - æ´»è·ƒ: {stats['active_count']} ä¸­æ¢")
            print(f"  - å¹³å‡å¼ºåº¦: {stats['avg_strength']:.3f}")
            print(f"  - å¹³å‡ç¨³å®šæ€§: {stats['avg_stability']:.3f}")
            print(f"  - å¹³å‡æŒç»­æ—¶é—´: {stats['avg_duration']:.1f} Kçº¿")
            print(f"  - å¹³å‡åŒºé—´æ¯”ä¾‹: {stats['avg_range_ratio']:.3%}")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªä¸­æ¢çš„è¯¦ç»†ä¿¡æ¯
            print(f"\nå‰3ä¸ªä¸­æ¢è¯¦æƒ…:")
            for i, zs in enumerate(zhongshu_list[:3]):
                print(f"  ä¸­æ¢{i+1}: {zs}")
                print(f"    æ„æˆçº¿æ®µ: {len(zs.forming_segs)} ä¸ª")
                print(f"    æ—¶é—´èŒƒå›´: {zs.start_time.strftime('%Y-%m-%d %H:%M')} ~ {zs.end_time.strftime('%Y-%m-%d %H:%M')}")
        else:
            print("æœªèƒ½æ„å»ºå‡ºä¸­æ¢ï¼Œå¯èƒ½æ˜¯:")
            print("  - çº¿æ®µæ•°é‡ä¸è¶³")
            print("  - çº¿æ®µé‡å åº¦ä¸å¤Ÿ")
            print("  - é…ç½®å‚æ•°è¿‡äºä¸¥æ ¼")
    
    def test_comprehensive_chan_theory_pipeline(self):
        """æµ‹è¯•å®Œæ•´çš„ç¼ è®ºåˆ†ææµç¨‹"""
        print("\n=== æµ‹è¯•å®Œæ•´ç¼ è®ºåˆ†ææµç¨‹ ===")
        
        # ç¬¬ä¸€æ­¥ï¼šè·å–Kçº¿æ•°æ®
        print("æ­¥éª¤1: è·å–Kçº¿æ•°æ®")
        klines = self._get_test_klines()
        print(f"Kçº¿æ•°é‡: {len(klines)}")
        
        # ç¬¬äºŒæ­¥ï¼šæ„å»ºåˆ†å‹
        print("\næ­¥éª¤2: æ„å»ºåˆ†å‹")
        start_time = time.time()
        processed_klines, fenxing_list = self.processor.process_klines(klines)
        fenxing_time = time.time() - start_time
        print(f"åˆ†å‹æ•°é‡: {len(fenxing_list)}, è€—æ—¶: {fenxing_time:.3f}s")
        
        if len(fenxing_list) == 0:
            print("æ— åˆ†å‹æ•°æ®ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
            return
        
        # ç¬¬ä¸‰æ­¥ï¼šæ„å»ºç¬”
        print("\næ­¥éª¤3: æ„å»ºç¬”")
        start_time = time.time()
        bi_list = BiList.from_fenxings(fenxing_list.fenxings)
        bi_time = time.time() - start_time
        print(f"ç¬”æ•°é‡: {len(bi_list)}, è€—æ—¶: {bi_time:.3f}s")
        
        if len(bi_list) == 0:
            print("æ— ç¬”æ•°æ®ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
            return
        
        # ç¬¬å››æ­¥ï¼šæ„å»ºçº¿æ®µ
        print("\næ­¥éª¤4: æ„å»ºçº¿æ®µ")
        start_time = time.time()
        seg_list = SegList.from_bis(bi_list.bis)
        seg_time = time.time() - start_time
        print(f"çº¿æ®µæ•°é‡: {len(seg_list)}, è€—æ—¶: {seg_time:.3f}s")
        
        if len(seg_list) == 0:
            print("æ— çº¿æ®µæ•°æ®ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
            return
        
        # ç¬¬äº”æ­¥ï¼šæ„å»ºä¸­æ¢
        print("\næ­¥éª¤5: æ„å»ºä¸­æ¢")
        start_time = time.time()
        zhongshu_list = ZhongShuList.from_segs(seg_list.segs)
        zhongshu_time = time.time() - start_time
        print(f"ä¸­æ¢æ•°é‡: {len(zhongshu_list)}, è€—æ—¶: {zhongshu_time:.3f}s")
        
        # æ€»ç»“
        total_time = fenxing_time + bi_time + seg_time + zhongshu_time
        print(f"\n=== æµç¨‹æ€»ç»“ ===")
        print(f"æ€»è€—æ—¶: {total_time:.3f}s")
        print(f"æ•°æ®é“¾è·¯: {len(klines)} Kçº¿ -> {len(fenxing_list)} åˆ†å‹ -> {len(bi_list)} ç¬” -> {len(seg_list)} çº¿æ®µ -> {len(zhongshu_list)} ä¸­æ¢")
        
        # éªŒè¯æ•°æ®é“¾è·¯çš„å®Œæ•´æ€§
        self.assertGreater(len(fenxing_list), 0, "åº”è¯¥æœ‰åˆ†å‹æ•°æ®")
        self.assertGreater(len(bi_list), 0, "åº”è¯¥æœ‰ç¬”æ•°æ®") 
        # çº¿æ®µå’Œä¸­æ¢å¯èƒ½ä¸º0ï¼Œè¿™åœ¨æŸäº›å¸‚åœºæ¡ä»¶ä¸‹æ˜¯æ­£å¸¸çš„
        
        print("å®Œæ•´æµç¨‹æµ‹è¯•å®Œæˆ")
    
    def test_extended_data_zhongshu_construction(self):
        """ä½¿ç”¨æ›´å¤§æ•°æ®é›†æµ‹è¯•ä¸­æ¢æ„å»º"""
        print("\n=== æ‰©å±•æ•°æ®é›†ä¸­æ¢æ„å»ºæµ‹è¯• ===")
        
        # ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†ï¼š1000æ ¹Kçº¿
        klines = self._get_test_klines(1000)
        print(f"è·å–Kçº¿æ•°é‡: {len(klines)}")
        
        # å¤„ç†è·å¾—åˆ†å‹
        processed_klines, fenxing_list = self.processor.process_klines(klines)
        print(f"å¤„ç†åKçº¿: {len(processed_klines)}, åˆ†å‹: {len(fenxing_list)}")
        
        if len(fenxing_list) < 10:
            print("åˆ†å‹æ•°é‡ä¸è¶³ï¼Œè·³è¿‡æµ‹è¯•")
            return
        
        # æ„å»ºç¬”
        bi_list = BiList.from_fenxings(fenxing_list.fenxings)
        print(f"æ„å»ºç¬”æ•°é‡: {len(bi_list)}")
        
        if len(bi_list) < 10:
            print("ç¬”æ•°é‡ä¸è¶³ï¼Œè·³è¿‡æµ‹è¯•")
            return
        
        # æ„å»ºçº¿æ®µ
        seg_list = SegList.from_bis(bi_list.bis)
        print(f"æ„å»ºçº¿æ®µæ•°é‡: {len(seg_list)}")
        
        if len(seg_list) < 5:
            print("çº¿æ®µæ•°é‡ä¸è¶³ï¼Œæ— æ³•å……åˆ†æµ‹è¯•ä¸­æ¢")
            # å°è¯•ä½¿ç”¨æ›´å®½æ¾çš„çº¿æ®µæ„å»ºé…ç½®
            loose_config = SegConfig(
                min_bi_count=3,
                require_overlap=False,  # ä¸è¦æ±‚ä¸¥æ ¼é‡å 
                min_amplitude_ratio=0.001,  # é™ä½æœ€å°å¹…åº¦è¦æ±‚
                build_mode="loose"
            )
            seg_list = SegList.from_bis(bi_list.bis, loose_config)
            print(f"ä½¿ç”¨å®½æ¾é…ç½®é‡æ–°æ„å»ºçº¿æ®µæ•°é‡: {len(seg_list)}")
        
        if len(seg_list) < 3:
            print("å³ä½¿ä½¿ç”¨å®½æ¾é…ç½®ï¼Œçº¿æ®µæ•°é‡ä»ç„¶ä¸è¶³")
            return
        
        # æ˜¾ç¤ºçº¿æ®µåŸºæœ¬ä¿¡æ¯
        print(f"\nçº¿æ®µè¯¦æƒ…:")
        for i, seg in enumerate(seg_list[:10]):  # æ˜¾ç¤ºå‰10ä¸ªçº¿æ®µ
            print(f"  çº¿æ®µ{i+1}: {seg.direction.value} "
                  f"{seg.start_price:.2f}->{seg.end_price:.2f} "
                  f"({seg.amplitude_ratio:+.2%}) "
                  f"{seg.bi_count}ç¬” å¼ºåº¦:{seg.strength:.3f}")
        
        # æµ‹è¯•ä¸åŒçš„ä¸­æ¢æ„å»ºé…ç½®
        zhongshu_configs = [
            ("è¶…å®½æ¾", ZhongShuConfig(
                build_mode="loose",
                min_overlap_ratio=0.001,  # æä½é‡å è¦æ±‚
                require_alternating=False,  # ä¸è¦æ±‚æ–¹å‘äº¤æ›¿
                min_duration=1,
                min_amplitude_ratio=0.001
            )),
            ("å®½æ¾", ZhongShuConfig(
                build_mode="loose", 
                min_overlap_ratio=0.01,
                require_alternating=True,
                min_duration=3
            )),
            ("æ ‡å‡†", ZhongShuConfig(
                build_mode="standard",
                min_overlap_ratio=0.05,
                require_alternating=True
            )),
            ("ä¸¥æ ¼", ZhongShuConfig(
                build_mode="strict",
                min_overlap_ratio=0.1,
                require_alternating=True
            ))
        ]
        
        for config_name, config in zhongshu_configs:
            print(f"\n--- {config_name}æ¨¡å¼ä¸­æ¢æ„å»º ---")
            
            start_time = time.time()
            zhongshu_list = ZhongShuList.from_segs(seg_list.segs, config)
            construction_time = time.time() - start_time
            
            print(f"æ„å»ºè€—æ—¶: {construction_time:.3f}s")
            print(f"æ„å»ºä¸­æ¢æ•°é‡: {len(zhongshu_list)}")
            
            if len(zhongshu_list) > 0:
                stats = zhongshu_list.get_statistics()
                print(f"ä¸­æ¢ç»Ÿè®¡:")
                print(f"  - æ€»æ•°: {stats['total_count']}")
                print(f"  - æ´»è·ƒ: {stats['active_count']}")
                print(f"  - å¹³å‡å¼ºåº¦: {stats['avg_strength']:.3f}")
                print(f"  - å¹³å‡ç¨³å®šæ€§: {stats['avg_stability']:.3f}")
                print(f"  - å¹³å‡æŒç»­æ—¶é—´: {stats['avg_duration']:.1f} Kçº¿")
                print(f"  - å¹³å‡åŒºé—´æ¯”ä¾‹: {stats['avg_range_ratio']:.3%}")
                
                # æ˜¾ç¤ºå‰å‡ ä¸ªä¸­æ¢çš„è¯¦ç»†ä¿¡æ¯
                print(f"\nå‰3ä¸ªä¸­æ¢è¯¦æƒ…:")
                for i, zs in enumerate(zhongshu_list[:3]):
                    print(f"  ä¸­æ¢{i+1}: {zs}")
                    print(f"    æ„æˆçº¿æ®µ: {len(zs.forming_segs)} ä¸ª")
                    print(f"    æ—¶é—´: {zs.start_time.strftime('%Y-%m-%d')} ~ {zs.end_time.strftime('%Y-%m-%d')}")
                    print(f"    ä»·æ ¼åŒºé—´: {zs.low:.2f} - {zs.high:.2f} (ä¸­å¿ƒ:{zs.center:.2f})")
                    
                # å¦‚æœæ‰¾åˆ°ä¸­æ¢ï¼Œå°±ä¸éœ€è¦ç»§ç»­æµ‹è¯•æ›´å®½æ¾çš„é…ç½®
                break
            else:
                print("  æœªæ„å»ºå‡ºä¸­æ¢")
        
        # éªŒè¯åŸºæœ¬è¦æ±‚
        self.assertGreater(len(seg_list), 2, "åº”è¯¥è‡³å°‘æœ‰3ä¸ªçº¿æ®µç”¨äºä¸­æ¢æ„å»º")
    
    def test_multiple_stocks_zhongshu_analysis(self):
        """å¤šåªè‚¡ç¥¨çš„ä¸­æ¢åˆ†ææµ‹è¯•"""
        print("\n=== å¤šè‚¡ç¥¨ä¸­æ¢åˆ†ææµ‹è¯• ===")
        
        # æµ‹è¯•å¤šåªè‚¡ç¥¨
        test_stocks = ["000001.SZ", "000002.SZ", "600000.SH"]
        results = {}
        
        for stock_code in test_stocks:
            print(f"\n--- åˆ†æè‚¡ç¥¨ {stock_code} ---")
            
            try:
                # è·å–è¯¥è‚¡ç¥¨çš„Kçº¿æ•°æ®
                collection = self.db['stock_kline_daily']
                query = {'ts_code': stock_code}
                cursor = collection.find(query).sort('trade_date', -1).limit(500)
                raw_data = list(cursor)
                # åè½¬ä¸ºå‡åºï¼ˆä»æ—§åˆ°æ–°ï¼‰
                raw_data.reverse()
                
                if len(raw_data) < 100:
                    print(f"  æ•°æ®ä¸è¶³: {len(raw_data)} æ¡")
                    continue
                
                # è½¬æ¢å¹¶å¤„ç†æ•°æ®
                converted_data = self._convert_daily_data_to_chan_format(raw_data)
                klines = KLineList.from_mongo_data(converted_data, TimeLevel.DAILY)
                
                # å®Œæ•´çš„ç¼ è®ºæµç¨‹
                processed_klines, fenxings = self.processor.process_klines(klines)
                bis = BiList.from_fenxings(fenxings.fenxings)
                
                # ä½¿ç”¨å®½æ¾é…ç½®æ„å»ºçº¿æ®µ
                loose_seg_config = SegConfig(
                    min_bi_count=3,
                    require_overlap=False,
                    min_amplitude_ratio=0.001,
                    build_mode="loose"
                )
                segs = SegList.from_bis(bis.bis, loose_seg_config)
                
                # ä½¿ç”¨å®½æ¾é…ç½®æ„å»ºä¸­æ¢
                loose_zs_config = ZhongShuConfig(
                    build_mode="loose",
                    min_overlap_ratio=0.001,
                    require_alternating=False,
                    min_duration=1
                )
                zhongshus = ZhongShuList.from_segs(segs.segs, loose_zs_config)
                
                # ç»Ÿè®¡ç»“æœ
                results[stock_code] = {
                    'klines': len(klines),
                    'fenxings': len(fenxings),
                    'bis': len(bis),
                    'segs': len(segs),
                    'zhongshus': len(zhongshus)
                }
                
                print(f"  Kçº¿: {len(klines)} -> åˆ†å‹: {len(fenxings)} -> ç¬”: {len(bis)} -> çº¿æ®µ: {len(segs)} -> ä¸­æ¢: {len(zhongshus)}")
                
                if len(zhongshus) > 0:
                    stats = zhongshus.get_statistics()
                    print(f"  ä¸­æ¢å¹³å‡å¼ºåº¦: {stats['avg_strength']:.3f}")
                    print(f"  ä¸­æ¢å¹³å‡ç¨³å®šæ€§: {stats['avg_stability']:.3f}")
                
            except Exception as e:
                print(f"  å¤„ç†å¤±è´¥: {e}")
                continue
        
        # æ±‡æ€»ç»Ÿè®¡
        print(f"\n=== å¤šè‚¡ç¥¨æ±‡æ€» ===")
        for stock, result in results.items():
            print(f"{stock}: Kçº¿{result['klines']} -> åˆ†å‹{result['fenxings']} -> ç¬”{result['bis']} -> çº¿æ®µ{result['segs']} -> ä¸­æ¢{result['zhongshus']}")
        
        # åŸºæœ¬éªŒè¯
        self.assertGreater(len(results), 0, "åº”è¯¥è‡³å°‘æˆåŠŸåˆ†æä¸€åªè‚¡ç¥¨")
    
    def _get_test_bis(self, limit: int = 1000):
        """è·å–æµ‹è¯•ç”¨çš„ç¬”æ•°æ®"""
        # å¤ç”¨å·²æœ‰çš„åˆ†å‹æ•°æ®æ„å»ºç¬”
        fenxings = self._get_test_fenxings(limit)
        bi_list = BiList.from_fenxings(fenxings)
        return bi_list.bis
    
    def _get_test_fenxings(self, limit: int = 1000):
        """è·å–æµ‹è¯•ç”¨çš„åˆ†å‹æ•°æ®"""
        # å¤ç”¨å·²æœ‰çš„Kçº¿æ•°æ®æ„å»ºåˆ†å‹
        klines = self._get_test_klines(limit)
        processed_klines, fenxings = self.processor.process_klines(klines)
        return fenxings.fenxings
    
    def _get_test_klines(self, limit: int = 200):
        """è·å–æµ‹è¯•ç”¨çš„Kçº¿æ•°æ®"""
        # åŠ è½½ä¸€åªè‚¡ç¥¨çš„æ•°æ®ä½œä¸ºæµ‹è¯•
        collection = self.db['stock_kline_daily']
        stock_code = "000001.SZ"
        
        query = {'ts_code': stock_code}
        cursor = collection.find(query).sort('trade_date', -1).limit(limit)
        raw_data = list(cursor)
        # åè½¬ä¸ºå‡åºï¼ˆä»æ—§åˆ°æ–°ï¼‰
        raw_data.reverse()
        
        converted_data = self._convert_daily_data_to_chan_format(raw_data)
        kline_list = KLineList.from_mongo_data(converted_data, TimeLevel.DAILY)
        return kline_list


class TestRealDataPerformance(unittest.TestCase):
    """çœŸå®æ•°æ®æ€§èƒ½æµ‹è¯•"""
    
    @classmethod
    def setUpClass(cls):
        if not MODULES_AVAILABLE:
            raise unittest.SkipTest("æ¨¡å—å¯¼å…¥å¤±è´¥")
        
        try:
            cls.db_handler = get_db_handler()
            cls.db = cls.db_handler.db
            cls.config = ChanConfig()
            cls.processor = KlineProcessor(cls.config)
        except Exception as e:
            raise unittest.SkipTest(f"åˆå§‹åŒ–å¤±è´¥: {e}")
    
    
    def test_large_dataset_performance(self):
        """å¤§æ•°æ®é›†æ€§èƒ½æµ‹è¯•"""
        collection = self.db['stock_kline_daily']
        
        # è·å–å¤šåªè‚¡ç¥¨çš„å¤§é‡æ•°æ®
        stocks = ["000001.SZ", "000002.SZ", "600000.SH"]
        all_data = []
        
        for stock in stocks:
            query = {'ts_code': stock}
            cursor = collection.find(query).sort('trade_date', -1).limit(500)
            stock_data = list(cursor)
            stock_data.reverse()  # åè½¬ä¸ºå‡åºï¼ˆä»æ—§åˆ°æ–°ï¼‰
            all_data.extend(stock_data)
        
        print(f"ğŸš€ å¤§æ•°æ®é›†æ€§èƒ½æµ‹è¯• - æ€»æ•°æ®é‡: {len(all_data)} æ¡")
        
        # è½¬æ¢æ•°æ®
        start_time = time.time()
        converted_data = []
        for item in all_data:
            try:
                trade_date_str = str(item['trade_date'])
                timestamp = datetime.strptime(trade_date_str, '%Y%m%d')
                
                converted_data.append({
                    'timestamp': timestamp,
                    'open': float(item['open']),
                    'high': float(item['high']),
                    'low': float(item['low']),
                    'close': float(item['close']),
                    'volume': int(float(item['vol'])),
                    'amount': float(item.get('amount', 0)),
                    'symbol': item['ts_code']
                })
            except:
                continue
        
        conversion_time = time.time() - start_time
        
        # å¤„ç†æ•°æ®
        start_time = time.time()
        kline_list = KLineList.from_mongo_data(converted_data, TimeLevel.DAILY)
        processed_klines, fenxings = self.processor.process_klines(kline_list)
        processing_time = time.time() - start_time
        
        total_time = conversion_time + processing_time
        
        print(f"ğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ:")
        print(f"  - æ•°æ®è½¬æ¢: {conversion_time:.3f} ç§’")
        print(f"  - æ•°æ®å¤„ç†: {processing_time:.3f} ç§’")
        print(f"  - æ€»è€—æ—¶: {total_time:.3f} ç§’")
        print(f"  - å¤„ç†é€Ÿåº¦: {len(processed_klines)/total_time:.0f} æ¡/ç§’")
        print(f"  - å†…å­˜æ•ˆç‡: {len(converted_data)}->{len(processed_klines)} æ¡")
        print(f"  - åˆ†å‹è¯†åˆ«: {len(fenxings)} ä¸ªåˆ†å‹")
        
        # æ€§èƒ½è¦æ±‚
        self.assertLess(total_time, 10.0, "å¤§æ•°æ®é›†å¤„ç†æ—¶é—´è¿‡é•¿")
        self.assertGreater(len(processed_klines)/total_time, 100, "å¤„ç†é€Ÿåº¦è¿‡æ…¢")


def cleanup_db_connections():
    """æ¸…ç†æ•°æ®åº“è¿æ¥"""
    try:
        from database.db_handler import _db_handler
        if _db_handler is not None:
            if hasattr(_db_handler, 'cloud_client') and _db_handler.cloud_client:
                _db_handler.cloud_client.close()
            if hasattr(_db_handler, 'local_client') and _db_handler.local_client:
                _db_handler.local_client.close()
    except Exception:
        pass

def run_bi_construction_tests():
    """ä¸“é—¨è¿è¡Œç¬”æ„å»ºç®—æ³•ä¿®å¤éªŒè¯æµ‹è¯•"""
    import atexit
    atexit.register(cleanup_db_connections)
    
    print("ğŸ”¥ ç¬”æ„å»ºç®—æ³•ä¿®å¤éªŒè¯æµ‹è¯•")
    print("=" * 70)
    print("ğŸ“Š æµ‹è¯•æ•°æ®æº:")
    print("  - stock_kline_daily: 720ä¸‡+æ¡æ—¥çº¿æ•°æ® (2019-2025)")
    print("ğŸ“‹ æµ‹è¯•å†…å®¹:")
    print("  - è¿ç»­åˆ†å‹ä¼˜åŒ–ç®—æ³•éªŒè¯")
    print("  - ç¼ è®ºæ ‡å‡†'ä¿ç•™ç¬¬ä¸€ä¸ª'åŸåˆ™éªŒè¯")
    print("  - ç¬”æ„å»ºå®Œæ•´æ€§å’Œè´¨é‡éªŒè¯")
    print("  - ç¬”åºåˆ—æ–¹å‘äº¤æ›¿éªŒè¯")
    print("  - ç¬”åˆ†å‹è¿æ¥æ­£ç¡®æ€§éªŒè¯")
    print("=" * 70)
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶ï¼Œä¸“é—¨æµ‹è¯•ç¬”æ„å»º
    suite = unittest.TestSuite()
    
    # æ·»åŠ ç¬”æ„å»ºæµ‹è¯•
    test_class = TestRealStockDataIntegration
    
    # æ•°æ®åº“è¿æ¥æµ‹è¯•
    suite.addTest(test_class('test_database_connection_and_collections'))
    suite.addTest(test_class('test_load_real_daily_kline_data'))
    
    # æ ¸å¿ƒï¼šç¬”æ„å»ºç®—æ³•ä¿®å¤éªŒè¯æµ‹è¯•
    suite.addTest(test_class('test_bi_construction_algorithm_fix'))
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # æµ‹è¯•ç»“æœæ€»ç»“
    print("\n" + "=" * 70)
    print("ğŸ¯ ç¬”æ„å»ºç®—æ³•ä¿®å¤æµ‹è¯•æ€»ç»“")
    print("=" * 70)
    print(f"æ€»æµ‹è¯•æ•°: {result.testsRun}")
    print(f"æˆåŠŸ: {result.testsRun - len(result.failures) - len(result.errors)}")
    print(f"å¤±è´¥: {len(result.failures)}")
    print(f"é”™è¯¯: {len(result.errors)}")
    
    if result.failures:
        print("\nâŒ å¤±è´¥çš„æµ‹è¯•:")
        for test, traceback in result.failures:
            print(f"  - {test}")
            print(f"    {traceback.splitlines()[-1]}")
    
    if result.errors:
        print("\nğŸ’¥ é”™è¯¯çš„æµ‹è¯•:")
        for test, traceback in result.errors:
            print(f"  - {test}")
            print(f"    {traceback.splitlines()[-1]}")
    
    if result.wasSuccessful():
        print("\nâœ… æ‰€æœ‰ç¬”æ„å»ºç®—æ³•ä¿®å¤æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ“ˆ ç¬”æ„å»ºç®—æ³•ä¿®å¤æˆåŠŸï¼Œå®Œå…¨ç¬¦åˆç¼ è®ºæ ‡å‡†")
        print("ğŸ¯ è¿ç»­åˆ†å‹å¤„ç†éµå¾ª'ä¿ç•™ç¬¬ä¸€ä¸ªï¼Œèˆå¼ƒåç»­'åŸåˆ™")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œè¯·æ£€æŸ¥å…·ä½“é—®é¢˜")
    
    print("=" * 70)
    
    return result.wasSuccessful()

if __name__ == '__main__':
    # è¿è¡Œä¸“é—¨çš„ç¬”æ„å»ºç®—æ³•ä¿®å¤éªŒè¯æµ‹è¯•
    success = run_bi_construction_tests()