#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸“é—¨æµ‹è¯•Kçº¿åˆå¹¶åŠŸèƒ½çš„ç‹¬ç«‹æµ‹è¯•è„šæœ¬
ä½¿ç”¨å•ä¸€è‚¡ç¥¨çš„å¹²å‡€æ•°æ®ï¼ŒéªŒè¯Kçº¿åˆå¹¶ç®—æ³•çš„æ­£ç¡®æ€§
"""

import unittest
import sys
import os
from datetime import datetime, timedelta
from typing import List

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(project_root)

chan_theory_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(chan_theory_root)

try:
    from database.db_handler import get_db_handler
    from models.kline import KLine, KLineList
    from models.enums import TimeLevel
    from core.kline_processor import KlineProcessor
    from config.chan_config import ChanConfig
    
    MODULES_AVAILABLE = True
except Exception as e:
    print(f"å¯¼å…¥å¤±è´¥: {e}")
    MODULES_AVAILABLE = False


class TestKlineMergeOnly(unittest.TestCase):
    """ä¸“é—¨æµ‹è¯•Kçº¿åˆå¹¶åŠŸèƒ½"""
    
    @classmethod
    def setUpClass(cls):
        if not MODULES_AVAILABLE:
            raise unittest.SkipTest("æ¨¡å—å¯¼å…¥å¤±è´¥")
        
        try:
            cls.db_handler = get_db_handler()
            cls.db = cls.db_handler.db
            cls.config = ChanConfig()
            cls.processor = KlineProcessor(cls.config)
            
            cls.db.command('ping')
            print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
            
        except Exception as e:
            raise unittest.SkipTest(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
    
    @classmethod
    def tearDownClass(cls):
        """æµ‹è¯•ç»“æŸåæ¸…ç†èµ„æº"""
        try:
            if hasattr(cls, 'db_handler') and cls.db_handler:
                # è°ƒç”¨ææ„æ–¹æ³•æ¥æ­£ç¡®å…³é—­æ‰€æœ‰è¿æ¥
                cls.db_handler.__del__()
                print("âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")
                
            # é‡ç½®å…¨å±€æ•°æ®åº“å¤„ç†å™¨å•ä¾‹ï¼Œç¡®ä¿èµ„æºå®Œå…¨é‡Šæ”¾
            from database.db_handler import reset_db_handler
            reset_db_handler()
            print("âœ… æ•°æ®åº“å¤„ç†å™¨å•ä¾‹å·²é‡ç½®")
            
        except Exception as e:
            print(f"âš ï¸ èµ„æºæ¸…ç†æ—¶å‡ºç°å¼‚å¸¸: {e}")
        finally:
            print("ğŸ”’ æ•°æ®åº“èµ„æºæ¸…ç†å®Œæˆ")
    
    def test_single_stock_clean_data(self):
        """æµ‹è¯•å•ä¸€è‚¡ç¥¨çš„å¹²å‡€æ•°æ®"""
        print("\nğŸ” å•ä¸€è‚¡ç¥¨Kçº¿åˆå¹¶æµ‹è¯•")
        
        # è·å–å¹³å®‰é“¶è¡Œæœ€è¿‘100æ ¹Kçº¿
        collection = self.db['stock_kline_daily']
        stock_code = "000001.SZ"
        
        # ä½¿ç”¨å‡åºæ’åºï¼Œç¡®ä¿æ—¶é—´é¡ºåºæ­£ç¡®
        query = {'ts_code': stock_code}
        cursor = collection.find(query).sort('trade_date', 1).limit(100)  # æ”¹ä¸ºå‡åº
        raw_data = list(cursor)
        
        if len(raw_data) < 50:
            self.skipTest("æ•°æ®é‡ä¸è¶³")
        
        print(f"ğŸ“Š è·å– {stock_code} æ•°æ®: {len(raw_data)} æ¡")
        
        # è½¬æ¢æ•°æ®
        converted_data = []
        for item in raw_data:
            try:
                trade_date_str = str(item['trade_date'])
                timestamp = datetime.strptime(trade_date_str, '%Y%m%d')
                
                converted_item = {
                    'timestamp': timestamp,
                    'open': float(item['open']),
                    'high': float(item['high']),
                    'low': float(item['low']),
                    'close': float(item['close']),
                    'volume': int(float(item['vol'])),
                    'amount': float(item.get('amount', 0)),
                    'symbol': item['ts_code']
                }
                
                if all(converted_item[k] > 0 for k in ['open', 'high', 'low', 'close']):
                    converted_data.append(converted_item)
                    
            except Exception:
                continue
        
        # éªŒè¯æ—¶é—´é¡ºåº
        timestamps = [item['timestamp'] for item in converted_data]
        self.assertEqual(timestamps, sorted(timestamps), "è¾“å…¥æ•°æ®æ—¶é—´é¡ºåºåº”è¯¥æ­£ç¡®")
        
        # åˆ›å»ºKLineList
        kline_list = KLineList.from_mongo_data(converted_data, TimeLevel.DAILY)
        
        print(f"âœ… åˆ›å»ºKLineList: {len(kline_list)}æ ¹Kçº¿")
        
        # éªŒè¯è¾“å…¥æ•°æ®æ—¶é—´é¡ºåº
        input_errors = self.processor._validate_input_data(kline_list)
        if input_errors:
            print(f"âš ï¸  è¾“å…¥æ•°æ®é—®é¢˜: {len(input_errors)}ä¸ª")
            for error in input_errors[:3]:
                print(f"  - {error}")
        else:
            print("âœ… è¾“å…¥æ•°æ®éªŒè¯é€šè¿‡")
        
        # å¤„ç†Kçº¿
        processed_klines, fenxings = self.processor.process_klines(kline_list)
        
        # éªŒè¯å¤„ç†ç»“æœ
        processing_errors = self.processor.validate_processed_klines(processed_klines)
        
        print(f"ğŸ“Š å¤„ç†ç»“æœ:")
        print(f"  - åŸå§‹Kçº¿: {len(kline_list)}æ ¹")
        print(f"  - å¤„ç†åKçº¿: {len(processed_klines)}æ ¹") 
        print(f"  - åˆå¹¶æ•°é‡: {len(kline_list) - len(processed_klines)}æ ¹")
        print(f"  - åˆå¹¶æ¯”ä¾‹: {(len(kline_list) - len(processed_klines)) / len(kline_list) * 100:.1f}%")
        print(f"  - åˆ†å‹æ•°é‡: {len(fenxings)}ä¸ª")
        print(f"  - å¤„ç†é”™è¯¯: {len(processing_errors)}ä¸ª")
        
        if processing_errors:
            print("âš ï¸  å¤„ç†è´¨é‡é—®é¢˜:")
            for error in processing_errors[:5]:
                print(f"  - {error}")
        else:
            print("âœ… å¤„ç†è´¨é‡éªŒè¯é€šè¿‡")
        
        # åŸºæœ¬æ–­è¨€
        self.assertGreater(len(processed_klines), 0, "åº”è¯¥æœ‰å¤„ç†åçš„Kçº¿")
        self.assertLessEqual(len(processed_klines), len(kline_list), "å¤„ç†åKçº¿ä¸åº”å¢åŠ ")
        
        # ä¸¥æ ¼çš„è´¨é‡è¦æ±‚ï¼šå¯¹äºå•ä¸€è‚¡ç¥¨çš„å¹²å‡€æ•°æ®ï¼Œä¸åº”è¯¥æœ‰æ—¶é—´é¡ºåºé”™è¯¯
        time_order_errors = [error for error in processing_errors if "æ—¶é—´é¡ºåºé”™è¯¯" in error]
        if time_order_errors:
            print(f"âŒ æ—¶é—´é¡ºåºé”™è¯¯: {len(time_order_errors)}ä¸ª")
            self.assertEqual(len(time_order_errors), 0, f"å•ä¸€è‚¡ç¥¨æ•°æ®ä¸åº”è¯¥æœ‰æ—¶é—´é¡ºåºé”™è¯¯: {time_order_errors[:3]}")
        else:
            print("âœ… æ—¶é—´é¡ºåºéªŒè¯é€šè¿‡")
        
        return processed_klines, fenxings
    
    def test_merge_algorithm_correctness(self):
        """æµ‹è¯•åˆå¹¶ç®—æ³•çš„æ­£ç¡®æ€§"""
        print("\nğŸ”§ æµ‹è¯•åˆå¹¶ç®—æ³•æ­£ç¡®æ€§")
        
        # å¤„ç†æ•°æ®
        processed_klines, fenxings = self.test_single_stock_clean_data()
        
        # æ£€æŸ¥ç¼ è®ºåˆè§„æ€§
        chan_violations = self.processor.validate_chan_theory_compliance(processed_klines)
        
        print(f"ğŸ“‹ ç¼ è®ºæ ‡å‡†éªŒè¯:")
        print(f"  - è¿è§„é¡¹ç›®: {len(chan_violations)}ä¸ª")
        
        if chan_violations:
            print("âš ï¸  ç¼ è®ºæ ‡å‡†é—®é¢˜:")
            for violation in chan_violations[:5]:
                print(f"  - {violation}")
        else:
            print("âœ… å®Œå…¨ç¬¦åˆç¼ è®ºæ ‡å‡†")
        
        # è·å–å¤„ç†ç»Ÿè®¡
        stats = self.processor.get_processing_statistics(
            KLineList([KLine(
                timestamp=datetime.now(),
                open=10, high=11, low=9, close=10.5, volume=1000
            )] * 100),  # æ¨¡æ‹ŸåŸå§‹æ•°æ®
            processed_klines,
            fenxings
        )
        
        print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"  - å¤„ç†è´¨é‡: {stats['processing_quality']}")
        print(f"  - è´¨é‡é—®é¢˜æ•°: {stats['quality_issues_count']}")
        print(f"  - å¹³å‡åˆå¹¶æ•°: {stats['avg_merge_count']:.2f}")
        print(f"  - æœ€å¤§åˆå¹¶æ•°: {stats['max_merge_count']}")
        
        # ä¸¥æ ¼è´¨é‡è¦æ±‚
        self.assertEqual(len(chan_violations), 0, "åº”è¯¥å®Œå…¨ç¬¦åˆç¼ è®ºæ ‡å‡†")
        self.assertIn(stats['processing_quality'], ['excellent', 'good'], 
                     f"å¤„ç†è´¨é‡åº”è¯¥ä¼˜ç§€: {stats['processing_quality']}")
        
        return stats

if __name__ == '__main__':
    print("ğŸ”¥ Kçº¿åˆå¹¶åŠŸèƒ½ä¸“é¡¹æµ‹è¯•")
    print("=" * 50)
    print("ğŸ“‹ æµ‹è¯•å†…å®¹ï¼šä¸“é—¨éªŒè¯Kçº¿åˆå¹¶ç®—æ³•æ­£ç¡®æ€§")
    print("=" * 50)
    
    unittest.main(verbosity=2)