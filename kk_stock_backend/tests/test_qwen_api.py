#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen APIæµ‹è¯•è„šæœ¬
ç”¨äºè¯Šæ–­APIè°ƒç”¨é—®é¢˜
"""

import requests
import json
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° sys.path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

# content_generatoræ¨¡å—å·²åˆ é™¤ï¼Œä½¿ç”¨ç¡¬ç¼–ç é…ç½®
def get_llm_config():
    """è·å–LLMé…ç½®ï¼ˆç¡¬ç¼–ç ç‰ˆæœ¬ï¼‰"""
    return {
        "api_base_url": "http://172.16.20.20:8008",
        "full_api_url": "http://172.16.20.20:8008/v1/chat/completions",
        "model_name": "QwQ"
    }


def test_basic_connection():
    """æµ‹è¯•åŸºæœ¬è¿æ¥"""
    print("ğŸ”— æµ‹è¯•åŸºæœ¬è¿æ¥...")
    
    llm_config = get_llm_config()
    api_url = llm_config.get("full_api_url", "http://172.16.20.20:8008/v1/chat/completions")
    
    try:
        # æµ‹è¯•æ ¹è·¯å¾„
        base_url = api_url.replace('/v1/chat/completions', '')
        response = requests.get(base_url, timeout=5)
        print(f"âœ… åŸºæœ¬è¿æ¥æµ‹è¯•: HTTP {response.status_code}")
        if response.status_code != 200:
            print(f"   å“åº”å†…å®¹: {response.text[:200]}...")
    except Exception as e:
        print(f"âŒ åŸºæœ¬è¿æ¥å¤±è´¥: {e}")


def test_openai_format():
    """æµ‹è¯•OpenAIæ ¼å¼çš„APIè°ƒç”¨"""
    print("\nğŸ¤– æµ‹è¯•OpenAIæ ¼å¼APIè°ƒç”¨...")
    
    llm_config = get_llm_config()
    api_url = llm_config.get("full_api_url", "http://172.16.20.20:8008/v1/chat/completions")
    model_name = llm_config.get("model_name", "QwQ")
    
    # æ ‡å‡†OpenAIæ ¼å¼
    payload = {
        "model": model_name,
        "messages": [
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç®€å•å›å¤"}
        ],
        "max_tokens": 50,
        "temperature": 0.7
    }
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer EMPTY"  # æœ¬åœ°æœåŠ¡é€šå¸¸ä¸éœ€è¦çœŸå®token
    }
    
    try:
        print(f"ğŸ“¡ å‘é€è¯·æ±‚åˆ°: {api_url}")
        print(f"ğŸ“ è¯·æ±‚ä½“: {json.dumps(payload, ensure_ascii=False, indent=2)}")
        
        response = requests.post(
            api_url,
            json=payload,
            headers=headers,
            timeout=30
        )
        
        print(f"ğŸ“Š å“åº”çŠ¶æ€: HTTP {response.status_code}")
        print(f"ğŸ“Š å“åº”å¤´: {dict(response.headers)}")
        
        if response.status_code == 200:
            result = response.json()
            print(f"âœ… APIè°ƒç”¨æˆåŠŸ!")
            print(f"ğŸ“œ å“åº”å†…å®¹: {json.dumps(result, ensure_ascii=False, indent=2)}")
        else:
            print(f"âŒ APIè°ƒç”¨å¤±è´¥: HTTP {response.status_code}")
            print(f"ğŸ“œ é”™è¯¯å†…å®¹: {response.text}")
            
    except Exception as e:
        print(f"ğŸ’¥ è¯·æ±‚å¼‚å¸¸: {e}")


def test_simple_format():
    """æµ‹è¯•ç®€åŒ–æ ¼å¼çš„APIè°ƒç”¨"""
    print("\nğŸ”§ æµ‹è¯•ç®€åŒ–æ ¼å¼APIè°ƒç”¨...")
    
    llm_config = get_llm_config()
    api_url = llm_config.get("full_api_url", "http://172.16.20.20:8008/v1/chat/completions")
    model_name = llm_config.get("model_name", "QwQ")
    
    # æœ€ç®€æ ¼å¼
    payload = {
        "model": model_name,
        "messages": [
            {"role": "user", "content": "Hello"}
        ]
    }
    
    try:
        print(f"ğŸ“¡ å‘é€ç®€åŒ–è¯·æ±‚åˆ°: {api_url}")
        print(f"ğŸ“ è¯·æ±‚ä½“: {json.dumps(payload, ensure_ascii=False, indent=2)}")
        
        response = requests.post(
            api_url,
            json=payload,
            timeout=30
        )
        
        print(f"ğŸ“Š å“åº”çŠ¶æ€: HTTP {response.status_code}")
        
        if response.status_code == 200:
            result = response.json()
            print(f"âœ… ç®€åŒ–APIè°ƒç”¨æˆåŠŸ!")
            print(f"ğŸ“œ å“åº”å†…å®¹: {json.dumps(result, ensure_ascii=False, indent=2)}")
        else:
            print(f"âŒ ç®€åŒ–APIè°ƒç”¨å¤±è´¥: HTTP {response.status_code}")
            print(f"ğŸ“œ é”™è¯¯å†…å®¹: {response.text}")
            
    except Exception as e:
        print(f"ğŸ’¥ è¯·æ±‚å¼‚å¸¸: {e}")


def test_different_models():
    """æµ‹è¯•ä¸åŒçš„æ¨¡å‹åç§°"""
    print("\nğŸ¯ æµ‹è¯•ä¸åŒæ¨¡å‹åç§°...")
    
    llm_config = get_llm_config()
    api_url = llm_config.get("full_api_url", "http://172.16.20.20:8008/v1/chat/completions")
    
    # å°è¯•ä¸åŒçš„æ¨¡å‹åç§°
    model_names = ["QwQ", "qwq", "Qwen", "default", ""]
    
    for model_name in model_names:
        print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹åç§°: '{model_name}'")
        
        payload = {
            "model": model_name,
            "messages": [{"role": "user", "content": "test"}],
            "max_tokens": 10
        }
        
        try:
            response = requests.post(api_url, json=payload, timeout=10)
            print(f"   çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                print(f"   âœ… æ¨¡å‹åç§° '{model_name}' æœ‰æ•ˆ")
                return model_name
            else:
                print(f"   âŒ æ¨¡å‹åç§° '{model_name}' æ— æ•ˆ: {response.text[:100]}")
                
        except Exception as e:
            print(f"   ğŸ’¥ è¯·æ±‚å¤±è´¥: {e}")
    
    return None


def test_model_list():
    """æµ‹è¯•è·å–æ¨¡å‹åˆ—è¡¨"""
    print("\nğŸ“‹ æµ‹è¯•è·å–æ¨¡å‹åˆ—è¡¨...")
    
    llm_config = get_llm_config()
    base_url = llm_config.get("api_base_url", "http://172.16.20.20:8008")
    models_url = f"{base_url}/v1/models"
    
    try:
        response = requests.get(models_url, timeout=10)
        print(f"ğŸ“Š æ¨¡å‹åˆ—è¡¨å“åº”: HTTP {response.status_code}")
        
        if response.status_code == 200:
            models = response.json()
            print(f"âœ… å¯ç”¨æ¨¡å‹: {json.dumps(models, ensure_ascii=False, indent=2)}")
        else:
            print(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {response.text}")
            
    except Exception as e:
        print(f"ğŸ’¥ è¯·æ±‚å¼‚å¸¸: {e}")


def run_comprehensive_test():
    """è¿è¡Œç»¼åˆæµ‹è¯•"""
    print("ğŸš€ Qwen API ç»¼åˆæµ‹è¯•")
    print("=" * 60)
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    try:
        llm_config = get_llm_config()
        print("\nâš™ï¸ å½“å‰é…ç½®:")
        print(f"  APIåœ°å€: {llm_config.get('api_base_url', 'N/A')}")
        print(f"  å®Œæ•´URL: {llm_config.get('full_api_url', 'N/A')}")
        print(f"  æ¨¡å‹åç§°: {llm_config.get('model_name', 'N/A')}")
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    test_basic_connection()
    test_model_list()
    test_different_models()
    test_simple_format()
    test_openai_format()
    
    print("\n" + "=" * 60)
    print("ğŸ¯ æµ‹è¯•å»ºè®®:")
    print("1. æ£€æŸ¥æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ")
    print("2. ç¡®è®¤æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")
    print("3. éªŒè¯APIç«¯ç‚¹è·¯å¾„")
    print("4. æ£€æŸ¥è¯·æ±‚æ ¼å¼æ˜¯å¦ç¬¦åˆæœåŠ¡è¦æ±‚")


if __name__ == "__main__":
    run_comprehensive_test()