# æœ¬åœ°Qwen3-32Bæ¨¡å‹éƒ¨ç½²è¯´æ˜

## ğŸ¯ æ¦‚è¿°

æœ¬ç³»ç»Ÿå®Œå…¨åŸºäº**æœ¬åœ°éƒ¨ç½²çš„Qwen3-32B**æ¨¡å‹ï¼Œæ”¯æŒå¤šç§éƒ¨ç½²æ–¹å¼ï¼Œæ— éœ€ä¾èµ–ä»»ä½•åœ¨çº¿APIæœåŠ¡ã€‚

## ğŸ“‹ æ”¯æŒçš„éƒ¨ç½²æ–¹å¼

### 1. **vLLMæœåŠ¡éƒ¨ç½²** (æ¨èç”Ÿäº§ç¯å¢ƒ)

```bash
# 1. å®‰è£…vLLM
pip install vllm

# 2. å¯åŠ¨vLLMæœåŠ¡
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-32B-Instruct \
    --served-model-name Qwen/Qwen2.5-32B-Instruct \
    --host 0.0.0.0 \
    --port 8000

# 3. éªŒè¯æœåŠ¡
curl http://localhost:8000/v1/models
```

### 2. **LM Studioéƒ¨ç½²** (æ¨èå¼€å‘è°ƒè¯•)

```bash
# 1. ä¸‹è½½å¹¶å®‰è£…LM Studio
# https://lmstudio.ai/

# 2. åœ¨LM Studioä¸­åŠ è½½Qwen2.5-32B-Instructæ¨¡å‹

# 3. å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨ (é»˜è®¤ç«¯å£1234)

# 4. éªŒè¯æœåŠ¡
curl http://localhost:1234/v1/models
```

### 3. **Transformersç›´æ¥è°ƒç”¨**

```python
# é€‚ç”¨äºæœ‰è¶³å¤ŸGPUå†…å­˜çš„æƒ…å†µ
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "Qwen/Qwen2.5-32B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
```

## ğŸ”§ ç³»ç»Ÿé…ç½®

### é…ç½®æ–‡ä»¶ç¤ºä¾‹

```python
# 1. vLLMé…ç½®
config = {
    'llm_provider': 'vllm',
    'model_name': 'Qwen/Qwen2.5-32B-Instruct',
    'api_url': 'http://localhost:8000/v1/chat/completions'
}

# 2. LM Studioé…ç½®
config = {
    'llm_provider': 'lm_studio',
    'model_name': 'Qwen/Qwen2.5-32B-Instruct',
    'api_url': 'http://localhost:1234/v1/chat/completions'
}

# 3. æœ¬åœ°ç›´æ¥è°ƒç”¨é…ç½®
config = {
    'llm_provider': 'local_qwen',
    'model_name': 'Qwen/Qwen2.5-32B-Instruct',
    'api_url': 'http://localhost:8000/v1/chat/completions'
}
```

## ğŸš€ å¿«é€Ÿå¯åŠ¨

### æ­¥éª¤1ï¼šå®‰è£…ä¾èµ–

```bash
# å®‰è£…Pythonä¾èµ–
pip install -r requirements.txt

# ç¡®ä¿å®‰è£…äº†PyTorch (GPUæ”¯æŒ)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### æ­¥éª¤2ï¼šå¯åŠ¨æ¨¡å‹æœåŠ¡

```bash
# æ–¹å¼1ï¼šä½¿ç”¨vLLM (æ¨è)
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-32B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --gpu-memory-utilization 0.8

# æ–¹å¼2ï¼šä½¿ç”¨LM Studio
# æ‰“å¼€LM Studio -> åŠ è½½æ¨¡å‹ -> å¯åŠ¨æœåŠ¡å™¨
```

### æ­¥éª¤3ï¼šæµ‹è¯•ç³»ç»Ÿ

```bash
# æµ‹è¯•æ¨¡å‹æœåŠ¡
curl http://localhost:8000/v1/models

# æˆ–è€…æµ‹è¯•èŠå¤©æ¥å£
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-32B-Instruct",
    "messages": [{"role": "user", "content": "Hello"}]
  }'
```

## ğŸ’¡ æ€§èƒ½è°ƒä¼˜å»ºè®®

### ç¡¬ä»¶è¦æ±‚

- **GPUå†…å­˜**: è‡³å°‘24GB (RTX 4090/A6000æˆ–æ›´é«˜)
- **ç³»ç»Ÿå†…å­˜**: è‡³å°‘32GB RAM
- **å­˜å‚¨**: SSDå­˜å‚¨ï¼Œè‡³å°‘100GBå¯ç”¨ç©ºé—´

### vLLMä¼˜åŒ–å‚æ•°

```bash
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-32B-Instruct \
    --tensor-parallel-size 2 \  # å¤šGPUå¹¶è¡Œ
    --gpu-memory-utilization 0.9 \  # GPUå†…å­˜ä½¿ç”¨ç‡
    --max-model-len 4096 \  # æœ€å¤§åºåˆ—é•¿åº¦
    --disable-log-stats \  # ç¦ç”¨ç»Ÿè®¡æ—¥å¿—
    --host 0.0.0.0 \
    --port 8000
```

### LM Studioä¼˜åŒ–

1. è®¾ç½®åˆé€‚çš„ä¸Šä¸‹æ–‡é•¿åº¦ (å»ºè®®4096)
2. å¯ç”¨GPUåŠ é€Ÿ
3. è°ƒæ•´æ‰¹å¤„ç†å¤§å°
4. è®¾ç½®åˆé€‚çš„æ¸©åº¦å‚æ•° (0.7)

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ¨¡å‹åŠ è½½å¤±è´¥**
   ```bash
   # æ£€æŸ¥æ¨¡å‹æ˜¯å¦ä¸‹è½½å®Œæ•´
   huggingface-cli download Qwen/Qwen2.5-32B-Instruct
   ```

2. **GPUå†…å­˜ä¸è¶³**
   ```bash
   # é™ä½GPUå†…å­˜ä½¿ç”¨ç‡
   --gpu-memory-utilization 0.6
   
   # æˆ–ä½¿ç”¨CPUæ¨ç† (è¾ƒæ…¢)
   --device cpu
   ```

3. **APIè¿æ¥å¤±è´¥**
   ```bash
   # æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ
   curl http://localhost:8000/v1/models
   
   # æ£€æŸ¥é˜²ç«å¢™è®¾ç½®
   netstat -an | grep :8000
   ```

### æ—¥å¿—è°ƒè¯•

```python
import logging
import requests

logging.basicConfig(level=logging.DEBUG)

# æµ‹è¯•APIè¿æ¥
response = requests.get("http://localhost:8000/v1/models")
print(f"APIçŠ¶æ€: {response.status_code}")
print(f"å¯ç”¨æ¨¡å‹: {response.json()}")
```

## ğŸ“Š æ€§èƒ½ç›‘æ§

### ç³»ç»Ÿç›‘æ§è„šæœ¬

```python
#!/usr/bin/env python3
import requests
import time
import psutil

def monitor_system():
    """ç›‘æ§ç³»ç»Ÿæ€§èƒ½"""
    while True:
        # æ£€æŸ¥APIå¥åº·çŠ¶æ€
        try:
            response = requests.get("http://localhost:8000/health")
            api_status = "âœ…" if response.status_code == 200 else "âŒ"
        except:
            api_status = "âŒ"
        
        # ç³»ç»Ÿèµ„æº
        cpu_percent = psutil.cpu_percent()
        memory_percent = psutil.virtual_memory().percent
        
        print(f"APIçŠ¶æ€: {api_status} | CPU: {cpu_percent:.1f}% | å†…å­˜: {memory_percent:.1f}%")
        time.sleep(10)

if __name__ == "__main__":
    monitor_system()
```

## ğŸ¯ æœ€ä½³å®è·µ

1. **å¼€å‘ç¯å¢ƒ**: ä½¿ç”¨LM Studioï¼Œä¾¿äºè°ƒè¯•å’Œæµ‹è¯•
2. **ç”Ÿäº§ç¯å¢ƒ**: ä½¿ç”¨vLLMï¼Œæ€§èƒ½æ›´å¥½ï¼Œæ”¯æŒå¹¶å‘
3. **æ¨¡å‹ç®¡ç†**: ä½¿ç”¨HuggingFace Hubç®¡ç†æ¨¡å‹ç‰ˆæœ¬
4. **èµ„æºç›‘æ§**: å®šæœŸç›‘æ§GPUå’Œå†…å­˜ä½¿ç”¨æƒ…å†µ
5. **å¤‡ä»½ç­–ç•¥**: å®šæœŸå¤‡ä»½æ¨¡å‹å’Œé…ç½®æ–‡ä»¶

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœåœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·ï¼š

1. æ£€æŸ¥æ—¥å¿—è¾“å‡º
2. éªŒè¯ç¡¬ä»¶é…ç½®
3. ç¡®è®¤ç½‘ç»œè¿æ¥
4. æŸ¥çœ‹å®˜æ–¹æ–‡æ¡£:
   - [vLLMæ–‡æ¡£](https://docs.vllm.ai/)
   - [Qwenæ¨¡å‹æ–‡æ¡£](https://github.com/QwenLM/Qwen)
   - [LM Studioæ–‡æ¡£](https://lmstudio.ai/docs)