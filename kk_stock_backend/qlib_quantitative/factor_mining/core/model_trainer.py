#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹è®­ç»ƒå™¨ - åŸºäºé€‰æ‹©å› å­çš„é¢„æµ‹æ¨¡å‹è®­ç»ƒ
æ”¯æŒå¤šç§æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå®ç°æ¨¡å‹é›†æˆå’Œè¶…å‚æ•°ä¼˜åŒ–
"""

import os
import sys
import pandas as pd
import numpy as np
import warnings
from typing import Dict, List, Tuple, Optional, Any, Union
from datetime import datetime
import yaml
import logging
import pickle
import joblib
from dataclasses import dataclass, asdict
import json

# æœºå™¨å­¦ä¹ åº“
from sklearn.ensemble import (
    RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor,
    VotingRegressor, StackingRegressor, BaggingRegressor
)
from sklearn.linear_model import (
    LinearRegression, Ridge, Lasso, ElasticNet, 
    RidgeCV, LassoCV, ElasticNetCV
)
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import (
    train_test_split, cross_val_score, GridSearchCV, 
    RandomizedSearchCV, TimeSeriesSplit
)
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    mean_absolute_percentage_error
)
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# é«˜çº§æœºå™¨å­¦ä¹ åº“
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostRegressor

# Optunaè¶…å‚æ•°ä¼˜åŒ–
try:
    import optuna
    from optuna.samplers import TPESampler
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False

# æ·±åº¦å­¦ä¹ åº“
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))))
from qlib_quantitative.factor_mining.core.factor_selector import FactorSelector
from api.global_db import db_handler

warnings.filterwarnings('ignore')

@dataclass
class ModelResult:
    """æ¨¡å‹ç»“æœæ•°æ®ç±»"""
    model_name: str
    model_type: str
    selected_factors: List[str]
    train_metrics: Dict[str, float]
    val_metrics: Dict[str, float]
    test_metrics: Dict[str, float]
    feature_importance: Optional[Dict[str, float]]
    hyperparameters: Dict[str, Any]
    cross_val_score: float
    model_path: Optional[str]
    training_date: datetime
    training_time: float
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        result = asdict(self)
        result['training_date'] = self.training_date.isoformat()
        return result


class ModelTrainer:
    """
    æ¨¡å‹è®­ç»ƒå™¨
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. å¤šç§æœºå™¨å­¦ä¹ ç®—æ³•æ”¯æŒ
    2. è¶…å‚æ•°è‡ªåŠ¨ä¼˜åŒ–
    3. æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
    4. æ¨¡å‹é›†æˆå’ŒStacking
    5. æ¨¡å‹æŒä¹…åŒ–å’Œç®¡ç†
    """
    
    def __init__(self, config_path: str = None):
        """
        åˆå§‹åŒ–æ¨¡å‹è®­ç»ƒå™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.logger = self._setup_logger()
        
        # åŠ è½½é…ç½®
        if config_path is None:
            config_path = os.path.join(os.path.dirname(__file__), "../config/factor_mining_config.yaml")
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
            self.logger.info(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            raise
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.factor_selector = FactorSelector(config_path)
        self.db_handler = db_handler
        
        # æ¨¡å‹å­˜å‚¨
        self.trained_models = {}
        self.model_results = {}
        
        # åˆ›å»ºæ¨¡å‹å­˜å‚¨ç›®å½•
        model_path = self.config['storage_config']['model_storage']['path']
        os.makedirs(model_path, exist_ok=True)
        
        self.logger.info("ğŸš€ æ¨¡å‹è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger(self.__class__.__name__)
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def prepare_training_data(self, 
                            selected_factors: List[str],
                            start_date: str = None,
                            end_date: str = None,
                            return_period: int = 20,
                            test_size: float = 0.2,
                            val_size: float = 0.2) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, 
                                                           pd.Series, pd.Series, pd.Series]:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®
        
        Args:
            selected_factors: é€‰æ‹©çš„å› å­åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            return_period: æ”¶ç›Šç‡å‘¨æœŸ
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            val_size: éªŒè¯é›†æ¯”ä¾‹
            
        Returns:
            (X_train, X_val, X_test, y_train, y_val, y_test)
        """
        try:
            self.logger.info("ğŸ“Š å¼€å§‹å‡†å¤‡è®­ç»ƒæ•°æ®")
            
            # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤æ—¥æœŸ
            if start_date is None:
                start_date = self.config['data_config']['train_period']['start_date']
            if end_date is None:
                end_date = self.config['data_config']['train_period']['end_date']
            
            # åŠ è½½A500æˆåˆ†è‚¡
            stock_codes = self.factor_selector.factor_analyzer.load_a500_universe()
            
            # åŠ è½½å› å­å’Œæ”¶ç›Šæ•°æ®
            factor_data = self.factor_selector.factor_analyzer.load_factor_data(
                stock_codes, start_date, end_date
            )
            return_data = self.factor_selector.factor_analyzer.calculate_forward_returns(
                stock_codes, start_date, end_date
            )
            
            if factor_data.empty or return_data.empty:
                raise ValueError("æ— æ³•åŠ è½½æ•°æ®")
            
            # é€‰æ‹©ç›¸å…³å› å­
            available_factors = [f for f in selected_factors if f in factor_data.columns]
            if not available_factors:
                raise ValueError("æ²¡æœ‰å¯ç”¨çš„é€‰æ‹©å› å­")
            
            self.logger.info(f"å¯ç”¨å› å­: {len(available_factors)}/{len(selected_factors)}")
            
            # åˆå¹¶æ•°æ®
            return_col = f'return_{return_period}d'
            merged_data = factor_data[available_factors].join(return_data[return_col], how='inner')
            merged_data = merged_data.dropna()
            
            if len(merged_data) < 1000:
                self.logger.warning("âš ï¸ æ ·æœ¬æ•°é‡è¾ƒå°‘ï¼Œå¯èƒ½å½±å“æ¨¡å‹æ•ˆæœ")
            
            # ç‰¹å¾å’Œç›®æ ‡
            X = merged_data[available_factors]
            y = merged_data[return_col]
            
            # æ—¶é—´åºåˆ—åˆ’åˆ†
            total_samples = len(X)
            train_end = int(total_samples * (1 - test_size - val_size))
            val_end = int(total_samples * (1 - test_size))
            
            X_train = X.iloc[:train_end]
            X_val = X.iloc[train_end:val_end]
            X_test = X.iloc[val_end:]
            
            y_train = y.iloc[:train_end]
            y_val = y.iloc[train_end:val_end]
            y_test = y.iloc[val_end:]
            
            self.logger.info(f"âœ… æ•°æ®åˆ’åˆ†å®Œæˆ")
            self.logger.info(f"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
            self.logger.info(f"  éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
            self.logger.info(f"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
            
            return X_train, X_val, X_test, y_train, y_val, y_test
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
            return None, None, None, None, None, None
    
    def train_linear_models(self, 
                          X_train: pd.DataFrame, 
                          y_train: pd.Series,
                          X_val: pd.DataFrame = None,
                          y_val: pd.Series = None) -> Dict[str, ModelResult]:
        """
        è®­ç»ƒçº¿æ€§æ¨¡å‹
        
        Args:
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒç›®æ ‡
            X_val: éªŒè¯ç‰¹å¾
            y_val: éªŒè¯ç›®æ ‡
            
        Returns:
            æ¨¡å‹ç»“æœå­—å…¸
        """
        try:
            self.logger.info("ğŸ“ˆ å¼€å§‹è®­ç»ƒçº¿æ€§æ¨¡å‹")
            results = {}
            
            # æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val) if X_val is not None else None
            
            # çº¿æ€§å›å½’
            lr = LinearRegression()
            start_time = datetime.now()
            lr.fit(X_train_scaled, y_train)
            training_time = (datetime.now() - start_time).total_seconds()
            
            results['linear_regression'] = self._evaluate_model(
                lr, X_train_scaled, y_train, X_val_scaled, y_val, 
                'linear_regression', 'linear', X_train.columns.tolist(),
                training_time=training_time, scaler=scaler
            )
            
            # Ridgeå›å½’
            ridge = RidgeCV(cv=5, alphas=np.logspace(-6, 2, 50))
            start_time = datetime.now()
            ridge.fit(X_train_scaled, y_train)
            training_time = (datetime.now() - start_time).total_seconds()
            
            results['ridge'] = self._evaluate_model(
                ridge, X_train_scaled, y_train, X_val_scaled, y_val,
                'ridge', 'linear', X_train.columns.tolist(),
                training_time=training_time, scaler=scaler,
                hyperparams={'alpha': float(ridge.alpha_)}
            )
            
            # Lassoå›å½’
            lasso = LassoCV(cv=5, alphas=np.logspace(-6, 2, 50), max_iter=2000)
            start_time = datetime.now()
            lasso.fit(X_train_scaled, y_train)
            training_time = (datetime.now() - start_time).total_seconds()
            
            results['lasso'] = self._evaluate_model(
                lasso, X_train_scaled, y_train, X_val_scaled, y_val,
                'lasso', 'linear', X_train.columns.tolist(),
                training_time=training_time, scaler=scaler,
                hyperparams={'alpha': float(lasso.alpha_)}
            )
            
            # ElasticNetå›å½’
            elastic = ElasticNetCV(cv=5, alphas=np.logspace(-6, 2, 20), 
                                 l1_ratio=[0.1, 0.5, 0.7, 0.9], max_iter=2000)
            start_time = datetime.now()
            elastic.fit(X_train_scaled, y_train)
            training_time = (datetime.now() - start_time).total_seconds()
            
            results['elastic_net'] = self._evaluate_model(
                elastic, X_train_scaled, y_train, X_val_scaled, y_val,
                'elastic_net', 'linear', X_train.columns.tolist(),
                training_time=training_time, scaler=scaler,
                hyperparams={'alpha': float(elastic.alpha_), 'l1_ratio': float(elastic.l1_ratio_)}
            )
            
            self.logger.info(f"âœ… çº¿æ€§æ¨¡å‹è®­ç»ƒå®Œæˆ: {len(results)}ä¸ªæ¨¡å‹")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ çº¿æ€§æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return {}
    
    def train_tree_models(self,
                        X_train: pd.DataFrame,
                        y_train: pd.Series,
                        X_val: pd.DataFrame = None,
                        y_val: pd.Series = None) -> Dict[str, ModelResult]:
        """
        è®­ç»ƒæ ‘æ¨¡å‹
        
        Args:
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒç›®æ ‡
            X_val: éªŒè¯ç‰¹å¾
            y_val: éªŒè¯ç›®æ ‡
            
        Returns:
            æ¨¡å‹ç»“æœå­—å…¸
        """
        try:
            self.logger.info("ğŸŒ³ å¼€å§‹è®­ç»ƒæ ‘æ¨¡å‹")
            results = {}
            
            # éšæœºæ£®æ—
            rf = RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            )
            start_time = datetime.now()
            rf.fit(X_train, y_train)
            training_time = (datetime.now() - start_time).total_seconds()
            
            results['random_forest'] = self._evaluate_model(
                rf, X_train, y_train, X_val, y_val,
                'random_forest', 'tree', X_train.columns.tolist(),
                training_time=training_time
            )
            
            # XGBoost
            xgb_model = xgb.XGBRegressor(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                n_jobs=-1
            )
            start_time = datetime.now()
            xgb_model.fit(X_train, y_train, 
                         eval_set=[(X_val, y_val)] if X_val is not None else None,
                         verbose=False)
            training_time = (datetime.now() - start_time).total_seconds()
            
            results['xgboost'] = self._evaluate_model(
                xgb_model, X_train, y_train, X_val, y_val,
                'xgboost', 'tree', X_train.columns.tolist(),
                training_time=training_time
            )
            
            # LightGBM
            lgb_model = lgb.LGBMRegressor(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                n_jobs=-1,
                verbose=-1
            )
            start_time = datetime.now()
            lgb_model.fit(X_train, y_train,
                         eval_set=[(X_val, y_val)] if X_val is not None else None,
                         callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])
            training_time = (datetime.now() - start_time).total_seconds()
            
            results['lightgbm'] = self._evaluate_model(
                lgb_model, X_train, y_train, X_val, y_val,
                'lightgbm', 'tree', X_train.columns.tolist(),
                training_time=training_time
            )
            
            # CatBoost
            try:
                cat_model = CatBoostRegressor(
                    iterations=100,
                    depth=6,
                    learning_rate=0.1,
                    random_seed=42,
                    verbose=False
                )
                start_time = datetime.now()
                cat_model.fit(X_train, y_train,
                             eval_set=(X_val, y_val) if X_val is not None else None,
                             verbose=False)
                training_time = (datetime.now() - start_time).total_seconds()
                
                results['catboost'] = self._evaluate_model(
                    cat_model, X_train, y_train, X_val, y_val,
                    'catboost', 'tree', X_train.columns.tolist(),
                    training_time=training_time
                )
            except Exception as e:
                self.logger.warning(f"âš ï¸ CatBoostè®­ç»ƒå¤±è´¥: {e}")
            
            # é¢å¤–æ ‘å›å½’å™¨
            et = ExtraTreesRegressor(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            )
            start_time = datetime.now()
            et.fit(X_train, y_train)
            training_time = (datetime.now() - start_time).total_seconds()
            
            results['extra_trees'] = self._evaluate_model(
                et, X_train, y_train, X_val, y_val,
                'extra_trees', 'tree', X_train.columns.tolist(),
                training_time=training_time
            )
            
            self.logger.info(f"âœ… æ ‘æ¨¡å‹è®­ç»ƒå®Œæˆ: {len(results)}ä¸ªæ¨¡å‹")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ æ ‘æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return {}
    
    def train_neural_models(self,
                          X_train: pd.DataFrame,
                          y_train: pd.Series,
                          X_val: pd.DataFrame = None,
                          y_val: pd.Series = None) -> Dict[str, ModelResult]:
        """
        è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹
        
        Args:
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒç›®æ ‡
            X_val: éªŒè¯ç‰¹å¾
            y_val: éªŒè¯ç›®æ ‡
            
        Returns:
            æ¨¡å‹ç»“æœå­—å…¸
        """
        try:
            self.logger.info("ğŸ§  å¼€å§‹è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹")
            results = {}
            
            # æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val) if X_val is not None else None
            
            # MLPå›å½’å™¨
            mlp = MLPRegressor(
                hidden_layer_sizes=(100, 50),
                activation='relu',
                solver='adam',
                alpha=0.01,
                learning_rate_init=0.001,
                max_iter=500,
                early_stopping=True,
                validation_fraction=0.2,
                random_state=42
            )
            
            start_time = datetime.now()
            mlp.fit(X_train_scaled, y_train)
            training_time = (datetime.now() - start_time).total_seconds()
            
            results['mlp'] = self._evaluate_model(
                mlp, X_train_scaled, y_train, X_val_scaled, y_val,
                'mlp', 'neural', X_train.columns.tolist(),
                training_time=training_time, scaler=scaler
            )
            
            # PyTorchæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if TORCH_AVAILABLE:
                try:
                    torch_result = self._train_pytorch_model(
                        X_train_scaled, y_train, X_val_scaled, y_val
                    )
                    if torch_result:
                        results['deep_neural'] = torch_result
                except Exception as e:
                    self.logger.warning(f"âš ï¸ PyTorchæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            
            self.logger.info(f"âœ… ç¥ç»ç½‘ç»œæ¨¡å‹è®­ç»ƒå®Œæˆ: {len(results)}ä¸ªæ¨¡å‹")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ç¥ç»ç½‘ç»œæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return {}
    
    def _train_pytorch_model(self, X_train, y_train, X_val=None, y_val=None):
        """è®­ç»ƒPyTorchæ·±åº¦å­¦ä¹ æ¨¡å‹"""
        try:
            if not TORCH_AVAILABLE:
                return None
            
            # è½¬æ¢ä¸ºå¼ é‡
            X_train_tensor = torch.FloatTensor(X_train)
            y_train_tensor = torch.FloatTensor(y_train.values).reshape(-1, 1)
            
            if X_val is not None:
                X_val_tensor = torch.FloatTensor(X_val)
                y_val_tensor = torch.FloatTensor(y_val.values).reshape(-1, 1)
            
            # å®šä¹‰ç½‘ç»œç»“æ„
            class DeepNN(nn.Module):
                def __init__(self, input_size):
                    super(DeepNN, self).__init__()
                    self.layers = nn.Sequential(
                        nn.Linear(input_size, 128),
                        nn.ReLU(),
                        nn.Dropout(0.2),
                        nn.Linear(128, 64),
                        nn.ReLU(),
                        nn.Dropout(0.2),
                        nn.Linear(64, 32),
                        nn.ReLU(),
                        nn.Linear(32, 1)
                    )
                
                def forward(self, x):
                    return self.layers(x)
            
            # åˆ›å»ºæ¨¡å‹
            model = DeepNN(X_train.shape[1])
            criterion = nn.MSELoss()
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            
            # è®­ç»ƒ
            start_time = datetime.now()
            model.train()
            
            for epoch in range(200):
                optimizer.zero_grad()
                outputs = model(X_train_tensor)
                loss = criterion(outputs, y_train_tensor)
                loss.backward()
                optimizer.step()
                
                if epoch % 50 == 0:
                    self.logger.info(f"  Epoch {epoch}, Loss: {loss.item():.6f}")
            
            training_time = (datetime.now() - start_time).total_seconds()
            
            # è¯„ä¼°
            model.eval()
            with torch.no_grad():
                train_pred = model(X_train_tensor).numpy()
                train_mse = mean_squared_error(y_train, train_pred)
                train_r2 = r2_score(y_train, train_pred)
                
                if X_val is not None:
                    val_pred = model(X_val_tensor).numpy()
                    val_mse = mean_squared_error(y_val, val_pred)
                    val_r2 = r2_score(y_val, val_pred)
                else:
                    val_mse = val_r2 = 0.0
            
            # åˆ›å»ºç»“æœ
            result = ModelResult(
                model_name='deep_neural',
                model_type='neural',
                selected_factors=[],
                train_metrics={'mse': train_mse, 'r2': train_r2},
                val_metrics={'mse': val_mse, 'r2': val_r2},
                test_metrics={},
                feature_importance=None,
                hyperparameters={},
                cross_val_score=0.0,
                model_path=None,
                training_date=datetime.now(),
                training_time=training_time
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ PyTorchæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return None
    
    def train_ensemble_models(self,
                            X_train: pd.DataFrame,
                            y_train: pd.Series,
                            X_val: pd.DataFrame = None,
                            y_val: pd.Series = None,
                            base_models: Dict = None) -> Dict[str, ModelResult]:
        """
        è®­ç»ƒé›†æˆæ¨¡å‹
        
        Args:
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒç›®æ ‡
            X_val: éªŒè¯ç‰¹å¾
            y_val: éªŒè¯ç›®æ ‡
            base_models: åŸºç¡€æ¨¡å‹å­—å…¸
            
        Returns:
            é›†æˆæ¨¡å‹ç»“æœå­—å…¸
        """
        try:
            self.logger.info("ğŸ¯ å¼€å§‹è®­ç»ƒé›†æˆæ¨¡å‹")
            results = {}
            
            # åŸºç¡€æ¨¡å‹
            if base_models is None:
                base_models = {
                    'rf': RandomForestRegressor(n_estimators=50, random_state=42),
                    'xgb': xgb.XGBRegressor(n_estimators=50, random_state=42),
                    'lgb': lgb.LGBMRegressor(n_estimators=50, random_state=42, verbose=-1)
                }
            
            # Votingå›å½’å™¨
            voting_regressor = VotingRegressor(
                estimators=list(base_models.items())
            )
            start_time = datetime.now()
            voting_regressor.fit(X_train, y_train)
            training_time = (datetime.now() - start_time).total_seconds()
            
            results['voting'] = self._evaluate_model(
                voting_regressor, X_train, y_train, X_val, y_val,
                'voting', 'ensemble', X_train.columns.tolist(),
                training_time=training_time
            )
            
            # Stackingå›å½’å™¨
            try:
                stacking_regressor = StackingRegressor(
                    estimators=list(base_models.items()),
                    final_estimator=Ridge(),
                    cv=3
                )
                start_time = datetime.now()
                stacking_regressor.fit(X_train, y_train)
                training_time = (datetime.now() - start_time).total_seconds()
                
                results['stacking'] = self._evaluate_model(
                    stacking_regressor, X_train, y_train, X_val, y_val,
                    'stacking', 'ensemble', X_train.columns.tolist(),
                    training_time=training_time
                )
            except Exception as e:
                self.logger.warning(f"âš ï¸ Stackingæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            
            # Baggingå›å½’å™¨
            bagging_regressor = BaggingRegressor(
                base_estimator=RandomForestRegressor(n_estimators=20, random_state=42),
                n_estimators=10,
                random_state=42,
                n_jobs=-1
            )
            start_time = datetime.now()
            bagging_regressor.fit(X_train, y_train)
            training_time = (datetime.now() - start_time).total_seconds()
            
            results['bagging'] = self._evaluate_model(
                bagging_regressor, X_train, y_train, X_val, y_val,
                'bagging', 'ensemble', X_train.columns.tolist(),
                training_time=training_time
            )
            
            self.logger.info(f"âœ… é›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ: {len(results)}ä¸ªæ¨¡å‹")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ é›†æˆæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return {}
    
    def _evaluate_model(self, model, X_train, y_train, X_val=None, y_val=None,
                       model_name='', model_type='', selected_factors=None,
                       training_time=0.0, scaler=None, hyperparams=None):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        try:
            # è®­ç»ƒé›†é¢„æµ‹
            train_pred = model.predict(X_train)
            train_metrics = {
                'mse': float(mean_squared_error(y_train, train_pred)),
                'mae': float(mean_absolute_error(y_train, train_pred)),
                'r2': float(r2_score(y_train, train_pred))
            }
            
            # éªŒè¯é›†é¢„æµ‹
            val_metrics = {}
            if X_val is not None and y_val is not None:
                val_pred = model.predict(X_val)
                val_metrics = {
                    'mse': float(mean_squared_error(y_val, val_pred)),
                    'mae': float(mean_absolute_error(y_val, val_pred)),
                    'r2': float(r2_score(y_val, val_pred))
                }
            
            # ç‰¹å¾é‡è¦æ€§
            feature_importance = None
            if hasattr(model, 'feature_importances_'):
                if selected_factors:
                    feature_importance = dict(zip(selected_factors, model.feature_importances_))
            elif hasattr(model, 'coef_'):
                if selected_factors and hasattr(model.coef_, '__len__'):
                    feature_importance = dict(zip(selected_factors, np.abs(model.coef_)))
            
            # äº¤å‰éªŒè¯å¾—åˆ†
            try:
                cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring='neg_mean_squared_error')
                cv_score = float(-np.mean(cv_scores))
            except:
                cv_score = 0.0
            
            # ä¿å­˜æ¨¡å‹
            model_path = self._save_model(model, model_name, scaler)
            
            result = ModelResult(
                model_name=model_name,
                model_type=model_type,
                selected_factors=selected_factors or [],
                train_metrics=train_metrics,
                val_metrics=val_metrics,
                test_metrics={},
                feature_importance=feature_importance,
                hyperparameters=hyperparams or {},
                cross_val_score=cv_score,
                model_path=model_path,
                training_date=datetime.now(),
                training_time=training_time
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            return None
    
    def _save_model(self, model, model_name, scaler=None):
        """ä¿å­˜æ¨¡å‹"""
        try:
            model_dir = self.config['storage_config']['model_storage']['path']
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ä¿å­˜æ¨¡å‹
            model_filename = f"{model_name}_{timestamp}.pkl"
            model_path = os.path.join(model_dir, model_filename)
            
            # ä¿å­˜æ¨¡å‹å’Œé¢„å¤„ç†å™¨
            model_package = {
                'model': model,
                'scaler': scaler,
                'model_name': model_name,
                'training_date': datetime.now()
            }
            
            with open(model_path, 'wb') as f:
                pickle.dump(model_package, f)
            
            self.logger.info(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")
            return model_path
            
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
            return None
    
    def run_comprehensive_training(self,
                                 selected_factors: List[str] = None,
                                 start_date: str = None,
                                 end_date: str = None,
                                 return_period: int = 20) -> Dict[str, ModelResult]:
        """
        è¿è¡Œå…¨é¢çš„æ¨¡å‹è®­ç»ƒæµç¨‹
        
        Args:
            selected_factors: é€‰æ‹©çš„å› å­åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            return_period: æ”¶ç›Šç‡å‘¨æœŸ
            
        Returns:
            æ‰€æœ‰æ¨¡å‹ç»“æœå­—å…¸
        """
        try:
            self.logger.info("ğŸš€ å¼€å§‹å…¨é¢æ¨¡å‹è®­ç»ƒæµç¨‹")
            
            # 1. å¦‚æœæ²¡æœ‰æä¾›å› å­ï¼Œå…ˆè¿è¡Œå› å­é€‰æ‹©
            if selected_factors is None:
                self.logger.info("ğŸ“Š æœªæä¾›é€‰æ‹©å› å­ï¼Œè¿è¡Œå› å­é€‰æ‹©...")
                selection_results = self.factor_selector.run_comprehensive_selection(
                    start_date, end_date, return_period
                )
                if not selection_results:
                    raise ValueError("å› å­é€‰æ‹©å¤±è´¥")
                
                selected_factors = self.factor_selector.get_selected_factors()
                if not selected_factors:
                    raise ValueError("æ²¡æœ‰é€‰æ‹©åˆ°æœ‰æ•ˆå› å­")
            
            self.logger.info(f"ğŸ¯ ä½¿ç”¨{len(selected_factors)}ä¸ªé€‰æ‹©å› å­è¿›è¡Œè®­ç»ƒ")
            
            # 2. å‡†å¤‡æ•°æ®
            data_result = self.prepare_training_data(
                selected_factors, start_date, end_date, return_period
            )
            
            if data_result[0] is None:
                raise ValueError("æ•°æ®å‡†å¤‡å¤±è´¥")
            
            X_train, X_val, X_test, y_train, y_val, y_test = data_result
            
            # 3. è®­ç»ƒå„ç±»æ¨¡å‹
            all_results = {}
            
            # çº¿æ€§æ¨¡å‹
            linear_results = self.train_linear_models(X_train, y_train, X_val, y_val)
            all_results.update(linear_results)
            
            # æ ‘æ¨¡å‹
            tree_results = self.train_tree_models(X_train, y_train, X_val, y_val)
            all_results.update(tree_results)
            
            # ç¥ç»ç½‘ç»œæ¨¡å‹
            neural_results = self.train_neural_models(X_train, y_train, X_val, y_val)
            all_results.update(neural_results)
            
            # é›†æˆæ¨¡å‹
            ensemble_results = self.train_ensemble_models(X_train, y_train, X_val, y_val)
            all_results.update(ensemble_results)
            
            # 4. æµ‹è¯•é›†è¯„ä¼°
            self.logger.info("ğŸ“Š è¿›è¡Œæµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°...")
            for model_name, result in all_results.items():
                try:
                    # åŠ è½½æ¨¡å‹è¿›è¡Œæµ‹è¯•
                    if result.model_path and os.path.exists(result.model_path):
                        with open(result.model_path, 'rb') as f:
                            model_package = pickle.load(f)
                        
                        model = model_package['model']
                        scaler = model_package.get('scaler')
                        
                        # é¢„æµ‹æµ‹è¯•é›†
                        X_test_processed = X_test
                        if scaler:
                            X_test_processed = scaler.transform(X_test)
                        
                        test_pred = model.predict(X_test_processed)
                        
                        # è®¡ç®—æµ‹è¯•æŒ‡æ ‡
                        test_metrics = {
                            'mse': float(mean_squared_error(y_test, test_pred)),
                            'mae': float(mean_absolute_error(y_test, test_pred)),
                            'r2': float(r2_score(y_test, test_pred))
                        }
                        
                        result.test_metrics = test_metrics
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {model_name} æµ‹è¯•é›†è¯„ä¼°å¤±è´¥: {e}")
            
            # 5. ä¿å­˜ç»“æœ
            self.model_results = all_results
            self.trained_models = {name: result for name, result in all_results.items()}
            
            self.logger.info(f"âœ… æ¨¡å‹è®­ç»ƒæµç¨‹å®Œæˆï¼Œè®­ç»ƒäº†{len(all_results)}ä¸ªæ¨¡å‹")
            
            # 6. æ˜¾ç¤ºæœ€ä½³æ¨¡å‹
            best_model = self.get_best_model()
            if best_model:
                self.logger.info(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model[0]} (éªŒè¯RÂ²: {best_model[1].val_metrics.get('r2', 0):.4f})")
            
            return all_results
            
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹è®­ç»ƒæµç¨‹å¤±è´¥: {e}")
            return {}
    
    def get_best_model(self, metric: str = 'r2') -> Optional[Tuple[str, ModelResult]]:
        """
        è·å–æœ€ä½³æ¨¡å‹
        
        Args:
            metric: è¯„ä¼°æŒ‡æ ‡ (r2, mse, mae)
            
        Returns:
            (æ¨¡å‹åç§°, æ¨¡å‹ç»“æœ) æˆ– None
        """
        try:
            if not self.model_results:
                return None
            
            best_score = None
            best_model = None
            
            for name, result in self.model_results.items():
                if not result.val_metrics:
                    continue
                
                score = result.val_metrics.get(metric, 0)
                
                # RÂ²è¶Šå¤§è¶Šå¥½ï¼ŒMSE/MAEè¶Šå°è¶Šå¥½
                is_better = False
                if metric == 'r2':
                    is_better = best_score is None or score > best_score
                else:  # mse, mae
                    is_better = best_score is None or score < best_score
                
                if is_better:
                    best_score = score
                    best_model = (name, result)
            
            return best_model
            
        except Exception as e:
            self.logger.error(f"âŒ è·å–æœ€ä½³æ¨¡å‹å¤±è´¥: {e}")
            return None
    
    def get_model_ranking(self, metric: str = 'r2') -> List[Tuple[str, float]]:
        """
        è·å–æ¨¡å‹æ’å
        
        Args:
            metric: è¯„ä¼°æŒ‡æ ‡
            
        Returns:
            [(æ¨¡å‹åç§°, å¾—åˆ†), ...] æŒ‰å¾—åˆ†æ’åº
        """
        try:
            if not self.model_results:
                return []
            
            model_scores = []
            for name, result in self.model_results.items():
                if result.val_metrics:
                    score = result.val_metrics.get(metric, 0)
                    model_scores.append((name, score))
            
            # æ’åº
            reverse = metric == 'r2'  # RÂ²é™åºï¼ŒMSE/MAEå‡åº
            model_scores.sort(key=lambda x: x[1], reverse=reverse)
            
            return model_scores
            
        except Exception as e:
            self.logger.error(f"âŒ è·å–æ¨¡å‹æ’åå¤±è´¥: {e}")
            return []


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸš€ æµ‹è¯•æ¨¡å‹è®­ç»ƒå™¨...")
    
    try:
        trainer = ModelTrainer()
        
        # è¿è¡Œå…¨é¢è®­ç»ƒ
        results = trainer.run_comprehensive_training(
            start_date="2023-01-01",
            end_date="2023-12-31",
            return_period=20
        )
        
        if results:
            print("âœ… æ¨¡å‹è®­ç»ƒæµ‹è¯•æˆåŠŸ")
            print(f"ğŸ“Š è®­ç»ƒæ¨¡å‹æ•°é‡: {len(results)}")
            
            # è·å–æœ€ä½³æ¨¡å‹
            best_model = trainer.get_best_model()
            if best_model:
                print(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model[0]}")
                print(f"  éªŒè¯RÂ²: {best_model[1].val_metrics.get('r2', 0):.4f}")
                print(f"  éªŒè¯MSE: {best_model[1].val_metrics.get('mse', 0):.6f}")
            
            # è·å–æ¨¡å‹æ’å
            ranking = trainer.get_model_ranking()
            print(f"ğŸ“ˆ æ¨¡å‹æ’å(æŒ‰RÂ²):")
            for i, (name, score) in enumerate(ranking[:5], 1):
                print(f"  {i}. {name}: {score:.4f}")
                
        else:
            print("âŒ æ¨¡å‹è®­ç»ƒæµ‹è¯•å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()