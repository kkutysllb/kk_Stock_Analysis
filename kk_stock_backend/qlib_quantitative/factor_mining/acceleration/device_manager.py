"""
è®¾å¤‡ç®¡ç†å™¨ - è‡ªåŠ¨æ£€æµ‹å’Œé…ç½®ç¡¬ä»¶åŠ é€Ÿè®¾å¤‡
æ”¯æŒNVIDIA GPU (CUDA)ã€Apple Silicon (MPS)ã€CPU
"""

import os
import platform
import logging
from typing import Dict, List, Optional, Tuple, Any
import psutil

class DeviceManager:
    """ç¡¬ä»¶è®¾å¤‡ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–è®¾å¤‡ç®¡ç†å™¨
        
        Args:
            config: åŠ é€Ÿé…ç½®
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # è®¾å¤‡çŠ¶æ€
        self.device = None
        self.device_type = None
        self.device_count = 0
        self.available_memory = 0
        
        # æ£€æµ‹å¹¶åˆå§‹åŒ–è®¾å¤‡
        self._detect_and_initialize_device()
    
    def _detect_and_initialize_device(self):
        """æ£€æµ‹å¹¶åˆå§‹åŒ–æœ€ä¼˜è®¾å¤‡"""
        device_priority = self.config.get('device_config', {}).get('device_priority', ['cuda', 'mps', 'cpu'])
        
        for device_type in device_priority:
            if self._try_initialize_device(device_type):
                self.device_type = device_type
                break
        
        self.logger.info(f"ğŸš€ å·²é€‰æ‹©è®¾å¤‡: {self.device_type}")
        self._log_device_info()
    
    def _try_initialize_device(self, device_type: str) -> bool:
        """å°è¯•åˆå§‹åŒ–æŒ‡å®šç±»å‹çš„è®¾å¤‡"""
        try:
            if device_type == 'cuda':
                return self._initialize_cuda()
            elif device_type == 'mps':
                return self._initialize_mps()
            elif device_type == 'cpu':
                return self._initialize_cpu()
            else:
                return False
        except Exception as e:
            self.logger.warning(f"âš ï¸ {device_type.upper()}åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _initialize_cuda(self) -> bool:
        """åˆå§‹åŒ–CUDAè®¾å¤‡"""
        cuda_config = self.config.get('device_config', {}).get('cuda', {})
        if not cuda_config.get('enabled', True):
            return False
            
        try:
            import torch
            if not torch.cuda.is_available():
                return False
            
            # æ£€æŸ¥GPUæ•°é‡å’Œå‹å·
            device_count = torch.cuda.device_count()
            if device_count == 0:
                return False
            
            # è·å–æŒ‡å®šçš„è®¾å¤‡ID
            device_ids = cuda_config.get('device_ids', [0])
            valid_device_ids = [i for i in device_ids if i < device_count]
            
            if not valid_device_ids:
                return False
            
            # è®¾ç½®ä¸»è®¾å¤‡
            primary_device = valid_device_ids[0]
            torch.cuda.set_device(primary_device)
            self.device = torch.device(f'cuda:{primary_device}')
            self.device_count = len(valid_device_ids)
            
            # è·å–æ˜¾å­˜ä¿¡æ¯
            total_memory = torch.cuda.get_device_properties(primary_device).total_memory
            self.available_memory = total_memory / (1024**3)  # GB
            
            # é…ç½®å†…å­˜ç®¡ç†
            if cuda_config.get('allow_growth', True):
                # PyTorché»˜è®¤å°±æ˜¯åŠ¨æ€åˆ†é…ï¼Œæ— éœ€ç‰¹æ®Šè®¾ç½®
                pass
            
            # æ··åˆç²¾åº¦é…ç½®
            if cuda_config.get('mixed_precision', True):
                self.mixed_precision = True
            
            self.logger.info(f"âœ… CUDAåˆå§‹åŒ–æˆåŠŸ: {device_count}å¼ GPUå¯ç”¨")
            for i, device_id in enumerate(valid_device_ids):
                gpu_name = torch.cuda.get_device_name(device_id)
                gpu_memory = torch.cuda.get_device_properties(device_id).total_memory / (1024**3)
                self.logger.info(f"   GPU {device_id}: {gpu_name} ({gpu_memory:.1f}GB)")
            
            return True
            
        except ImportError:
            self.logger.warning("âš ï¸ PyTorchæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨CUDA")
            return False
        except Exception as e:
            self.logger.error(f"âŒ CUDAåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _initialize_mps(self) -> bool:
        """åˆå§‹åŒ–MPSè®¾å¤‡ (Apple Silicon)"""
        mps_config = self.config.get('device_config', {}).get('mps', {})
        if not mps_config.get('enabled', True):
            return False
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºMacç³»ç»Ÿ
        if platform.system() != 'Darwin':
            return False
            
        try:
            import torch
            if not torch.backends.mps.is_available():
                return False
            
            self.device = torch.device('mps')
            self.device_count = 1
            
            # è·å–ç³»ç»Ÿå†…å­˜ä½œä¸ºå‚è€ƒ
            total_memory = psutil.virtual_memory().total / (1024**3)
            memory_fraction = mps_config.get('memory_fraction', 0.7)
            self.available_memory = total_memory * memory_fraction
            
            self.logger.info(f"âœ… MPSåˆå§‹åŒ–æˆåŠŸ: Apple Silicon GPU")
            self.logger.info(f"   å¯ç”¨å†…å­˜: {self.available_memory:.1f}GB")
            
            return True
            
        except ImportError:
            self.logger.warning("âš ï¸ PyTorch MPSæ”¯æŒæœªå®‰è£…")
            return False
        except Exception as e:
            self.logger.error(f"âŒ MPSåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _initialize_cpu(self) -> bool:
        """åˆå§‹åŒ–CPUè®¾å¤‡"""
        cpu_config = self.config.get('device_config', {}).get('cpu', {})
        
        try:
            # CPUåŸºæœ¬ä¿¡æ¯ - ä¸ä¾èµ–PyTorch
            cpu_count = psutil.cpu_count(logical=True)
            total_memory = psutil.virtual_memory().total / (1024**3)
            self.available_memory = total_memory * 0.8  # é¢„ç•™20%å†…å­˜
            self.device_count = 1
            
            # è®¾ç½®çº¿ç¨‹æ•°
            n_jobs = cpu_config.get('n_jobs', -1)
            if n_jobs == -1:
                n_jobs = cpu_count
            
            # å°è¯•è®¾ç½®PyTorchçº¿ç¨‹æ•° (å¦‚æœå¯ç”¨)
            try:
                import torch
                self.device = torch.device('cpu')
                torch.set_num_threads(min(n_jobs, cpu_count))
                self.logger.info(f"   PyTorch CPUåç«¯: å·²å¯ç”¨")
            except ImportError:
                self.device = None  # æ²¡æœ‰PyTorchï¼Œä½¿ç”¨çº¯NumPy/Pandas
                self.logger.info(f"   PyTorch CPUåç«¯: æœªå®‰è£…ï¼Œä½¿ç”¨NumPy/Pandas")
            
            # è®¾ç½®NumPyçº¿ç¨‹æ•°
            try:
                import os
                os.environ['OMP_NUM_THREADS'] = str(min(n_jobs, cpu_count))
                os.environ['MKL_NUM_THREADS'] = str(min(n_jobs, cpu_count))
                os.environ['NUMEXPR_NUM_THREADS'] = str(min(n_jobs, cpu_count))
            except:
                pass
            
            self.logger.info(f"âœ… CPUåˆå§‹åŒ–æˆåŠŸ")
            self.logger.info(f"   CPUæ ¸å¿ƒæ•°: {cpu_count}")
            self.logger.info(f"   ä½¿ç”¨çº¿ç¨‹æ•°: {min(n_jobs, cpu_count)}")
            self.logger.info(f"   å¯ç”¨å†…å­˜: {self.available_memory:.1f}GB")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ CPUåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _log_device_info(self):
        """è®°å½•è®¾å¤‡ä¿¡æ¯"""
        self.logger.info("=" * 60)
        self.logger.info("ğŸ–¥ï¸  ç¡¬ä»¶é…ç½®ä¿¡æ¯")
        self.logger.info("=" * 60)
        self.logger.info(f"ğŸ“± æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
        self.logger.info(f"ğŸ”§ è®¾å¤‡ç±»å‹: {self.device_type.upper()}")
        self.logger.info(f"ğŸ’¾ å¯ç”¨å†…å­˜: {self.available_memory:.1f}GB")
        self.logger.info(f"ğŸ¯ è®¾å¤‡æ•°é‡: {self.device_count}")
        
        if self.device_type == 'cuda':
            self.logger.info(f"âš¡ æ··åˆç²¾åº¦: {'å¯ç”¨' if hasattr(self, 'mixed_precision') else 'ç¦ç”¨'}")
        
        self.logger.info("=" * 60)
    
    def get_optimal_batch_size(self, base_batch_size: int, factor_count: int, stock_count: int) -> int:
        """
        æ ¹æ®è®¾å¤‡æ€§èƒ½è®¡ç®—æœ€ä¼˜æ‰¹å¤„ç†å¤§å°
        
        Args:
            base_batch_size: åŸºç¡€æ‰¹å¤§å°
            factor_count: å› å­æ•°é‡
            stock_count: è‚¡ç¥¨æ•°é‡
            
        Returns:
            ä¼˜åŒ–åçš„æ‰¹å¤§å°
        """
        try:
            # ä¼°ç®—å†…å­˜éœ€æ±‚ (GB)
            # å‡è®¾æ¯ä¸ªå› å­æ¯åªè‚¡ç¥¨æ¯å¤©å ç”¨8å­—èŠ‚ (float64)
            memory_per_batch = base_batch_size * factor_count * 1000 * 8 / (1024**3)  # 1000å¤©æ•°æ®
            
            # æ ¹æ®å¯ç”¨å†…å­˜è°ƒæ•´æ‰¹å¤§å°
            if memory_per_batch > self.available_memory * 0.5:  # ä¸è¶…è¿‡50%å†…å­˜
                scale_factor = (self.available_memory * 0.5) / memory_per_batch
                optimal_batch_size = max(int(base_batch_size * scale_factor), 10)
            else:
                optimal_batch_size = base_batch_size
            
            # è®¾å¤‡ç±»å‹ç‰¹å®šä¼˜åŒ–
            if self.device_type == 'cuda':
                # GPUå¯ä»¥å¤„ç†æ›´å¤§çš„æ‰¹
                optimal_batch_size = min(optimal_batch_size * 2, stock_count)
            elif self.device_type == 'mps':
                # MPSå†…å­˜ç®¡ç†è¾ƒä¿å®ˆ
                optimal_batch_size = min(optimal_batch_size, 200)
            
            # ç¡®ä¿æ‰¹å¤§å°åœ¨åˆç†èŒƒå›´å†…
            batch_config = self.config.get('performance_optimization', {}).get('batch_processing', {})
            min_batch = batch_config.get('min_batch_size', 10)
            max_batch = batch_config.get('max_batch_size', 1000)
            
            optimal_batch_size = max(min_batch, min(optimal_batch_size, max_batch))
            
            self.logger.info(f"ğŸ“Š ä¼˜åŒ–æ‰¹å¤§å°: {base_batch_size} -> {optimal_batch_size}")
            return optimal_batch_size
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ æ‰¹å¤§å°ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
            return base_batch_size
    
    def get_device_info(self) -> Dict[str, Any]:
        """è·å–è®¾å¤‡ä¿¡æ¯"""
        return {
            'device': str(self.device) if self.device else None,
            'device_type': self.device_type,
            'device_count': self.device_count,
            'available_memory_gb': self.available_memory,
            'mixed_precision': hasattr(self, 'mixed_precision') and self.mixed_precision
        }
    
    def move_to_device(self, tensor):
        """å°†å¼ é‡ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
        if self.device is not None and hasattr(tensor, 'to'):
            return tensor.to(self.device)
        return tensor
    
    def clear_cache(self):
        """æ¸…ç†è®¾å¤‡ç¼“å­˜"""
        try:
            if self.device_type == 'cuda':
                import torch
                torch.cuda.empty_cache()
                self.logger.debug("ğŸ§¹ CUDAç¼“å­˜å·²æ¸…ç†")
            elif self.device_type == 'mps':
                import torch
                if hasattr(torch.mps, 'empty_cache'):
                    torch.mps.empty_cache()
                    self.logger.debug("ğŸ§¹ MPSç¼“å­˜å·²æ¸…ç†")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")

def create_device_manager(config: Dict[str, Any]) -> DeviceManager:
    """åˆ›å»ºè®¾å¤‡ç®¡ç†å™¨çš„å·¥å‚å‡½æ•°"""
    return DeviceManager(config.get('acceleration_config', {}))