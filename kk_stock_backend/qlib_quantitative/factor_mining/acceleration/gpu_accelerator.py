"""
GPUåŠ é€Ÿè®¡ç®—æ¨¡å—
æä¾›å› å­è®¡ç®—ã€çŸ©é˜µè¿ç®—ã€æœºå™¨å­¦ä¹ çš„GPUåŠ é€ŸåŠŸèƒ½
"""

import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Optional, Tuple, Any, Union
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import gc

class GPUAccelerator:
    """GPUåŠ é€Ÿè®¡ç®—å™¨"""
    
    def __init__(self, device_manager, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–GPUåŠ é€Ÿå™¨
        
        Args:
            device_manager: è®¾å¤‡ç®¡ç†å™¨
            config: åŠ é€Ÿé…ç½®
        """
        self.device_manager = device_manager
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # åŠ é€Ÿé…ç½®
        self.compute_config = config.get('compute_acceleration', {})
        self.performance_config = config.get('performance_optimization', {})
        
        # åˆå§‹åŒ–è®¡ç®—åç«¯
        self._initialize_backends()
    
    def _initialize_backends(self):
        """åˆå§‹åŒ–è®¡ç®—åç«¯"""
        try:
            # PyTorchåç«¯
            import torch
            self.torch = torch
            self.torch_available = True
            
            # è®¾ç½®è®¾å¤‡
            if self.device_manager.device:
                self.device = self.device_manager.device
            else:
                self.device = torch.device('cpu')
                
        except ImportError:
            self.logger.warning("âš ï¸ PyTorchæœªå®‰è£…ï¼Œå°†ä½¿ç”¨CPUè®¡ç®—")
            self.torch_available = False
            self.device = None
        
        # NumPyä¼˜åŒ–
        try:
            # å°è¯•ä½¿ç”¨ä¼˜åŒ–çš„BLASåº“
            import numpy as np
            self.logger.info(f"ğŸ“Š NumPyåç«¯: {np.show_config()}")
        except:
            pass
    
    def accelerated_factor_calculation(self, data: pd.DataFrame, 
                                     factor_functions: List[callable],
                                     chunk_size: Optional[int] = None) -> pd.DataFrame:
        """
        åŠ é€Ÿå› å­è®¡ç®—
        
        Args:
            data: è¾“å…¥æ•°æ®
            factor_functions: å› å­è®¡ç®—å‡½æ•°åˆ—è¡¨
            chunk_size: åˆ†å—å¤§å°
            
        Returns:
            è®¡ç®—ç»“æœ
        """
        if data.empty:
            return data
            
        start_time = time.time()
        
        # ç¡®å®šåˆ†å—å¤§å°
        if chunk_size is None:
            chunk_size = self._get_optimal_chunk_size(data)
        
        # å¹¶è¡Œè®¡ç®—é…ç½®
        parallel_config = self.performance_config.get('parallelization', {})
        max_workers = parallel_config.get('max_workers', 4)
        
        if self.compute_config.get('factor_computation', {}).get('parallel_factors', True):
            # å¹¶è¡Œè®¡ç®—å› å­
            results = self._parallel_factor_computation(data, factor_functions, chunk_size, max_workers)
        else:
            # ä¸²è¡Œè®¡ç®—
            results = self._sequential_factor_computation(data, factor_functions)
        
        computation_time = time.time() - start_time
        self.logger.info(f"âš¡ å› å­è®¡ç®—å®Œæˆ: {computation_time:.2f}ç§’")
        
        return results
    
    def _parallel_factor_computation(self, data: pd.DataFrame, 
                                   factor_functions: List[callable],
                                   chunk_size: int, 
                                   max_workers: int) -> pd.DataFrame:
        """å¹¶è¡Œå› å­è®¡ç®—"""
        # æ•°æ®åˆ†å—
        chunks = self._split_data_chunks(data, chunk_size)
        
        # å¹¶è¡Œå¤„ç†
        all_results = []
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤ä»»åŠ¡
            futures = []
            for i, chunk in enumerate(chunks):
                future = executor.submit(self._compute_chunk_factors, chunk, factor_functions, i)
                futures.append(future)
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(futures):
                try:
                    result = future.result()
                    if result is not None:
                        all_results.append(result)
                except Exception as e:
                    self.logger.error(f"âŒ åˆ†å—è®¡ç®—å¤±è´¥: {e}")
        
        # åˆå¹¶ç»“æœ
        if all_results:
            return pd.concat(all_results, ignore_index=True)
        else:
            return data.copy()
    
    def _compute_chunk_factors(self, chunk: pd.DataFrame, 
                             factor_functions: List[callable], 
                             chunk_id: int) -> pd.DataFrame:
        """è®¡ç®—å•ä¸ªæ•°æ®å—çš„å› å­"""
        try:
            start_time = time.time()
            
            # ç§»åŠ¨æ•°æ®åˆ°GPU (å¦‚æœå¯ç”¨)
            if self.torch_available and self.device_manager.device_type in ['cuda', 'mps']:
                chunk_tensor = self._dataframe_to_tensor(chunk)
                chunk_tensor = self.device_manager.move_to_device(chunk_tensor)
                
                # GPUè®¡ç®—
                result_tensor = self._gpu_factor_computation(chunk_tensor, factor_functions)
                result = self._tensor_to_dataframe(result_tensor, chunk.index, chunk.columns)
            else:
                # CPUè®¡ç®—
                result = self._cpu_factor_computation(chunk, factor_functions)
            
            computation_time = time.time() - start_time
            self.logger.debug(f"ğŸ“Š åˆ†å—{chunk_id}è®¡ç®—å®Œæˆ: {computation_time:.2f}ç§’")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ†å—{chunk_id}è®¡ç®—å¤±è´¥: {e}")
            return chunk.copy()
        finally:
            # æ¸…ç†GPUå†…å­˜
            if self.torch_available:
                self._cleanup_gpu_memory()
    
    def _gpu_factor_computation(self, tensor, factor_functions: List[callable]):
        """GPUå› å­è®¡ç®—"""
        # è¿™é‡Œå®ç°å…·ä½“çš„GPUå› å­è®¡ç®—é€»è¾‘
        # ç”±äºå› å­è®¡ç®—å‡½æ•°å¯èƒ½å¾ˆå¤æ‚ï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªæ¡†æ¶
        
        results = []
        for func in factor_functions:
            try:
                if hasattr(func, 'gpu_compute'):
                    # å¦‚æœå‡½æ•°æ”¯æŒGPUè®¡ç®—
                    result = func.gpu_compute(tensor)
                else:
                    # è½¬æ¢ä¸ºCPUè®¡ç®—
                    cpu_data = tensor.cpu().numpy()
                    result = func(cpu_data)
                    result = self.torch.from_numpy(result).to(self.device)
                
                results.append(result)
            except Exception as e:
                self.logger.warning(f"âš ï¸ GPUå› å­è®¡ç®—å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
                cpu_data = tensor.cpu().numpy()
                result = func(cpu_data)
                result = self.torch.from_numpy(result).to(self.device)
                results.append(result)
        
        return self.torch.stack(results, dim=-1) if results else tensor
    
    def _cpu_factor_computation(self, data: pd.DataFrame, factor_functions: List[callable]) -> pd.DataFrame:
        """CPUå› å­è®¡ç®—"""
        result = data.copy()
        
        for i, func in enumerate(factor_functions):
            try:
                factor_result = func(data)
                if isinstance(factor_result, pd.Series):
                    result[f'factor_{i}'] = factor_result
                elif isinstance(factor_result, pd.DataFrame):
                    result = pd.concat([result, factor_result], axis=1)
            except Exception as e:
                self.logger.warning(f"âš ï¸ å› å­{i}è®¡ç®—å¤±è´¥: {e}")
        
        return result
    
    def accelerated_ic_calculation(self, factors: pd.DataFrame, 
                                 returns: pd.DataFrame) -> pd.DataFrame:
        """
        åŠ é€ŸICè®¡ç®—
        
        Args:
            factors: å› å­æ•°æ®
            returns: æ”¶ç›Šç‡æ•°æ®
            
        Returns:
            ICç»“æœ
        """
        start_time = time.time()
        
        if self.torch_available and self.device_manager.device_type in ['cuda', 'mps']:
            ic_result = self._gpu_ic_calculation(factors, returns)
        else:
            ic_result = self._cpu_ic_calculation(factors, returns)
        
        computation_time = time.time() - start_time
        self.logger.info(f"âš¡ ICè®¡ç®—å®Œæˆ: {computation_time:.2f}ç§’")
        
        return ic_result
    
    def _gpu_ic_calculation(self, factors: pd.DataFrame, returns: pd.DataFrame) -> pd.DataFrame:
        """GPUåŠ é€ŸICè®¡ç®—"""
        try:
            # è½¬æ¢ä¸ºå¼ é‡
            factors_tensor = self.torch.from_numpy(factors.values).float()
            returns_tensor = self.torch.from_numpy(returns.values).float()
            
            # ç§»åŠ¨åˆ°GPU
            factors_tensor = self.device_manager.move_to_device(factors_tensor)
            returns_tensor = self.device_manager.move_to_device(returns_tensor)
            
            # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
            # æ ‡å‡†åŒ–
            factors_norm = (factors_tensor - factors_tensor.mean(dim=0)) / factors_tensor.std(dim=0)
            returns_norm = (returns_tensor - returns_tensor.mean(dim=0)) / returns_tensor.std(dim=0)
            
            # è®¡ç®—ç›¸å…³ç³»æ•°
            ic_matrix = self.torch.mm(factors_norm.T, returns_norm) / (factors_tensor.shape[0] - 1)
            
            # è½¬æ¢å›CPUå’ŒDataFrame
            ic_result = ic_matrix.cpu().numpy()
            ic_df = pd.DataFrame(ic_result, 
                               index=factors.columns, 
                               columns=returns.columns)
            
            return ic_df
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ GPU ICè®¡ç®—å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
            return self._cpu_ic_calculation(factors, returns)
    
    def _cpu_ic_calculation(self, factors: pd.DataFrame, returns: pd.DataFrame) -> pd.DataFrame:
        """CPU ICè®¡ç®—"""
        # ä½¿ç”¨NumPyçš„ä¼˜åŒ–ç›¸å…³ç³»æ•°è®¡ç®—
        ic_result = np.corrcoef(factors.T, returns.T)
        
        # æå–å› å­ä¸æ”¶ç›Šçš„ç›¸å…³ç³»æ•°éƒ¨åˆ†
        n_factors = factors.shape[1]
        ic_matrix = ic_result[:n_factors, n_factors:]
        
        ic_df = pd.DataFrame(ic_matrix, 
                           index=factors.columns, 
                           columns=returns.columns)
        
        return ic_df
    
    def accelerated_matrix_operations(self, operation: str, *matrices) -> np.ndarray:
        """
        åŠ é€ŸçŸ©é˜µè¿ç®—
        
        Args:
            operation: è¿ç®—ç±»å‹ ('multiply', 'add', 'subtract', 'inverse', 'svd')
            matrices: è¾“å…¥çŸ©é˜µ
            
        Returns:
            è¿ç®—ç»“æœ
        """
        if not self.torch_available:
            return self._cpu_matrix_operations(operation, *matrices)
        
        try:
            # è½¬æ¢ä¸ºå¼ é‡
            tensors = []
            for matrix in matrices:
                if isinstance(matrix, np.ndarray):
                    tensor = self.torch.from_numpy(matrix).float()
                elif isinstance(matrix, pd.DataFrame):
                    tensor = self.torch.from_numpy(matrix.values).float()
                else:
                    tensor = matrix
                
                tensor = self.device_manager.move_to_device(tensor)
                tensors.append(tensor)
            
            # æ‰§è¡Œè¿ç®—
            if operation == 'multiply':
                result = self.torch.mm(tensors[0], tensors[1])
            elif operation == 'add':
                result = tensors[0] + tensors[1]
            elif operation == 'subtract':
                result = tensors[0] - tensors[1]
            elif operation == 'inverse':
                result = self.torch.inverse(tensors[0])
            elif operation == 'svd':
                U, S, V = self.torch.svd(tensors[0])
                return U.cpu().numpy(), S.cpu().numpy(), V.cpu().numpy()
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„è¿ç®—ç±»å‹: {operation}")
            
            return result.cpu().numpy()
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ GPUçŸ©é˜µè¿ç®—å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
            return self._cpu_matrix_operations(operation, *matrices)
    
    def _cpu_matrix_operations(self, operation: str, *matrices) -> np.ndarray:
        """CPUçŸ©é˜µè¿ç®—"""
        arrays = []
        for matrix in matrices:
            if isinstance(matrix, pd.DataFrame):
                arrays.append(matrix.values)
            else:
                arrays.append(np.array(matrix))
        
        if operation == 'multiply':
            return np.dot(arrays[0], arrays[1])
        elif operation == 'add':
            return arrays[0] + arrays[1]
        elif operation == 'subtract':
            return arrays[0] - arrays[1]
        elif operation == 'inverse':
            return np.linalg.inv(arrays[0])
        elif operation == 'svd':
            return np.linalg.svd(arrays[0])
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¿ç®—ç±»å‹: {operation}")
    
    def _get_optimal_chunk_size(self, data: pd.DataFrame) -> int:
        """è·å–æœ€ä¼˜åˆ†å—å¤§å°"""
        # åŸºç¡€åˆ†å—å¤§å°
        base_chunk_size = self.compute_config.get('factor_computation', {}).get('chunk_size', 1000)
        
        # æ ¹æ®æ•°æ®å¤§å°å’Œè®¾å¤‡æ€§èƒ½è°ƒæ•´
        data_size_mb = data.memory_usage(deep=True).sum() / (1024 * 1024)
        
        if self.device_manager.device_type == 'cuda':
            # GPUå¯ä»¥å¤„ç†æ›´å¤§çš„å—
            optimal_size = min(base_chunk_size * 2, len(data))
        elif self.device_manager.device_type == 'mps':
            # MPSå†…å­˜è¾ƒä¿å®ˆ
            optimal_size = min(base_chunk_size, len(data))
        else:
            # CPUæ ¹æ®å†…å­˜è°ƒæ•´
            available_memory_mb = self.device_manager.available_memory * 1024
            if data_size_mb > available_memory_mb * 0.5:
                scale_factor = (available_memory_mb * 0.5) / data_size_mb
                optimal_size = max(int(base_chunk_size * scale_factor), 100)
            else:
                optimal_size = base_chunk_size
        
        return min(optimal_size, len(data))
    
    def _split_data_chunks(self, data: pd.DataFrame, chunk_size: int) -> List[pd.DataFrame]:
        """åˆ†å‰²æ•°æ®ä¸ºå—"""
        chunks = []
        for i in range(0, len(data), chunk_size):
            chunk = data.iloc[i:i + chunk_size].copy()
            chunks.append(chunk)
        return chunks
    
    def _dataframe_to_tensor(self, df: pd.DataFrame):
        """DataFrameè½¬æ¢ä¸ºå¼ é‡"""
        if not self.torch_available:
            return df.values
        
        # åªé€‰æ‹©æ•°å€¼åˆ—
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        numeric_data = df[numeric_columns].fillna(0)
        
        return self.torch.from_numpy(numeric_data.values).float()
    
    def _tensor_to_dataframe(self, tensor, index, columns) -> pd.DataFrame:
        """å¼ é‡è½¬æ¢ä¸ºDataFrame"""
        if self.torch_available and hasattr(tensor, 'cpu'):
            array = tensor.cpu().numpy()
        else:
            array = tensor
        
        # ç¡®ä¿ç»´åº¦åŒ¹é…
        if array.shape[1] > len(columns):
            # å¦‚æœç»“æœæœ‰æ›´å¤šåˆ—ï¼Œåˆ›å»ºæ–°çš„åˆ—å
            new_columns = list(columns) + [f'computed_{i}' for i in range(len(columns), array.shape[1])]
        else:
            new_columns = columns[:array.shape[1]]
        
        return pd.DataFrame(array, index=index[:len(array)], columns=new_columns)
    
    def _cleanup_gpu_memory(self):
        """æ¸…ç†GPUå†…å­˜"""
        if self.torch_available:
            if hasattr(self.torch.cuda, 'empty_cache'):
                self.torch.cuda.empty_cache()
            elif hasattr(self.torch.mps, 'empty_cache'):
                self.torch.mps.empty_cache()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
    
    def _sequential_factor_computation(self, data: pd.DataFrame, 
                                     factor_functions: List[callable]) -> pd.DataFrame:
        """ä¸²è¡Œå› å­è®¡ç®—"""
        return self._cpu_factor_computation(data, factor_functions)
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        stats = {
            'device_type': self.device_manager.device_type,
            'device_count': self.device_manager.device_count,
            'available_memory_gb': self.device_manager.available_memory
        }
        
        if self.torch_available and self.device_manager.device_type == 'cuda':
            stats['gpu_memory_allocated'] = self.torch.cuda.memory_allocated() / (1024**3)
            stats['gpu_memory_cached'] = self.torch.cuda.memory_reserved() / (1024**3)
        
        return stats