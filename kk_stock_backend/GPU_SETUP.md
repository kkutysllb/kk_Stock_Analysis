# GPUåŠ é€Ÿå®‰è£…æŒ‡å—

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä¸ºå› å­æŒ–æ˜ç³»ç»Ÿé…ç½®GPUåŠ é€Ÿç¯å¢ƒã€‚

## ğŸš€ æ”¯æŒçš„ç¡¬ä»¶

### NVIDIA GPU
- **RTX 4090** (æ¨è) - 24GBæ˜¾å­˜
- **RTX 4080** - 16GBæ˜¾å­˜  
- **RTX 3090/3080** - 10-24GBæ˜¾å­˜
- **Tesla V100/A100** (æœåŠ¡å™¨)
- éœ€è¦CUDA 11.8+æ”¯æŒ

### Apple Silicon
- **M1 Pro/Max/Ultra**
- **M2 Pro/Max/Ultra** 
- **M3 Pro/Max/Ultra**
- é€šè¿‡Metal Performance Shaders (MPS)åŠ é€Ÿ

## ğŸ“¦ å®‰è£…æ­¥éª¤

### 1. NVIDIA GPUç¯å¢ƒ (Linux/Windows)

#### æ£€æŸ¥GPUçŠ¶æ€
```bash
nvidia-smi
```

#### å®‰è£…PyTorch (æ¨èconda)
```bash
# CUDA 12.4ç‰ˆæœ¬ (æ¨è)
conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia

# æˆ–CUDA 11.8ç‰ˆæœ¬
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
```

#### å®‰è£…GPUåŠ é€Ÿä¾èµ–
```bash
pip install -r requirements-gpu.txt
```

#### éªŒè¯GPUå®‰è£…
```python
import torch
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
print(f"å½“å‰GPU: {torch.cuda.get_device_name(0)}")
```

### 2. Apple Siliconç¯å¢ƒ (Mac)

#### å®‰è£…PyTorch
```bash
# Apple Siliconä¼˜åŒ–ç‰ˆæœ¬
conda install pytorch torchvision torchaudio -c pytorch
```

#### å®‰è£…Macä¾èµ–
```bash
pip install -r requirements-mac.txt
```

#### éªŒè¯MPSå®‰è£…
```python
import torch
print(f"MPSå¯ç”¨: {torch.backends.mps.is_available()}")
```

### 3. CPUç¯å¢ƒ (å¤‡ç”¨)

å¦‚æœæ²¡æœ‰GPUæˆ–é‡åˆ°é—®é¢˜ï¼š
```bash
pip install -r requirements.txt
```

## âš¡ æ€§èƒ½ä¼˜åŒ–é…ç½®

### é…ç½®æ–‡ä»¶è°ƒä¼˜
åœ¨ `factor_mining_config.yaml` ä¸­è°ƒæ•´ï¼š

```yaml
acceleration_config:
  device_config:
    # åŒ4090é…ç½®
    cuda:
      device_ids: [0, 1]          # ä½¿ç”¨ä¸¤å¼ å¡
      memory_fraction: 0.8        # æ¯å¼ å¡80%å†…å­˜
      mixed_precision: true       # æ··åˆç²¾åº¦åŠ é€Ÿ
      
    # Macé…ç½®  
    mps:
      memory_fraction: 0.7        # 70%å†…å­˜
      
  performance_optimization:
    batch_processing:
      adaptive_batch_size: true   # è‡ªé€‚åº”æ‰¹å¤§å°
      max_batch_size: 1000        # æœ€å¤§æ‰¹å¤„ç†å¤§å°
```

### å‘½ä»¤è¡Œä½¿ç”¨
```bash
# è‡ªåŠ¨æ£€æµ‹æœ€ä¼˜è®¾å¤‡
python run_factor_mining_system.py --index-type csi_a500

# å¼ºåˆ¶ä½¿ç”¨CUDA
python run_factor_mining_system.py --index-type csi_a500 --device cuda

# å¼ºåˆ¶ä½¿ç”¨MPS (Mac)  
python run_factor_mining_system.py --index-type csi_a500 --device mps

# ç¦ç”¨GPU
python run_factor_mining_system.py --index-type csi_a500 --disable-gpu

# è‡ªå®šä¹‰æ‰¹å¤§å°
python run_factor_mining_system.py --index-type csi_a500 --batch-size 500
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **PyTorchæœªå®‰è£…**
   ```
   No module named 'torch'
   ```
   è§£å†³ï¼šæŒ‰ä¸Šè¿°æ­¥éª¤å®‰è£…PyTorch

2. **CUDAç‰ˆæœ¬ä¸åŒ¹é…**
   ```
   CUDA runtime version mismatch
   ```
   è§£å†³ï¼šé‡æ–°å®‰è£…åŒ¹é…çš„PyTorchç‰ˆæœ¬

3. **GPUå†…å­˜ä¸è¶³**
   ```
   CUDA out of memory
   ```
   è§£å†³ï¼šé™ä½batch_sizeæˆ–memory_fraction

4. **Mac MPSé”™è¯¯**
   ```
   MPS backend not available
   ```
   è§£å†³ï¼šç¡®ä¿macOS 12.3+å’ŒPyTorch 1.12+

### æ€§èƒ½ç›‘æ§
```bash
# ç›‘æ§GPUä½¿ç”¨ç‡
nvidia-smi -l 1

# ç›‘æ§ç³»ç»Ÿèµ„æº
htop

# Pythonå†…ç›‘æ§
python -c "
import psutil, GPUtil
print('CPU:', psutil.cpu_percent())
print('Memory:', psutil.virtual_memory().percent, '%')
if GPUtil.getGPUs():
    gpu = GPUtil.getGPUs()[0]
    print(f'GPU: {gpu.load*100:.1f}%, Memory: {gpu.memoryUtil*100:.1f}%')
"
```

## ğŸ“ˆ æ€§èƒ½é¢„æœŸ

### åŒRTX 4090ç¯å¢ƒ
- å› å­è®¡ç®—: **10-20x**åŠ é€Ÿ
- ICè®¡ç®—: **15-30x**åŠ é€Ÿ  
- æ¨¡å‹è®­ç»ƒ: **5-15x**åŠ é€Ÿ
- å†…å­˜ä½¿ç”¨: å‡å°‘**50%**

### Apple Mç³»åˆ—
- å› å­è®¡ç®—: **3-5x**åŠ é€Ÿ
- ICè®¡ç®—: **5-10x**åŠ é€Ÿ
- æ¨¡å‹è®­ç»ƒ: **2-5x**åŠ é€Ÿ

### æ³¨æ„äº‹é¡¹
- é¦–æ¬¡è¿è¡Œä¼šæœ‰JITç¼–è¯‘å¼€é”€
- å°æ•°æ®é›†å¯èƒ½CPUæ›´å¿«
- æ‰¹å¤§å°è¶Šå¤§ï¼ŒGPUä¼˜åŠ¿è¶Šæ˜æ˜¾
- å®šæœŸæ¸…ç†GPUç¼“å­˜é¿å…å†…å­˜æ³„æ¼